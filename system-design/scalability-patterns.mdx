---
title: "Scalability Patterns"
description: "Deep dive into scaling strategies for senior engineers"
icon: "arrow-up-right-dots"
---

<Warning>
**Senior Level**: This covers advanced scaling patterns expected at L5+ interviews. Know when and why to apply each pattern.
</Warning>

## Horizontal vs Vertical: The Real Trade-offs

```
┌─────────────────────────────────────────────────────────────────┐
│            When to Scale Vertically                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  • Database that can't be easily sharded                       │
│  • Application with heavy in-memory state                      │
│  • When horizontal adds too much complexity                    │
│  • When you can afford the bigger machine                      │
│                                                                 │
│  Real example: PostgreSQL on a single beefy machine            │
│  can handle 50K+ TPS. That's enough for most companies.        │
│                                                                 │
│  AWS largest instance: u-24tb1.112xlarge                       │
│  • 448 vCPUs                                                   │
│  • 24 TB RAM                                                   │
│  • That's a LOT of vertical headroom                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

<Tip>
**Interview Insight**: Don't immediately jump to "scale horizontally." First ask: "What's the actual bottleneck?" Sometimes a bigger machine or query optimization is the right answer.
</Tip>

## Stateless vs Stateful Services

### Making Services Stateless

```
┌─────────────────────────────────────────────────────────────────┐
│                 Stateless Service Pattern                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                                                                 │
│  User ──► LB ──► Server 1 ──┐                                  │
│              ──► Server 2 ──┼──► Redis (sessions)              │
│              ──► Server 3 ──┘                                  │
│                                                                 │
│  How to externalize state:                                     │
│  • Sessions → Redis                                            │
│  • Files → S3/Object storage                                   │
│  • Cache → Distributed cache (Redis, Memcached)                │
│  • Locks → Distributed lock (Redis, ZooKeeper)                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### When Stateful is Okay

```
Stateful services are fine when:
• WebSocket connections (natural affinity)
• Real-time gaming (session state)
• In-memory caching (local cache + distributed)
• Batch processing (worker owns work)

Key: Design for graceful degradation when state is lost
```

## Caching at Scale

### Multi-Level Caching

```
┌─────────────────────────────────────────────────────────────────┐
│                 Multi-Level Cache Architecture                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                    ┌─────────────┐                             │
│                    │   Client    │                             │
│                    └──────┬──────┘                             │
│                           │                                     │
│  L0: Browser Cache        │  • HTTP cache headers             │
│  (Cache-Control)          │  • Service worker cache            │
│                           │  • localStorage/IndexedDB          │
│                           ▼                                     │
│                    ┌─────────────┐                             │
│  L1: CDN Edge      │     CDN     │  • Static assets           │
│                    │   (Edge)    │  • API responses (GET)      │
│                    └──────┬──────┘  • TTL: minutes to days     │
│                           │                                     │
│                           ▼                                     │
│                    ┌─────────────┐                             │
│  L2: API Gateway   │  Gateway    │  • Response caching         │
│                    │   Cache     │  • Rate limiting cache      │
│                    └──────┬──────┘                             │
│                           │                                     │
│                           ▼                                     │
│                    ┌─────────────┐                             │
│  L3: App Local     │  In-Memory  │  • Hot data                 │
│  (per server)      │   (Caffeine)│  • Computed results         │
│                    └──────┬──────┘  • TTL: seconds             │
│                           │                                     │
│                           ▼                                     │
│                    ┌─────────────┐                             │
│  L4: Distributed   │   Redis/    │  • Shared state            │
│  Cache             │  Memcached  │  • Sessions                 │
│                    └──────┬──────┘  • TTL: minutes to hours    │
│                           │                                     │
│                           ▼                                     │
│                    ┌─────────────┐                             │
│  L5: Database      │  Database   │  • Query result cache      │
│  Query Cache       │   Cache     │  • Materialized views       │
│                    └─────────────┘                             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Cache Consistency Strategies

```python
# Strategy 1: Cache-Aside with TTL (Simple)
def get_user(user_id):
    # 1. Try cache
    user = cache.get(f"user:{user_id}")
    if user:
        return user
    
    # 2. Load from DB
    user = db.query("SELECT * FROM users WHERE id = ?", user_id)
    
    # 3. Set cache with TTL
    cache.set(f"user:{user_id}", user, ttl=300)  # 5 min
    return user

def update_user(user_id, data):
    # Update DB
    db.update(user_id, data)
    # Invalidate cache
    cache.delete(f"user:{user_id}")


# Strategy 2: Write-Through (Strong Consistency)
def update_user(user_id, data):
    with transaction():
        # Update DB
        db.update(user_id, data)
        # Update cache in same transaction
        cache.set(f"user:{user_id}", data)


# Strategy 3: Event-Driven Invalidation (Best for microservices)
def update_user(user_id, data):
    db.update(user_id, data)
    # Publish event
    event_bus.publish("user.updated", {"user_id": user_id})

# Cache service listens for events
@event_handler("user.updated")
def invalidate_user_cache(event):
    cache.delete(f"user:{event.user_id}")
```

## Database Scaling Patterns

### Read Replicas Pattern

```
┌─────────────────────────────────────────────────────────────────┐
│                 Read Scaling with Replicas                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                     Application                                 │
│                          │                                      │
│         ┌────────────────┴────────────────┐                    │
│         │                                 │                     │
│     WRITES                             READS                    │
│         │                                 │                     │
│         ▼                                 ▼                     │
│    ┌─────────┐                  ┌──────────────────┐           │
│    │ Primary │──Replication────►│     Replicas     │           │
│    │   DB    │                  │   (Read Pool)    │           │
│    └─────────┘                  └──────────────────┘           │
│                                    │    │    │                  │
│                                    ▼    ▼    ▼                  │
│                                  ┌──┐ ┌──┐ ┌──┐                │
│                                  │R1│ │R2│ │R3│                │
│                                  └──┘ └──┘ └──┘                │
│                                                                 │
│  Replication Lag Handling:                                     │
│  • Read-your-writes: Read from primary after write             │
│  • Monotonic reads: Stick to same replica per session         │
│  • Lag monitoring: Alert if replica > 1s behind               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

```python
class ReadWriteRouter:
    def __init__(self, primary, replicas):
        self.primary = primary
        self.replicas = replicas
        self.replica_index = 0
    
    def get_connection(self, query_type: str, user_session=None):
        if query_type == "write":
            return self.primary
        
        # Read-your-writes: Check if user recently wrote
        if user_session and user_session.last_write_time:
            if time.time() - user_session.last_write_time < 5:
                # Recent write, use primary to avoid stale reads
                return self.primary
        
        # Round-robin across replicas
        replica = self.replicas[self.replica_index % len(self.replicas)]
        self.replica_index += 1
        return replica
```

### Sharding Strategies Deep Dive

```
┌─────────────────────────────────────────────────────────────────┐
│                  Sharding Strategy Selection                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  RANGE-BASED SHARDING                                           │
│  ─────────────────────                                          │
│  Shard 1: user_id 1-1M                                         │
│  Shard 2: user_id 1M-2M                                        │
│                                                                 │
│  + Range queries within shard                                  │
│  + Easy to understand                                          │
│  - Hot shards (new users → last shard)                         │
│  - Uneven distribution                                         │
│                                                                 │
│  Use when: Time-series data, rarely query across ranges        │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  HASH-BASED SHARDING                                            │
│  ─────────────────────                                          │
│  Shard = hash(user_id) % num_shards                            │
│                                                                 │
│  + Even distribution                                           │
│  + No hot shards                                               │
│  - Resharding nightmare (all data moves)                       │
│  - Range queries hit all shards                                │
│                                                                 │
│  Use when: High cardinality, even access, no range queries     │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  CONSISTENT HASHING                                             │
│  ───────────────────                                            │
│  Keys → positions on hash ring                                 │
│  Keys assigned to next node clockwise                          │
│                                                                 │
│  + Minimal data movement on node add/remove                    │
│  + Virtual nodes for balance                                   │
│  - More complex implementation                                 │
│                                                                 │
│  Use when: Caches, distributed storage, dynamic cluster        │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  DIRECTORY-BASED SHARDING                                       │
│  ───────────────────────────                                    │
│  Lookup service maps key → shard                               │
│                                                                 │
│  + Maximum flexibility                                         │
│  + Easy resharding                                             │
│  - Lookup service is SPOF                                      │
│  - Extra hop for every query                                   │
│                                                                 │
│  Use when: Complex sharding logic, frequent rebalancing        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Cross-Shard Operations

```python
# Problem: Query needs data from multiple shards

# Solution 1: Scatter-Gather
async def get_user_orders_all_time(user_id):
    # User's orders might be on different time-based shards
    shard_ids = get_all_shards()
    
    # Query all shards in parallel
    tasks = [query_shard(shard, user_id) for shard in shard_ids]
    results = await asyncio.gather(*tasks)
    
    # Merge results
    return merge_and_sort(results)

# Solution 2: Denormalization
# Store frequently-joined data together
# Instead of: users shard + orders shard + products shard
# Store: user_orders (denormalized) on user's shard

# Solution 3: Global Tables
# Some tables replicated to all shards (read-only)
# Example: countries, currencies, product categories
```

## Async Processing Patterns

### Task Queue Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│              Production Task Queue Pattern                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Producers                     Queue                Workers     │
│  ─────────                     ─────               ───────      │
│  ┌─────────┐                                       ┌─────────┐ │
│  │  API    │───┐           ┌─────────────┐    ┌───►│Worker 1 │ │
│  │ Server  │   │           │             │    │    └─────────┘ │
│  └─────────┘   │           │  Message    │    │    ┌─────────┐ │
│  ┌─────────┐   │──────────►│   Queue     │────┼───►│Worker 2 │ │
│  │  Cron   │───┤           │  (RabbitMQ/ │    │    └─────────┘ │
│  │  Jobs   │   │           │   SQS/Redis)│    │    ┌─────────┐ │
│  └─────────┘   │           │             │    └───►│Worker N │ │
│  ┌─────────┐   │           └──────┬──────┘         └─────────┘ │
│  │ Event   │───┘                  │                            │
│  │ Handler │                      │                            │
│  └─────────┘                      ▼                            │
│                           ┌─────────────┐                      │
│                           │  Dead Letter│                      │
│                           │    Queue    │                      │
│                           └─────────────┘                      │
│                                                                 │
│  Key Features:                                                 │
│  • Visibility timeout (prevents double processing)             │
│  • Retry with exponential backoff                              │
│  • Dead letter queue for failed messages                       │
│  • Priority queues for urgent tasks                            │
│  • Rate limiting per worker                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

```python
class ReliableTaskProcessor:
    """
    Production-grade task processor with exactly-once semantics
    """
    
    def __init__(self, queue, db, max_retries=3):
        self.queue = queue
        self.db = db
        self.max_retries = max_retries
    
    async def process_task(self, task):
        task_id = task.id
        
        # Idempotency check
        if await self.is_processed(task_id):
            await self.queue.ack(task)
            return
        
        try:
            # Process with timeout
            async with asyncio.timeout(30):
                result = await self.do_work(task)
            
            # Mark as processed (atomically with result storage)
            await self.mark_completed(task_id, result)
            await self.queue.ack(task)
            
        except asyncio.TimeoutError:
            await self.handle_timeout(task)
            
        except RetryableError as e:
            await self.retry_or_dlq(task, e)
            
        except Exception as e:
            # Non-retryable, send to DLQ immediately
            await self.send_to_dlq(task, e)
    
    async def retry_or_dlq(self, task, error):
        if task.retry_count < self.max_retries:
            delay = 2 ** task.retry_count  # Exponential backoff
            await self.queue.retry(task, delay_seconds=delay)
        else:
            await self.send_to_dlq(task, error)
```

### Event-Driven Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│               Event-Driven Architecture                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Order Service publishes event, others react:                  │
│                                                                 │
│                    ┌─────────────────┐                         │
│                    │  Event Bus      │                         │
│                    │  (Kafka/SNS)    │                         │
│                    └────────┬────────┘                         │
│                             │                                   │
│    ┌────────────────────────┼────────────────────────┐         │
│    │             │          │         │              │          │
│    ▼             ▼          ▼         ▼              ▼          │
│ ┌──────┐    ┌────────┐ ┌────────┐ ┌────────┐   ┌────────┐     │
│ │Email │    │Inventory│ │Payment│ │Analytics│   │Search  │     │
│ │Service│   │Service │ │Service│ │Service │   │Index   │     │
│ └──────┘    └────────┘ └────────┘ └────────┘   └────────┘     │
│                                                                 │
│  Event: OrderCreated                                           │
│  {                                                             │
│    "event_id": "evt_123",                                      │
│    "event_type": "order.created",                              │
│    "timestamp": "2024-01-15T10:30:00Z",                        │
│    "data": {                                                   │
│      "order_id": "ord_456",                                    │
│      "user_id": "usr_789",                                     │
│      "items": [...],                                           │
│      "total": 150.00                                           │
│    }                                                           │
│  }                                                             │
│                                                                 │
│  Benefits:                                                     │
│  • Loose coupling (services don't know about each other)       │
│  • Easy to add new consumers                                   │
│  • Natural audit trail                                         │
│                                                                 │
│  Challenges:                                                   │
│  • Eventual consistency                                        │
│  • Event ordering                                              │
│  • Debugging distributed flows                                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Load Shedding & Backpressure

### Graceful Degradation

```
┌─────────────────────────────────────────────────────────────────┐
│               Load Shedding Strategies                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  When system is overloaded, choose what to drop:               │
│                                                                 │
│  1. PRIORITY-BASED SHEDDING                                     │
│     ────────────────────────                                    │
│     Queue has priorities: Critical > Normal > Low              │
│     Drop low priority first                                    │
│                                                                 │
│     Example: Keep payment processing, drop analytics           │
│                                                                 │
│  2. PERCENTAGE SHEDDING                                         │
│     ─────────────────────                                       │
│     Reject X% of requests randomly                             │
│     "Sorry, we're busy, try again"                             │
│                                                                 │
│  3. TIMEOUT-BASED SHEDDING                                      │
│     ────────────────────────                                    │
│     Drop requests that have been waiting too long              │
│     "If user waited 30s, they've probably left"                │
│                                                                 │
│  4. USER-TIER SHEDDING                                          │
│     ─────────────────────                                       │
│     Premium users: Never shed                                  │
│     Free users: Shed when overloaded                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

```python
class AdaptiveLoadShedder:
    """
    Dynamically shed load based on system health
    """
    
    def __init__(self, target_latency_ms=100, window_size=100):
        self.target_latency = target_latency_ms
        self.latencies = deque(maxlen=window_size)
        self.shed_probability = 0.0
    
    def should_accept_request(self, request) -> bool:
        # Never shed high-priority requests
        if request.priority == "critical":
            return True
        
        # Calculate current drop probability
        if random.random() < self.shed_probability:
            return False
        
        return True
    
    def record_latency(self, latency_ms):
        self.latencies.append(latency_ms)
        self._adjust_shed_rate()
    
    def _adjust_shed_rate(self):
        if len(self.latencies) < 10:
            return
        
        p99 = sorted(self.latencies)[int(len(self.latencies) * 0.99)]
        
        if p99 > self.target_latency * 2:
            # Way over target, increase shedding
            self.shed_probability = min(0.9, self.shed_probability + 0.1)
        elif p99 > self.target_latency:
            # Slightly over, small increase
            self.shed_probability = min(0.9, self.shed_probability + 0.02)
        elif p99 < self.target_latency * 0.5:
            # Under target, decrease shedding
            self.shed_probability = max(0, self.shed_probability - 0.05)
```

## Senior Interview Questions

<Accordion title="How do you scale a write-heavy system?">
**Approach**:
1. **Identify the bottleneck**: Is it DB? Network? Application?
2. **Batching**: Combine multiple writes into one
3. **Async writes**: Write to queue, persist later
4. **Sharding**: Distribute writes across nodes
5. **LSM-tree databases**: Cassandra, RocksDB (optimized for writes)

**Example answer**: "First, I'd batch writes on the application side - instead of 1000 individual inserts, do bulk insert. Then add a queue like Kafka as a buffer. Finally, use a write-optimized database like Cassandra if the volume is truly massive."
</Accordion>

<Accordion title="What's your approach to capacity planning?">
**Framework**:
1. **Current baseline**: Measure current QPS, latency, resource usage
2. **Growth projection**: Expected traffic increase (e.g., 2x in 6 months)
3. **Headroom**: Plan for 3x current load (for spikes)
4. **Load testing**: Verify system handles projected load
5. **Monitoring**: Track capacity metrics, alert at 70% utilization

**Key metrics to track**:
- CPU utilization by service
- Memory usage and GC pressure
- Database connections and query latency
- Queue depth and processing rate
- Network bandwidth
</Accordion>

<Accordion title="How do you handle database migrations at scale?">
**Safe migration strategy**:
1. **Dual-write**: Write to both old and new schema
2. **Backfill**: Migrate historical data in batches
3. **Shadow read**: Read from new, compare with old
4. **Cutover**: Switch reads to new schema
5. **Cleanup**: Remove dual-write, drop old schema

**Key principles**:
- Never lock tables in production
- Migrations must be reversible
- Test with production-sized data
- Have a rollback plan
- Do it during low-traffic periods
</Accordion>

<Accordion title="How do you debug a system serving stale data?">
**Systematic approach**:
1. **Identify scope**: All users? Some? Specific data?
2. **Check cache layers**: CDN, app cache, Redis, DB cache
3. **Verify TTLs**: Are caches expiring correctly?
4. **Check replication**: Is replica lagging?
5. **Trace the write**: Did write actually succeed?

**Common causes**:
- Cache not being invalidated on write
- Reading from stale replica
- CDN caching dynamic content
- Race condition between cache invalidation and read
</Accordion>
