---
title: "Advanced Concepts"
description: "Senior-level system design concepts for Staff+ interviews"
icon: "brain"
---

<Warning>
**Senior/Staff Level Content**: This section covers advanced topics that differentiate senior engineers from mid-level. Master these for L5+ (Senior) and Staff interviews at FAANG.
</Warning>

## Consistency Deep Dive

### Linearizability vs Serializability

Most candidates confuse these. Know the difference!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Linearizability vs Serializability                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  LINEARIZABILITY (Single-object, real-time)                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  â€¢ Operations appear to happen atomically at some point         â”‚
â”‚  â€¢ Real-time ordering: if op A completes before op B starts,   â”‚
â”‚    then A is ordered before B                                   â”‚
â”‚  â€¢ Think: "single copy of data that everyone sees"             â”‚
â”‚                                                                 â”‚
â”‚  Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º            â”‚
â”‚                                                                 â”‚
â”‚  Client A:  â”€â”€[write x=1]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  Client B:        â”€â”€â”€â”€â”€â”€[read x]â”€â”€â”€                            â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â””â”€â–º Must return 1 (write completed)   â”‚
â”‚                                                                 â”‚
â”‚  SERIALIZABILITY (Multi-object, transactions)                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
â”‚  â€¢ Transactions appear to execute in SOME serial order          â”‚
â”‚  â€¢ Doesn't guarantee real-time ordering                         â”‚
â”‚  â€¢ Think: "transactions don't see partial state"               â”‚
â”‚                                                                 â”‚
â”‚  T1: read(A), write(B)                                         â”‚
â”‚  T2: read(B), write(A)                                         â”‚
â”‚  Serial order: T1â†’T2 or T2â†’T1, but not interleaved            â”‚
â”‚                                                                 â”‚
â”‚  STRICT SERIALIZABILITY                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  â€¢ Both! Serial order + real-time ordering                      â”‚
â”‚  â€¢ Most expensive, what Spanner provides                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Isolation Levels (Know All Four!)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Isolation Levels                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  READ UNCOMMITTED (Weakest)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  â€¢ Can see uncommitted changes (dirty reads)                   â”‚
â”‚  â€¢ Almost never used                                            â”‚
â”‚                                                                 â”‚
â”‚  T1: write(x=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ rollback                          â”‚
â”‚  T2:     read(x) â”€â–º 1 (dirty read!)                            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  READ COMMITTED (Default in PostgreSQL)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  â€¢ Only see committed data                                     â”‚
â”‚  â€¢ Allows non-repeatable reads                                  â”‚
â”‚                                                                 â”‚
â”‚  T1: read(x) â”€â–º 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ read(x) â”€â–º 2 (changed!)         â”‚
â”‚  T2:      write(x=2), commit                                   â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  REPEATABLE READ (Default in MySQL InnoDB)                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  â€¢ Same query returns same results in a transaction             â”‚
â”‚  â€¢ Allows phantom reads (new rows appear)                       â”‚
â”‚                                                                 â”‚
â”‚  T1: SELECT COUNT(*) WHERE age > 30 â”€â–º 5                       â”‚
â”‚  T2:      INSERT INTO users (age=35), commit                   â”‚
â”‚  T1: SELECT COUNT(*) WHERE age > 30 â”€â–º 6 (phantom!)            â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  SERIALIZABLE (Strongest)                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  â€¢ Full isolation, as if transactions ran serially              â”‚
â”‚  â€¢ Highest safety, lowest performance                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Level | Dirty Read | Non-Repeatable Read | Phantom Read |
|-------|------------|---------------------|--------------|
| Read Uncommitted | âœ“ | âœ“ | âœ“ |
| Read Committed | âœ— | âœ“ | âœ“ |
| Repeatable Read | âœ— | âœ— | âœ“ |
| Serializable | âœ— | âœ— | âœ— |

<Tip>
**Interview Answer**: "I'd use Read Committed for most cases. Serializable only for critical financial transactions where consistency matters more than throughput."
</Tip>

## Distributed Consensus Deep Dive

### Leader Election: Why It's Hard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Split Brain Problem                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Initial State: Node A is leader                               â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Node A â”‚â”€â”€â”€â”€â”€â”€â”‚ Node B â”‚â”€â”€â”€â”€â”€â”€â”‚ Node C â”‚                   â”‚
â”‚  â”‚ LEADER â”‚      â”‚FOLLOWERâ”‚      â”‚FOLLOWERâ”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                 â”‚
â”‚  Network partition occurs:                                     â”‚
â”‚                                                                 â”‚
â”‚  Partition 1         â•‘         Partition 2                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â•‘          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Node A â”‚          â•‘          â”‚ Node B â”‚â”€â”‚ Node C â”‚         â”‚
â”‚  â”‚ LEADER â”‚          â•‘          â”‚FOLLOWERâ”‚ â”‚FOLLOWERâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â•‘          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                      â•‘               â”‚                         â”‚
â”‚  A thinks it's       â•‘          B or C becomes leader!        â”‚
â”‚  still leader!       â•‘          (timeout, no heartbeat)        â”‚
â”‚                      â•‘                                         â”‚
â”‚  DANGER: Two leaders! (Split brain)                            â”‚
â”‚                                                                 â”‚
â”‚  Solution: Quorum-based voting                                 â”‚
â”‚  â€¢ Need majority (N/2 + 1) to elect leader                     â”‚
â”‚  â€¢ Partition 1 has 1/3 (minority) â†’ can't write               â”‚
â”‚  â€¢ Partition 2 has 2/3 (majority) â†’ can elect, can write      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Raft: The Algorithm You Should Know

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Raft Consensus                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  KEY CONCEPTS:                                                  â”‚
â”‚                                                                 â”‚
â”‚  1. TERMS (Logical Clock)                                       â”‚
â”‚     â€¢ Time divided into terms                                   â”‚
â”‚     â€¢ Each term has at most one leader                         â”‚
â”‚     â€¢ Term number monotonically increases                       â”‚
â”‚                                                                 â”‚
â”‚     Term 1       Term 2       Term 3                           â”‚
â”‚     â”œâ”€â”€Leader Aâ”€â”€â”¼â”€â”€Electionâ”€â”€â”¼â”€â”€Leader Bâ”€â”€â”€â”€â”€â”€â–º               â”‚
â”‚                  â”‚  (failed)  â”‚                                 â”‚
â”‚                                                                 â”‚
â”‚  2. LOG REPLICATION                                             â”‚
â”‚                                                                 â”‚
â”‚     Leader Log:  [1:x=1] [1:y=2] [2:x=3] [2:z=4]               â”‚
â”‚                    â†“       â†“       â†“       â†“                   â”‚
â”‚     Follower 1:  [1:x=1] [1:y=2] [2:x=3] [2:z=4] âœ“             â”‚
â”‚     Follower 2:  [1:x=1] [1:y=2] [2:x=3]         (catching up) â”‚
â”‚                                                                 â”‚
â”‚     Entry committed when replicated to majority                 â”‚
â”‚                                                                 â”‚
â”‚  3. LEADER ELECTION                                             â”‚
â”‚                                                                 â”‚
â”‚     Follower timeout â†’ Candidate â†’ RequestVote RPC             â”‚
â”‚     â€¢ Vote granted if:                                         â”‚
â”‚       - Candidate's term >= voter's term                       â”‚
â”‚       - Candidate's log is at least as up-to-date              â”‚
â”‚       - Voter hasn't voted for someone else this term          â”‚
â”‚     â€¢ Majority votes â†’ become Leader                           â”‚
â”‚                                                                 â”‚
â”‚  4. SAFETY GUARANTEE                                            â”‚
â”‚     â€¢ Elected leader has all committed entries                  â”‚
â”‚     â€¢ Leaders never overwrite their log                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Clock Synchronization

### Why Wall Clocks Fail

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Clock Synchronization Problems                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Physical clocks drift (100 ms/day typical)                    â”‚
â”‚  NTP sync has error bounds (1-10ms typically)                  â”‚
â”‚                                                                 â”‚
â”‚  Problem scenario:                                              â”‚
â”‚                                                                 â”‚
â”‚  Machine A (clock: 10:00:00.000):  write(x=1) @ 10:00:00.000  â”‚
â”‚  Machine B (clock: 10:00:00.050):  write(x=2) @ 10:00:00.050  â”‚
â”‚                                                                 â”‚
â”‚  But B's clock is 100ms ahead! Real order:                     â”‚
â”‚  â€¢ B actually wrote at real time 09:59:59.950                  â”‚
â”‚  â€¢ A wrote at real time 10:00:00.000                           â”‚
â”‚  â€¢ A happened AFTER B, but timestamps say B is newer!          â”‚
â”‚                                                                 â”‚
â”‚  Solution 1: Logical clocks (Lamport)                          â”‚
â”‚  Solution 2: Vector clocks                                      â”‚
â”‚  Solution 3: Hybrid clocks (HLC)                               â”‚
â”‚  Solution 4: GPS/atomic clocks (Spanner's TrueTime)            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vector Clocks (Conflict Detection)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Vector Clocks                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Each node maintains vector: {A: count, B: count, C: count}    â”‚
â”‚                                                                 â”‚
â”‚  Node A         Node B         Node C                          â”‚
â”‚  {A:0,B:0,C:0}  {A:0,B:0,C:0}  {A:0,B:0,C:0}                   â”‚
â”‚      â”‚              â”‚              â”‚                            â”‚
â”‚  write(x=1)         â”‚              â”‚                            â”‚
â”‚  {A:1,B:0,C:0}      â”‚              â”‚                            â”‚
â”‚      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚                            â”‚
â”‚      â”‚         {A:1,B:0,C:0}       â”‚                            â”‚
â”‚      â”‚              â”‚              â”‚                            â”‚
â”‚      â”‚         write(y=2)          â”‚                            â”‚
â”‚      â”‚         {A:1,B:1,C:0}       â”‚                            â”‚
â”‚      â”‚              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                            â”‚
â”‚      â”‚              â”‚         {A:1,B:1,C:0}                     â”‚
â”‚      â”‚              â”‚              â”‚                            â”‚
â”‚                                                                 â”‚
â”‚  Comparison:                                                    â”‚
â”‚  {A:1,B:2,C:0} vs {A:2,B:1,C:0}                                â”‚
â”‚  Neither dominates â†’ CONFLICT! Must resolve                    â”‚
â”‚                                                                 â”‚
â”‚  {A:1,B:2,C:0} vs {A:1,B:3,C:0}                                â”‚
â”‚  Second dominates â†’ Second is newer, no conflict               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Partitioning Strategies

### Partition Key Selection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Choosing Partition Keys                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  GOOD Partition Key Properties:                                 â”‚
â”‚  âœ“ High cardinality (many unique values)                       â”‚
â”‚  âœ“ Evenly distributed access                                   â”‚
â”‚  âœ“ Matches query patterns                                      â”‚
â”‚                                                                 â”‚
â”‚  Example: E-commerce Orders                                     â”‚
â”‚                                                                 â”‚
â”‚  âŒ BAD: partition by date                                      â”‚
â”‚     â€¢ Today's partition gets all traffic (hot partition)       â”‚
â”‚     â€¢ Old partitions sit idle                                  â”‚
â”‚                                                                 â”‚
â”‚  âŒ BAD: partition by country                                   â”‚
â”‚     â€¢ US partition has 60% of traffic                          â”‚
â”‚     â€¢ Small countries underutilized                            â”‚
â”‚                                                                 â”‚
â”‚  âœ… GOOD: partition by order_id                                â”‚
â”‚     â€¢ Random distribution                                       â”‚
â”‚     â€¢ But: can't query "all orders for user X" easily          â”‚
â”‚                                                                 â”‚
â”‚  âœ… BETTER: partition by user_id                                â”‚
â”‚     â€¢ User's orders on same partition (locality)               â”‚
â”‚     â€¢ Query patterns match                                     â”‚
â”‚     â€¢ Watch for celebrity users (hot partition)                â”‚
â”‚                                                                 â”‚
â”‚  âœ… BEST: compound key (user_id, order_date)                   â”‚
â”‚     â€¢ Partition by user_id                                     â”‚
â”‚     â€¢ Sort by order_date within partition                      â”‚
â”‚     â€¢ Efficient for "user X's orders in last month"           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Handling Hot Partitions

```python
# Strategy 1: Add random suffix
def get_partition_key(celebrity_id):
    if is_celebrity(celebrity_id):
        # Split celebrity across 10 partitions
        suffix = random.randint(0, 9)
        return f"{celebrity_id}_{suffix}"
    return celebrity_id

# Reading requires scatter-gather
def get_celebrity_data(celebrity_id):
    results = []
    for suffix in range(10):
        key = f"{celebrity_id}_{suffix}"
        results.extend(query_partition(key))
    return results

# Strategy 2: Time-based suffix
def get_partition_key_time(user_id):
    # Different partition each hour
    hour = datetime.now().hour
    return f"{user_id}_{hour % 4}"
```

## Exactly-Once Semantics

### The Three Delivery Guarantees

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Message Delivery Guarantees                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  AT-MOST-ONCE                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Send and forget. May lose messages.                           â”‚
â”‚  Use case: Metrics, logs (some loss OK)                        â”‚
â”‚                                                                 â”‚
â”‚  Producer â”€â”€[msg]â”€â”€â–º Broker    (might fail silently)           â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  AT-LEAST-ONCE                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Retry until ACK. May duplicate messages.                      â”‚
â”‚  Use case: Most applications (with idempotent consumers)       â”‚
â”‚                                                                 â”‚
â”‚  Producer â”€â”€[msg]â”€â”€â–º Broker â”€â”€[ACK]â”€â”€â–º Producer                â”‚
â”‚      â”‚                  â”‚                                       â”‚
â”‚      â””â”€â”€[retry]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (if no ACK, retry â†’ duplicate)       â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  EXACTLY-ONCE                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚  Each message processed exactly once. Hard to achieve!         â”‚
â”‚  Use case: Financial transactions, critical data               â”‚
â”‚                                                                 â”‚
â”‚  Achieved via: Idempotency + Deduplication + Transactions      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementing Exactly-Once

```python
class ExactlyOnceProcessor:
    """
    Exactly-once processing with idempotency keys
    """
    
    def process_message(self, message):
        idempotency_key = message.id
        
        # Step 1: Check if already processed
        if self.is_processed(idempotency_key):
            return self.get_cached_result(idempotency_key)
        
        # Step 2: Process with transaction
        with self.db.transaction():
            # Do the work
            result = self.do_business_logic(message)
            
            # Record as processed (same transaction!)
            self.mark_processed(idempotency_key, result)
            
            # Commit message offset (Kafka-style)
            self.commit_offset(message.offset)
        
        return result
    
    def is_processed(self, key):
        return self.db.exists(f"processed:{key}")
    
    def mark_processed(self, key, result):
        self.db.set(f"processed:{key}", result, ttl=86400)
```

## Distributed Caching Patterns

### Cache Stampede Prevention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Cache Stampede Problem                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Popular key expires â†’ 1000 servers hit DB simultaneously      â”‚
â”‚                                                                 â”‚
â”‚  Server 1 â”€â”                                                    â”‚
â”‚  Server 2 â”€â”¤                                                    â”‚
â”‚  Server 3 â”€â”¼â”€â”€â”€â–º Cache MISS â”€â”€â”€â–º Database â† ðŸ’¥ OVERWHELMED     â”‚
â”‚  ...       â”‚                                                    â”‚
â”‚  Server N â”€â”˜                                                    â”‚
â”‚                                                                 â”‚
â”‚  SOLUTIONS:                                                     â”‚
â”‚                                                                 â”‚
â”‚  1. LOCKING (Mutex)                                             â”‚
â”‚     First request acquires lock, others wait or get stale      â”‚
â”‚                                                                 â”‚
â”‚  2. PROBABILISTIC EARLY EXPIRATION                              â”‚
â”‚     Refresh before expiry with some probability                â”‚
â”‚                                                                 â”‚
â”‚  3. BACKGROUND REFRESH                                          â”‚
â”‚     Never expire, refresh asynchronously                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
import random
import time

class StampedePreventingCache:
    def get(self, key, fetch_func, ttl=3600, beta=1.0):
        cached = self.cache.get(key)
        
        if cached:
            value, expiry, delta = cached
            now = time.time()
            
            # Probabilistic early refresh
            # As we approach expiry, probability increases
            gap = expiry - now
            if gap > 0:
                # XFetch algorithm
                random_early = delta * beta * math.log(random.random())
                if gap + random_early > 0:
                    return value  # Use cached value
        
        # Cache miss or early refresh triggered
        # Use distributed lock to prevent stampede
        lock_key = f"lock:{key}"
        if self.acquire_lock(lock_key, timeout=5):
            try:
                start = time.time()
                value = fetch_func()
                delta = time.time() - start
                
                self.cache.set(key, (value, time.time() + ttl, delta))
                return value
            finally:
                self.release_lock(lock_key)
        else:
            # Someone else is refreshing, return stale or wait
            if cached:
                return cached[0]  # Return stale
            time.sleep(0.1)
            return self.get(key, fetch_func, ttl, beta)  # Retry
```

## Rate Limiting at Scale

### Distributed Rate Limiting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Distributed Rate Limiting Strategies                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  STRATEGY 1: Centralized (Redis)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  All servers check same Redis                                  â”‚
â”‚  âœ“ Accurate   âœ— Single point of failure                       â”‚
â”‚                                                                 â”‚
â”‚  Server 1 â”€â”                                                    â”‚
â”‚  Server 2 â”€â”¼â”€â”€â”€â–º Redis â”€â”€â”€â–º Accurate count                     â”‚
â”‚  Server 3 â”€â”˜                                                    â”‚
â”‚                                                                 â”‚
â”‚  STRATEGY 2: Local + Sync                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  Local counters, periodically sync                             â”‚
â”‚  âœ“ Fast   âœ— Approximate                                        â”‚
â”‚                                                                 â”‚
â”‚  Server 1: local_count=50 â”€â”€â”                                  â”‚
â”‚  Server 2: local_count=40 â”€â”€â”¼â”€â”€â–º Sync every 1s                 â”‚
â”‚  Server 3: local_count=30 â”€â”€â”˜                                  â”‚
â”‚                                                                 â”‚
â”‚  STRATEGY 3: Token Bucket with Redis                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  Each request: DECR if tokens > 0                              â”‚
â”‚  Background: Refill tokens at fixed rate                       â”‚
â”‚                                                                 â”‚
â”‚  Lua script for atomic check-and-decrement:                    â”‚
â”‚  local tokens = redis.call('GET', key)                         â”‚
â”‚  if tokens > 0 then                                            â”‚
â”‚      redis.call('DECR', key)                                   â”‚
â”‚      return 1  -- allowed                                      â”‚
â”‚  end                                                           â”‚
â”‚  return 0  -- rejected                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Interview Questions: Senior Level

<Accordion title="How would you design for multi-region active-active?">
**Key Points**:
1. **Data replication**: Async replication between regions (eventual consistency)
2. **Conflict resolution**: Last-write-wins (with vector clocks) or custom merge
3. **Routing**: GeoDNS to route users to nearest region
4. **Failover**: Health checks + automatic DNS failover
5. **Consistency**: Accept that cross-region writes may conflict

**Trade-offs to mention**:
- Latency vs consistency
- Cost of running in multiple regions
- Complexity of conflict resolution
</Accordion>

<Accordion title="How do you handle a database that can't keep up with writes?">
**Solutions in order of complexity**:
1. **Batch writes**: Accumulate and write in batches
2. **Write-behind cache**: Write to Redis, async persist to DB
3. **Message queue**: Queue writes, process at sustainable rate
4. **Sharding**: Distribute writes across multiple DB nodes
5. **Different DB**: Switch to write-optimized DB (Cassandra, ScyllaDB)

**Always ask**: "What's the consistency requirement? Can we lose some writes?"
</Accordion>

<Accordion title="Explain how you'd implement distributed transactions">
**Answer structure**:
1. **First ask**: "Do we really need distributed transactions?" Often can redesign.
2. **2PC**: Strong consistency, but blocking and slow
3. **Saga**: Eventual consistency, compensating transactions
4. **Outbox pattern**: Reliable event publishing with local transaction

**Code example for Saga**:
```python
async def create_order_saga(order):
    try:
        order_id = await order_service.create(order)
        await inventory_service.reserve(order.items)
        await payment_service.charge(order.payment)
        await order_service.confirm(order_id)
    except PaymentFailed:
        await inventory_service.release(order.items)
        await order_service.cancel(order_id)
```
</Accordion>

<Accordion title="How do you debug a latency spike in a distributed system?">
**Systematic approach**:
1. **Observe**: Check metrics dashboards (p99 latency by service)
2. **Trace**: Use distributed tracing (Jaeger/Zipkin) to find slow span
3. **Correlate**: Check if spike correlates with deployments, traffic, or GC
4. **Drill down**: Once you find the service, check:
   - CPU/memory usage
   - DB query times (slow query log)
   - Network latency between services
   - Thread pool saturation
   - Lock contention

**Common causes**: DB slow queries, GC pauses, connection pool exhaustion, lock contention, network issues
</Accordion>

<Accordion title="How would you design a system that handles 1M requests per second?">
**Approach**:
1. **Back of envelope**: 1M RPS = ~60K servers at 16 RPS each (conservative)
2. **Stateless compute**: Horizontal scaling with load balancer
3. **Caching**: Cache everything possible (aim for 99%+ cache hit)
4. **CDN**: Serve static content from edge
5. **Database**: Shard aggressively, read replicas
6. **Async**: Queue non-critical work

**Bottleneck analysis**:
- Network: 1M Ã— 10KB = 10GB/s = 80Gbps (need multiple LBs)
- Compute: 1M / 10K (RPS per server) = 100 servers minimum
- Database: Can't hit DB for every request, need 99%+ cache hit
</Accordion>
