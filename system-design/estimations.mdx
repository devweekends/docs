---
title: "Back-of-Envelope Calculations"
description: "Essential estimation skills for system design interviews"
icon: "calculator"
---

## Why Estimation Matters

In system design interviews, you're expected to:

- **Size your system** - How much storage? How many servers?
- **Identify bottlenecks** - Where will the system break?
- **Make trade-offs** - Is this worth the complexity?
- **Validate assumptions** - Does this approach even work?

<Note>
**Don't aim for precision.** Round numbers aggressively. The goal is order of magnitude, not exact values. 86,400 seconds ≈ 100,000 is perfectly fine.
</Note>

## Essential Numbers to Memorize

### Time & Scale

| Duration | Seconds | Rounded |
|----------|---------|---------|
| 1 second | 1 | 1 |
| 1 minute | 60 | ~100 |
| 1 hour | 3,600 | ~4,000 |
| 1 day | 86,400 | ~100,000 |
| 1 month | 2,592,000 | ~2.5 million |
| 1 year | 31,536,000 | ~30 million |

### Data Units

| Unit | Bytes | Power of 2 |
|------|-------|------------|
| 1 KB | 1,000 | 2^10 ≈ 1,000 |
| 1 MB | 1,000,000 | 2^20 ≈ 1 million |
| 1 GB | 10^9 | 2^30 ≈ 1 billion |
| 1 TB | 10^12 | 2^40 ≈ 1 trillion |
| 1 PB | 10^15 | 2^50 |

### Latency Numbers

```
┌─────────────────────────────────────────────────────────────────┐
│                    Latency Comparison (2024)                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  L1 cache reference                    0.5 ns   ■               │
│  L2 cache reference                    7 ns     ■               │
│  RAM reference                         100 ns   ██              │
│  SSD random read                       150 µs   ████████        │
│  HDD seek                              10 ms    ████████████    │
│  Network (same datacenter)             0.5 ms   ████            │
│  Network (cross-continent)             150 ms   █████████████   │
│                                                                 │
│  Rule of thumb:                                                 │
│  • Memory is ~100x faster than SSD                             │
│  • SSD is ~100x faster than HDD                                │
│  • Same DC network is ~300x faster than cross-continent        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Availability Numbers

| Availability | Downtime/Year | Downtime/Month | Downtime/Week |
|-------------|---------------|----------------|---------------|
| 99% (two 9s) | 3.65 days | 7.3 hours | 1.68 hours |
| 99.9% (three 9s) | 8.76 hours | 43.8 min | 10.1 min |
| 99.99% (four 9s) | 52.6 min | 4.38 min | 1.01 min |
| 99.999% (five 9s) | 5.26 min | 26.3 sec | 6.05 sec |

## Common Calculation Patterns

### Pattern 1: QPS from Daily Active Users

```
┌─────────────────────────────────────────────────────────────────┐
│                 DAU → QPS Calculation                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Given:                                                         │
│  • 100 million DAU (Daily Active Users)                        │
│  • Average user makes 10 requests per day                      │
│                                                                 │
│  Total requests per day:                                        │
│  = 100M × 10 = 1 billion requests/day                          │
│                                                                 │
│  Average QPS (Queries Per Second):                             │
│  = 1B / 86,400 seconds                                         │
│  = 1B / 100K (rounded)                                         │
│  = 10,000 QPS                                                  │
│                                                                 │
│  Peak QPS (typically 2-3x average):                            │
│  = 10,000 × 2.5 = 25,000 QPS                                  │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  Quick Formula:                                                 │
│  QPS ≈ DAU × requests_per_user / 100,000                       │
│  Peak QPS ≈ QPS × 2.5                                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Pattern 2: Storage Estimation

```
┌─────────────────────────────────────────────────────────────────┐
│                 Storage Calculation                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Example: Twitter-like service                                  │
│                                                                 │
│  Given:                                                         │
│  • 500 million users                                           │
│  • 20% are daily active (100M DAU)                             │
│  • Average 2 tweets per active user per day                    │
│  • Tweet = 140 chars (280 bytes) + 200 bytes metadata          │
│                                                                 │
│  Daily tweet storage:                                           │
│  = 100M users × 2 tweets × 500 bytes                           │
│  = 200M × 500 bytes                                            │
│  = 100 GB/day                                                  │
│                                                                 │
│  Annual storage (text only):                                   │
│  = 100 GB × 365                                                │
│  = 36.5 TB/year                                                │
│                                                                 │
│  With media (assume 10% of tweets have 2MB image):            │
│  = 100M × 2 × 0.1 × 2MB = 40 TB/day                           │
│  = 40 TB × 365 = 14.6 PB/year                                 │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  Key insight: Media dominates text storage by 100x+            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Pattern 3: Bandwidth Estimation

```
┌─────────────────────────────────────────────────────────────────┐
│                 Bandwidth Calculation                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Example: Video streaming service (Netflix-like)               │
│                                                                 │
│  Given:                                                         │
│  • 200 million subscribers                                     │
│  • 50% watch something daily (100M daily viewers)              │
│  • Average 2 hours of video per viewer                         │
│  • Video bitrate: 5 Mbps (1080p average)                       │
│                                                                 │
│  Peak concurrent viewers (assume 10% of daily):                │
│  = 100M × 0.1 = 10 million concurrent                          │
│                                                                 │
│  Peak bandwidth:                                                │
│  = 10M viewers × 5 Mbps                                        │
│  = 50 million Mbps                                             │
│  = 50 Tbps (Terabits per second)                               │
│                                                                 │
│  Daily data transfer:                                           │
│  = 100M viewers × 2 hours × 3600 sec × 5 Mbps                  │
│  = 100M × 7200 × 5 Mb                                          │
│  = 3.6 × 10^12 Mb = 3.6 Petabits = 450 PB/day                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Pattern 4: Server Capacity

```
┌─────────────────────────────────────────────────────────────────┐
│                 Server Capacity Estimation                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Rule of thumb for web servers:                                │
│  • 1 server can handle ~1000 concurrent connections            │
│  • 1 server can handle ~500-1000 QPS for simple API           │
│  • 1 server can handle ~100-200 QPS for complex operations    │
│                                                                 │
│  Example: 50,000 QPS API service                               │
│                                                                 │
│  Servers needed:                                                │
│  = 50,000 QPS / 500 QPS per server                             │
│  = 100 servers                                                  │
│                                                                 │
│  With 3x capacity buffer (for spikes + failures):              │
│  = 100 × 3 = 300 servers                                       │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  Memory sizing:                                                 │
│  • 100K concurrent users, 10KB session data each               │
│  • Memory = 100K × 10KB = 1GB                                  │
│  • Per server (assume 10 servers): 100MB each                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Pattern 5: Cache Sizing (80/20 Rule)

```
┌─────────────────────────────────────────────────────────────────┐
│                 Cache Sizing with 80/20 Rule                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Principle: 20% of data serves 80% of requests                 │
│                                                                 │
│  Example: E-commerce product catalog                           │
│                                                                 │
│  Given:                                                         │
│  • 10 million products                                         │
│  • Average product data: 5 KB                                  │
│  • Total catalog: 10M × 5KB = 50 GB                            │
│                                                                 │
│  Cache 20% of products:                                        │
│  = 50 GB × 0.2 = 10 GB cache                                   │
│                                                                 │
│  Expected cache hit rate: ~80%                                 │
│                                                                 │
│  ─────────────────────────────────────────────────────────────  │
│                                                                 │
│  Alternative: Time-based estimation                            │
│                                                                 │
│  • 1 million requests/day to product pages                     │
│  • 70% are repeat views (dedup = 300K unique)                  │
│  • Cache last 24 hours of views                                │
│  • Cache size = 300K × 5KB = 1.5 GB                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Complete Example: URL Shortener

Let's walk through a complete estimation for a URL shortener like bit.ly.

### Requirements & Assumptions

```
┌─────────────────────────────────────────────────────────────────┐
│                 URL Shortener Estimation                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Functional:                                                    │
│  • Shorten long URLs → short URLs                              │
│  • Redirect short URLs → original URLs                         │
│  • Analytics (optional)                                        │
│                                                                 │
│  Non-functional:                                                │
│  • 100 million new URLs per month                              │
│  • 10:1 read-to-write ratio                                    │
│  • 5 year data retention                                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Traffic Estimation

```python
# Write traffic
new_urls_per_month = 100_000_000
new_urls_per_second = 100_000_000 / (30 * 24 * 3600)
# ≈ 100M / 2.5M = 40 URLs/second

# Read traffic (10:1 ratio)
redirects_per_second = 40 * 10 = 400 QPS

# Peak traffic (3x average)
peak_write_qps = 40 * 3 = 120 QPS
peak_read_qps = 400 * 3 = 1200 QPS
```

### Storage Estimation

```python
# URL data
original_url_size = 500  # bytes average
short_url_size = 7       # characters (base62)
metadata_size = 100      # bytes (created_at, user_id, etc.)
total_per_url = 500 + 7 + 100 = ~600 bytes

# Monthly storage
monthly_storage = 100_000_000 * 600 = 60 GB/month

# 5 year storage
total_storage = 60 GB * 12 * 5 = 3.6 TB

# With 2x replication
replicated_storage = 3.6 * 2 = 7.2 TB
```

### Short URL Length

```python
# How many URLs can we encode?
# Using base62 (a-z, A-Z, 0-9)

# 62^6 = 56 billion combinations
# 62^7 = 3.5 trillion combinations

# We need: 100M/month × 12 × 5 = 6 billion URLs

# 7 characters is sufficient (3.5 trillion >> 6 billion)
```

### Bandwidth Estimation

```python
# Write bandwidth
write_bandwidth = 40 * 600 bytes = 24 KB/s

# Read bandwidth
# Redirect is small response (Location header)
read_bandwidth = 400 * 200 bytes = 80 KB/s

# Minimal bandwidth, not a concern
```

### Memory (Cache) Estimation

```python
# Cache hot URLs for fast redirects
# 80/20 rule: 20% URLs get 80% traffic

daily_reads = 400 * 86400 = 34.5 million reads
# Assume 30% unique URLs accessed daily
unique_daily = 34.5M * 0.3 = 10 million URLs

# Cache size
cache_size = 10_000_000 * 600 bytes = 6 GB

# Redis can easily handle this
```

### Summary

```
┌─────────────────────────────────────────────────────────────────┐
│                 URL Shortener - Final Numbers                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Traffic:                                                       │
│  • Write: 40 QPS (peak: 120 QPS)                               │
│  • Read: 400 QPS (peak: 1200 QPS)                              │
│                                                                 │
│  Storage:                                                       │
│  • 3.6 TB over 5 years                                         │
│  • 7.2 TB with replication                                     │
│                                                                 │
│  Cache:                                                         │
│  • 6 GB Redis cache                                            │
│                                                                 │
│  Key Design Decisions:                                          │
│  • 7 character short URLs (base62)                             │
│  • Read-heavy → cache aggressively                             │
│  • Single database instance is sufficient                      │
│  • 2-3 application servers for redundancy                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Complete Example: Twitter Timeline

### Requirements

```
┌─────────────────────────────────────────────────────────────────┐
│                 Twitter Timeline Estimation                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Given:                                                         │
│  • 500 million total users                                     │
│  • 200 million DAU                                             │
│  • Average user follows 200 people                             │
│  • 10% of users post daily (20M tweets/day)                    │
│  • Average user checks timeline 5 times/day                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Timeline Generation QPS

```python
# Timeline reads
timeline_reads_per_day = 200_000_000 * 5 = 1 billion/day
timeline_qps = 1_000_000_000 / 86400 ≈ 11,600 QPS

# Tweet writes
tweet_writes_per_day = 20_000_000
tweet_qps = 20_000_000 / 86400 ≈ 230 QPS

# Ratio: 50:1 (heavily read-oriented)
```

### Fan-out Calculation

```
┌─────────────────────────────────────────────────────────────────┐
│                 Fan-out on Write vs Read                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Fan-out on Write (push model):                                │
│  ────────────────────────────                                   │
│  • New tweet → push to all followers' timelines                │
│  • 230 QPS × 200 followers = 46,000 cache writes/sec           │
│                                                                 │
│  Problem: Celebrity with 50M followers                         │
│  • 1 tweet = 50 million cache writes!                          │
│  • Takes minutes to propagate                                  │
│                                                                 │
│  Solution: Hybrid approach                                      │
│  • Small accounts: Fan-out on write                            │
│  • Celebrities (>10K followers): Fan-out on read               │
│                                                                 │
│  Fan-out on Read (pull model):                                 │
│  ───────────────────────────                                    │
│  • User opens app → query all followees for recent tweets      │
│  • 11,600 QPS × 200 followees = 2.3M queries/sec              │
│  • Too expensive at read time!                                 │
│                                                                 │
│  Hybrid is the answer                                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Storage for Timelines

```python
# Pre-computed timeline storage (fan-out on write)
# Store last 800 tweet IDs per user

timeline_size = 800 * 8 bytes (tweet ID) = 6.4 KB per user
total_timeline_storage = 500_000_000 * 6.4 KB = 3.2 TB

# Redis cluster with 3.2 TB RAM
# Or multiple Redis instances (10 × 320 GB)
```

## Estimation Cheat Sheet

```
┌─────────────────────────────────────────────────────────────────┐
│                 Quick Reference Formulas                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  DAU to QPS:                                                    │
│  QPS = DAU × requests_per_user / 100,000                       │
│                                                                 │
│  Peak QPS:                                                      │
│  Peak = Average × 2.5 (or ×3 for social)                       │
│                                                                 │
│  Storage:                                                       │
│  Daily = DAU × actions × data_size                             │
│  Yearly = Daily × 365                                          │
│                                                                 │
│  Bandwidth:                                                     │
│  BW = QPS × response_size                                      │
│                                                                 │
│  Servers:                                                       │
│  Count = QPS / QPS_per_server × 3 (buffer)                     │
│                                                                 │
│  Cache:                                                         │
│  Size = working_set_size × 0.2 (80/20 rule)                    │
│                                                                 │
│  URL length (base62):                                          │
│  62^n > total_items_expected                                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Interview Tips

<Tip>
**Show your work**: Write down assumptions clearly. State "assuming 100K seconds in a day" before calculating.

**Round aggressively**: Use powers of 10. 86,400 → 100,000 is fine.

**Sanity check**: Does the answer make sense? 1 million GB is suspicious.

**Ask about scale**: "Are we designing for 1M or 100M users?" This changes everything.

**Know your powers**: 2^10 ≈ 1000, 2^20 ≈ 1M, 2^30 ≈ 1B, 2^40 ≈ 1T
</Tip>
