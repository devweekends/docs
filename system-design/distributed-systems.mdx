---
title: "Distributed Systems"
description: "Consistency, Consensus, and handling failures in distributed systems"
icon: "share-nodes"
---

<img
  className="block rounded-lg"
  src="/images/system-design/distributed-systems.png"
  alt="Distributed Systems"
/>

## What are Distributed Systems?

A distributed system is a collection of independent computers that appear to users as a single coherent system.

### Why Distributed?

<CardGroup cols={2}>
  <Card title="Scalability" icon="chart-line">
    Handle more load than a single machine
  </Card>
  <Card title="Reliability" icon="shield">
    Survive individual machine failures
  </Card>
  <Card title="Latency" icon="bolt">
    Serve users from nearby locations
  </Card>
  <Card title="Compliance" icon="globe">
    Data locality requirements
  </Card>
</CardGroup>

## The Eight Fallacies

Things developers wrongly assume about networks:

1. **The network is reliable** → Packets get lost
2. **Latency is zero** → Cross-region calls take 100+ ms
3. **Bandwidth is infinite** → Large payloads slow things down
4. **The network is secure** → Always encrypt
5. **Topology doesn't change** → Servers come and go
6. **There is one administrator** → Multiple teams, policies
7. **Transport cost is zero** → Data transfer costs money
8. **The network is homogeneous** → Different hardware everywhere

## Consistency Models

### Strong Consistency

All nodes see the same data at the same time.

```
Write: x = 5
        │
        ▼
┌───────────────┐     ┌───────────────┐
│   Node A      │────►│   Node B      │
│   x = 5       │ ACK │   x = 5       │
└───────────────┘     └───────────────┘
        │
        ▼
Return to client (only after ALL nodes updated)

Read from any node: x = 5 ✓
```

### Eventual Consistency

All nodes will eventually have the same data.

```
Write: x = 5
        │
        ▼
┌───────────────┐     ┌───────────────┐
│   Node A      │─ ─ ►│   Node B      │
│   x = 5       │async│   x = 1 (old) │
└───────────────┘     └───────────────┘
        │
        ▼
Return immediately

Read from Node B: x = 1 ✗ (temporarily stale)
Later read: x = 5 ✓ (eventually consistent)
```

### Consistency Levels

| Level | Description | Trade-off |
|-------|-------------|-----------|
| **Strong** | All reads see latest write | High latency |
| **Eventual** | Reads may be stale | Low latency |
| **Causal** | Respects cause-effect | Medium |
| **Read-your-writes** | See your own writes | Good UX |
| **Session** | Consistency within session | Practical |

## Consensus Algorithms

How do distributed nodes agree on a value?

### Paxos (Simplified)

```
┌─────────┐     ┌─────────┐     ┌─────────┐
│Proposer │     │Acceptor │     │Acceptor │
│         │     │    1    │     │    2    │
└────┬────┘     └────┬────┘     └────┬────┘
     │               │               │
     │  1. Prepare(n)│               │
     │──────────────►│               │
     │──────────────────────────────►│
     │               │               │
     │  2. Promise   │               │
     │◄──────────────│               │
     │◄──────────────────────────────│
     │               │               │
     │  3. Accept(n, value)          │
     │──────────────►│               │
     │──────────────────────────────►│
     │               │               │
     │  4. Accepted  │               │
     │◄──────────────│               │
     │◄──────────────────────────────│
```

### Raft (Easier to Understand)

```
┌─────────────────────────────────────────────────────────────┐
│                     Raft Consensus                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. Leader Election                                         │
│     • Nodes start as followers                             │
│     • Timeout → become candidate                           │
│     • Get majority votes → become leader                   │
│                                                             │
│  2. Log Replication                                         │
│     • Leader receives writes                               │
│     • Leader replicates to followers                       │
│     • Commit when majority acknowledges                     │
│                                                             │
│  3. Safety                                                  │
│     • Only nodes with complete logs can be leaders         │
│     • Committed entries never lost                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘

┌────────┐         ┌────────┐         ┌────────┐
│ Leader │────────►│Follower│         │Follower│
│  ★     │         │        │◄────────│        │
└────────┘         └────────┘         └────────┘
    │                  │                  │
    │   Heartbeats + Log Replication      │
    └──────────────────┴──────────────────┘
```

## Distributed Transactions

### Two-Phase Commit (2PC)

```
┌────────────────┐
│  Coordinator   │
└───────┬────────┘
        │
        │  Phase 1: PREPARE
        │  "Can you commit?"
        │
   ┌────┼────┐
   │    │    │
   ▼    ▼    ▼
┌────┐┌────┐┌────┐
│ P1 ││ P2 ││ P3 │
└────┘└────┘└────┘
   │    │    │
   │ YES│YES │YES
   └────┼────┘
        │
        │  Phase 2: COMMIT
        │  (or ABORT if any NO)
        │
   ┌────┼────┐
   ▼    ▼    ▼
┌────┐┌────┐┌────┐
│ P1 ││ P2 ││ P3 │
└────┘└────┘└────┘

Problem: Coordinator fails between phases → participants stuck
```

### Saga Pattern

```
┌─────────────────────────────────────────────────────────────┐
│                    Order Saga                               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Forward Path (on success):                                 │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐       │
│  │ Create  │─►│ Reserve │─►│ Process │─►│ Fulfill │       │
│  │ Order   │  │Inventory│  │ Payment │  │  Order  │       │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘       │
│                                                             │
│  Compensating Path (on failure):                            │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐                     │
│  │ Cancel  │◄─│ Release │◄─│  Refund │  (if payment fails) │
│  │ Order   │  │Inventory│  │ Payment │                     │
│  └─────────┘  └─────────┘  └─────────┘                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Choreography vs Orchestration

```
Choreography (Event-driven)

Order ─► OrderCreated event
              │
    ┌─────────┼─────────┐
    ▼         ▼         ▼
Inventory  Payment  Shipping
    │         │         │
    └─────────┴─────────┘
          Events

• No central controller
• Services react to events
• More decoupled


Orchestration (Command-driven)

    ┌───────────────────┐
    │   Saga Manager    │
    └─────────┬─────────┘
              │
    ┌─────────┼─────────┐
    ▼         ▼         ▼
Inventory  Payment  Shipping
    
• Central controller
• Explicit flow control
• Easier to understand
```

## Handling Failures

### Circuit Breaker Pattern

```
┌─────────────────────────────────────────────────────────────┐
│                    Circuit Breaker                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│     CLOSED                    OPEN                          │
│   (Normal)                (Failing Fast)                    │
│   ┌─────────┐             ┌─────────┐                      │
│   │ Requests│             │ Fail    │                      │
│   │ pass    │             │ fast    │                      │
│   └────┬────┘             └────┬────┘                      │
│        │                       │                            │
│        │ Failures > threshold  │ Timeout expires           │
│        └──────────────────────►│◄──────────────┐           │
│        ◄───────────────────────┘               │           │
│        │                                        │           │
│        │                  HALF-OPEN            │           │
│        │                (Testing)              │           │
│        │                ┌─────────┐            │           │
│        │                │ Allow   │            │           │
│        └────────────────│ some    │────────────┘           │
│         Success         │requests │  Failure               │
│                         └─────────┘                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.state = "CLOSED"
        self.last_failure_time = None
    
    def call(self, func):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
            else:
                raise CircuitOpenException()
        
        try:
            result = func()
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise
    
    def on_success(self):
        self.failure_count = 0
        self.state = "CLOSED"
    
    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
```

### Retry with Exponential Backoff

```python
import time
import random

def retry_with_backoff(func, max_retries=5, base_delay=1):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            
            # Exponential backoff with jitter
            delay = base_delay * (2 ** attempt)
            jitter = random.uniform(0, delay * 0.1)
            time.sleep(delay + jitter)
            
            print(f"Retry {attempt + 1}/{max_retries} after {delay:.2f}s")
```

### Bulkhead Pattern

Isolate failures to prevent cascade.

```
┌─────────────────────────────────────────────────────────────┐
│                    Bulkhead Pattern                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Without Bulkhead:                                          │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Shared Thread Pool (100)               │   │
│  │   Service A calls + Service B calls + Service C     │   │
│  │   If B is slow, exhausts all threads → A, C fail   │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  With Bulkhead:                                             │
│  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐          │
│  │ Service A   │ │ Service B   │ │ Service C   │          │
│  │ Pool (30)   │ │ Pool (40)   │ │ Pool (30)   │          │
│  └─────────────┘ └─────────────┘ └─────────────┘          │
│  If B is slow, only B's pool exhausted → A, C continue     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

## Key Takeaways

| Concept | Remember |
|---------|----------|
| **CAP Theorem** | Pick 2 of 3: Consistency, Availability, Partition Tolerance |
| **Consensus** | Use Raft for leader election, state machine replication |
| **Transactions** | 2PC for strong consistency, Sagas for microservices |
| **Failures** | Design for failure: retries, circuit breakers, bulkheads |

<Warning>
**Distributed systems are hard.** Every network call can fail, be slow, or return stale data. Design for failure from day one.
</Warning>
