---
title: "SRE & Production Excellence"
description: "Master Site Reliability Engineering, SLIs/SLOs/SLAs, error budgets, chaos engineering, and production operations"
icon: "shield-heart"
---

# SRE & Production Excellence

Learn how Google's Site Reliability Engineering practices apply to Azure, from setting SLOs to running chaos experiments.

![SRE Pyramid](/images/azure/17-sre-pyramid.svg)

> [!TIP]
> **Jargon Alert: SLI vs SLO vs SLA**
> **SLI (Indicator)**: What you measure (e.g., latency = 250ms)
> **SLO (Objective)**: Your internal goal (e.g., p95 latency < 300ms)
> **SLA (Agreement)**: Your customer promise (e.g., 99.9% uptime or money back)
> SLO should be tighter than SLA to give you a buffer!

> [!WARNING]
> **Gotcha: 99.9% vs 99.99% Availability**
> The difference is NOT 0.09%‚Äîit's **10x more downtime**!
> 99.9% = 43 minutes/month downtime
> 99.99% = 4.3 minutes/month downtime
> Achieving 99.99% can cost 10x more. Choose based on business impact, not ego.

---

## 1. The SRE Mindset

**SRE** = What happens when you ask a software engineer to design operations.

### Core SRE Principles

<CardGroup cols={2}>
  <Card title="Embrace Risk" icon="dice">
    100% reliability is impossible and wasteful. Find the right balance between reliability and velocity.
  </Card>

  <Card title="Service Level Objectives" icon="bullseye">
    Define clear, measurable reliability targets. Make data-driven decisions.
  </Card>

  <Card title="Eliminate Toil" icon="robot">
    Automate repetitive manual work. Toil is the enemy of scaling.
  </Card>

  <Card title="Monitor Everything" icon="chart-line">
    You can't improve what you don't measure. Observability is mandatory.
  </Card>

  <Card title="Error Budgets" icon="wallet">
    Use remaining error budget to balance innovation and stability.
  </Card>

  <Card title="Blameless Postmortems" icon="magnifying-glass">
    Learn from failures without blaming individuals. Focus on systemic improvements.
  </Card>
</CardGroup>

---

## 2. Service Level Indicators (SLIs)

**SLIs** are carefully chosen metrics that represent the health of your service from the user's perspective.

### The Golden Signals (Google SRE)

<Tabs>
  <Tab title="Latency">
    **How long does a request take?**

    ```kusto
    // Azure Application Insights - Latency SLI
    requests
    | where timestamp > ago(24h)
    | where success == true
    | summarize
        p50 = percentile(duration, 50),
        p95 = percentile(duration, 95),
        p99 = percentile(duration, 99)
        by bin(timestamp, 5m), name
    | render timechart
    ```

    **Good SLI**:
    - p95 latency < 300ms (fast)
    - p99 latency < 1000ms (acceptable)

    **Why percentiles?**
    - Average hides outliers (1ms + 10,000ms = 5,000ms average, useless!)
    - p95 = 95% of users have this experience or better
  </Tab>

  <Tab title="Traffic">
    **How many requests per second?**

    ```kusto
    // Requests per second
    requests
    | where timestamp > ago(1h)
    | summarize count() by bin(timestamp, 1m)
    | extend rps = count_ / 60
    | render timechart
    ```

    **Good SLI**:
    - Current traffic: 5,000 RPS
    - Peak capacity: 10,000 RPS
    - Headroom: 100% (good!)
  </Tab>

  <Tab title="Errors">
    **What percentage of requests fail?**

    ```kusto
    // Error rate SLI
    requests
    | where timestamp > ago(24h)
    | summarize
        total = count(),
        failed = countif(success == false)
        by bin(timestamp, 5m)
    | extend errorRate = (failed * 100.0) / total
    | render timechart
    ```

    **Good SLI**:
    - Error rate < 0.1% (99.9% success)

    **Error classification**:
    - 4xx errors: Client errors (not counted against SLO)
    - 5xx errors: Server errors (counted!)
  </Tab>

  <Tab title="Saturation">
    **How full is your service?**

    ```kusto
    // CPU saturation
    performanceCounters
    | where timestamp > ago(1h)
    | where name == "% Processor Time"
    | summarize avg(value) by bin(timestamp, 5m), cloud_RoleInstance
    | render timechart

    // Memory saturation
    performanceCounters
    | where name == "Available Bytes"
    | summarize avg(value / 1024 / 1024 / 1024) by bin(timestamp, 5m)
    | render timechart
    ```

    **Good SLI**:
    - CPU < 70% (30% headroom)
    - Memory < 80%
    - Disk < 80%
  </Tab>
</Tabs>

### Choosing Good SLIs

**Bad SLI Examples**:
```
‚ùå "Server is up" (doesn't reflect user experience)
‚ùå "Average latency" (hides outliers)
‚ùå "Internal queue depth" (users don't care)
```

**Good SLI Examples**:
```
‚úÖ "95% of API requests complete in < 300ms"
‚úÖ "99.9% of requests return 2xx or 3xx"
‚úÖ "Page load completes in < 2 seconds (p95)"
‚úÖ "Search results returned in < 500ms (p99)"
```

---

## 3. Service Level Objectives (SLOs)

**SLOs** are internal reliability targets. They should be slightly tighter than your SLA.

### SLO Examples

<AccordionGroup>
  <Accordion title="E-Commerce Site" icon="cart-shopping">
    **SLIs & SLOs**:

    ```
    Latency:
    - SLI: p95 page load time
    - SLO: < 2 seconds
    - Why: Studies show 2s+ loads = 20% abandonment

    Availability:
    - SLI: Successful checkout rate
    - SLO: 99.9% (43 min downtime/month)
    - Why: Every minute down = $10K revenue lost

    Errors:
    - SLI: Payment processing error rate
    - SLO: < 0.01%
    - Why: Payment errors damage trust
    ```
  </Accordion>

  <Accordion title="API Service" icon="code">
    **SLIs & SLOs**:

    ```
    Latency:
    - SLI: p95 API response time
    - SLO: < 100ms
    - Why: Third-party integrations need fast responses

    Availability:
    - SLI: Successful API call rate
    - SLO: 99.95% (21 min downtime/month)
    - Why: Higher than web app (upstream dependency)

    Throughput:
    - SLI: Requests per second handled
    - SLO: 10,000 RPS sustained
    - Why: Contract guarantees 5,000 RPS average
    ```
  </Accordion>

  <Accordion title="Batch Processing System" icon="gears">
    **SLIs & SLOs**:

    ```
    Freshness:
    - SLI: Time from data arrival to processing completion
    - SLO: < 5 minutes (p95)
    - Why: Near real-time reporting requirement

    Correctness:
    - SLI: Percentage of jobs completing without errors
    - SLO: 99.99%
    - Why: Data quality is critical

    Throughput:
    - SLI: Events processed per hour
    - SLO: 100 million events/hour
    - Why: Peak load during business hours
    ```
  </Accordion>
</AccordionGroup>

---

## 4. Error Budgets

**Error Budget** = 100% - SLO

If your SLO is 99.9% availability, your error budget is 0.1% = 43.2 minutes/month.

### Error Budget Policy

<div className="flex justify-center my-4">
  <div className="w-full max-w-4xl">
    ```mermaid
    graph TD
        A[Check Error Budget] --> B{Budget Remaining?}
        B -->|> 50%| C[‚úÖ Ship Features<br/>Take Risks<br/>Accelerate]
        B -->|10-50%| D[‚ö†Ô∏è  Caution Mode<br/>Increase Testing<br/>Slow Down]
        B -->|< 10%| E[üö® Stop Releases<br/>Fix Reliability<br/>Pay Down Debt]

        C --> F[Monitor Closely]
        D --> F
        E --> G[Emergency Fixes Only]

        style C fill:#51CF66
        style D fill:#FFB900
        style E fill:#FF6B6B
    ```
  </div>
</div>

**Example Decision Making**:

```
Month: January
SLO: 99.9% availability (error budget = 0.1% = 43.2 minutes)

Week 1: 5 minutes downtime
  Remaining budget: 38.2 minutes (88%)
  ‚úÖ Status: GREEN - ship new features

Week 2: 15 minutes downtime (cumulative: 20 min)
  Remaining budget: 23.2 minutes (54%)
  ‚úÖ Status: GREEN - continue

Week 3: 20 minutes downtime (cumulative: 40 min)
  Remaining budget: 3.2 minutes (7%)
  üö® Status: RED - STOP feature releases
  Actions:
    - Cancel Friday deployment
    - Focus on reliability fixes
    - Root cause analysis of incidents
    - Increase monitoring

Week 4: 0 minutes downtime (cumulative: 40 min)
  Remaining budget: 3.2 minutes (7%)
  ‚ö†Ô∏è  Status: YELLOW - Careful releases only
  Actions:
    - Small, low-risk changes only
    - Extended bake time
    - Manual approval required
```

### Calculating Error Budget Burn Rate

```kusto
// Error budget burn rate (Azure Monitor)
let slo = 99.9; // 99.9% availability SLO
let errorBudget = 100 - slo; // 0.1%
let timeWindow = 30d; // Monthly budget

requests
| where timestamp > ago(timeWindow)
| summarize
    total = count(),
    failed = countif(success == false)
| extend
    actualAvailability = ((total - failed) * 100.0) / total,
    budgetUsed = 100 - actualAvailability,
    budgetRemaining = errorBudget - (100 - actualAvailability),
    burnRate = (100 - actualAvailability) / errorBudget // 1.0 = burning at expected rate
| project
    SLO = slo,
    ActualAvailability = round(actualAvailability, 2),
    ErrorBudget = errorBudget,
    BudgetUsed = round(budgetUsed, 4),
    BudgetRemaining = round(budgetRemaining, 4),
    BurnRate = round(burnRate, 2),
    Status = case(
        burnRate < 0.5, "GREEN - Under budget",
        burnRate < 1.0, "YELLOW - On track",
        burnRate < 2.0, "ORANGE - Over budget",
        "RED - Critical"
    )
```

---

## 5. Toil Reduction

**Toil** = Repetitive, manual, automatable work that scales linearly with service growth.

### What is Toil?

<Tabs>
  <Tab title="‚úÖ Toil Examples">
    ```
    ‚úÖ Manually restarting failed services
    ‚úÖ Provisioning new servers by hand
    ‚úÖ Copy-pasting database queries for reports
    ‚úÖ Manually scaling resources up/down
    ‚úÖ SSH into servers to check logs
    ‚úÖ Running the same kubectl commands daily
    ‚úÖ Manually updating configuration files
    ‚úÖ Responding to the same alerts with same fix
    ```

    **Characteristics**:
    - Manual
    - Repetitive
    - Automatable
    - Tactical (no long-term value)
    - Scales linearly with service size
  </Tab>

  <Tab title="‚ùå Not Toil">
    ```
    ‚ùå Writing automation scripts (engineering work)
    ‚ùå Planning capacity (strategic thinking)
    ‚ùå Incident response (novel problem-solving)
    ‚ùå Code reviews (knowledge sharing)
    ‚ùå Architecture design (creative work)
    ‚ùå Debugging new issues (requires expertise)
    ```

    **Characteristics**:
    - Requires human judgment
    - Novel problems
    - Strategic value
    - Knowledge creation
  </Tab>
</Tabs>

### Toil Elimination Strategy

<Steps>
  <Step title="Measure Toil">
    Track time spent on repetitive tasks

    ```
    Weekly time audit:
    - Manual deployments: 8 hours
    - Alert triage (false positives): 5 hours
    - Log searching: 4 hours
    - Manual scaling: 3 hours
    - Config updates: 2 hours

    Total toil: 22 hours/week (55% of time!)
    ```
  </Step>

  <Step title="Prioritize by ROI">
    Automate high-frequency, high-time tasks first

    ```
    Task                  Frequency    Time    Total/Week    ROI
    Manual deployments    10/week      0.5h    5h            High
    False alert triage    50/week      0.1h    5h            High
    Manual scaling        20/week      0.15h   3h            Medium
    Config updates        5/week       0.4h    2h            Low
    ```
  </Step>

  <Step title="Automate">
    Examples:

    ```bash
    # Before: Manual deployment (30 min)
    ssh server
    sudo systemctl stop app
    git pull
    dotnet publish
    sudo systemctl start app

    # After: CI/CD pipeline (2 min, no human)
    git push
    # GitHub Actions deploys automatically

    # Before: Manual scaling (20 min)
    # Check metrics, decide, update config, restart

    # After: Autoscaling (0 min, automatic)
    az monitor autoscale create \
      --resource-group rg-prod \
      --resource webapp \
      --min-count 2 \
      --max-count 10 \
      --count 2

    # Before: Searching logs (15 min per incident)
    ssh server
    tail -f /var/log/app.log | grep ERROR

    # After: Centralized logging (30 seconds)
    # KQL query in Application Insights
    traces | where severityLevel == 3 | take 100
    ```
  </Step>

  <Step title="Measure Improvement">
    Track toil reduction over time

    ```
    Q1: 55% time spent on toil
    Q2: 40% (automated deployments)
    Q3: 25% (added autoscaling)
    Q4: 15% (improved alerting)

    Time reclaimed: 20 hours/week
    Used for: Feature development, reliability improvements
    ```
  </Step>
</Steps>

---

## 6. Chaos Engineering

**Chaos Engineering** = Intentionally breaking things in production to build confidence.

### Chaos Principles

1. **Build a hypothesis** (e.g., "If we kill 30% of pods, requests should still succeed")
2. **Define steady state** (e.g., "Error rate < 1%, latency p95 < 500ms")
3. **Introduce chaos** (e.g., kill pods, inject latency, fail database)
4. **Measure deviation** (Did error rate spike? Did latency increase?)
5. **Learn and improve** (Add retries? Implement circuit breaker?)

### Azure Chaos Studio

**Setup**:

```bash
# Enable Chaos Studio on AKS cluster
az aks update \
  --resource-group rg-prod \
  --name aks-prod \
  --enable-chaos

# Create chaos experiment
az chaos experiment create \
  --name "pod-failure-experiment" \
  --resource-group rg-chaos \
  --location eastus \
  --identity SystemAssigned \
  --steps '[
    {
      "name": "Kill Random Pods",
      "branches": [
        {
          "name": "Kill 30% of pods",
          "actions": [
            {
              "type": "continuous",
              "name": "urn:csci:microsoft:azureKubernetesServiceChaosMesh:podChaos/2.1",
              "duration": "PT5M",
              "parameters": [
                {
                  "key": "jsonSpec",
                  "value": "{\"action\":\"pod-kill\",\"mode\":\"fixed-percent\",\"value\":\"30\"}"
                }
              ],
              "selectorId": "aks-cluster-target"
            }
          ]
        }
      ]
    }
  ]'

# Run experiment
az chaos experiment start \
  --name "pod-failure-experiment" \
  --resource-group rg-chaos
```

### Chaos Scenarios

<AccordionGroup>
  <Accordion title="1. Pod Failure (AKS)" icon="skull">
    **Hypothesis**: Application survives 30% pod failure

    **Test**:
    ```yaml
    # Using Chaos Mesh
    apiVersion: chaos-mesh.org/v1alpha1
    kind: PodChaos
    metadata:
      name: pod-kill-experiment
    spec:
      action: pod-kill
      mode: fixed-percent
      value: "30"
      selector:
        namespaces:
          - production
        labelSelectors:
          app: web-api
      scheduler:
        cron: "@every 1h"
    ```

    **Expected**: No user-visible errors (replicas take over)
    **Actual**: ?
    **Learnings**: Need faster readiness probes? Increase replica count?
  </Accordion>

  <Accordion title="2. Network Latency" icon="network-wired">
    **Hypothesis**: Application handles 500ms network latency

    **Test**:
    ```yaml
    apiVersion: chaos-mesh.org/v1alpha1
    kind: NetworkChaos
    metadata:
      name: network-delay
    spec:
      action: delay
      mode: one
      selector:
        namespaces:
          - production
        labelSelectors:
          app: payment-api
      delay:
        latency: "500ms"
        correlation: "100"
      duration: "5m"
    ```

    **Expected**: Requests timeout gracefully, circuit breaker opens
    **Actual**: ?
    **Learnings**: Add timeout? Implement retry with backoff?
  </Accordion>

  <Accordion title="3. CPU Stress" icon="microchip">
    **Hypothesis**: Autoscaling kicks in under CPU load

    **Test**:
    ```yaml
    apiVersion: chaos-mesh.org/v1alpha1
    kind: StressChaos
    metadata:
      name: cpu-stress
    spec:
      mode: one
      selector:
        namespaces:
          - production
        labelSelectors:
          app: worker
      stressors:
        cpu:
          workers: 4
          load: 100
      duration: "10m"
    ```

    **Expected**: HPA scales pods from 3 ‚Üí 10 within 2 minutes
    **Actual**: ?
    **Learnings**: Adjust HPA thresholds? Add more headroom?
  </Accordion>

  <Accordion title="4. Database Failover" icon="database">
    **Hypothesis**: App survives database failover

    **Test**:
    ```bash
    # Trigger Azure SQL failover
    az sql db failover \
      --resource-group rg-prod \
      --server sql-prod \
      --database mydb

    # Expected failover time: 30-60 seconds
    ```

    **Expected**: Brief spike in errors (< 1 minute), then recovery
    **Actual**: ?
    **Learnings**: Connection pool handles failover? Need retry logic?
  </Accordion>
</AccordionGroup>

### GameDay Exercise

**Scenario**: Black Friday Simulation

```
Timeline:
09:00 - Start load test (5,000 RPS)
09:15 - Kill 50% of web pods
09:20 - Inject 1s latency to payment service
09:25 - Trigger database failover
09:30 - Simulate CDN failure (disable Azure Front Door)
09:35 - End test

Metrics to Track:
- Error rate (target: < 1%)
- p95 latency (target: < 2s)
- Successful checkouts (target: > 99%)
- Revenue lost (target: < $1,000)

Team Roles:
- Incident Commander
- Operations (executes fixes)
- Communications (updates stakeholders)
- Observers (document learnings)

Post-GameDay:
- Blameless postmortem
- Action items with owners
- Update runbooks
```

---

## 7. Incident Management

### Incident Response Lifecycle

<div className="flex justify-center my-4">
  <div className="w-full max-w-4xl">
    ```mermaid
    graph LR
        A[üö® Alert Fires] --> B[Acknowledge]
        B --> C{Severity?}
        C -->|SEV1: Critical| D[Page On-Call]
        C -->|SEV2: Major| E[Create Incident]
        C -->|SEV3: Minor| F[Create Ticket]

        D --> G[Incident Commander Assigned]
        G --> H[Assemble Team]
        H --> I[Diagnose]
        I --> J[Mitigate]
        J --> K{Resolved?}
        K -->|No| I
        K -->|Yes| L[Monitor]
        L --> M[Postmortem]

        style A fill:#FF6B6B
        style L fill:#51CF66
    ```
  </div>
</div>

### Severity Levels

| Severity | Impact | Response Time | Example |
|----------|--------|---------------|---------|
| **SEV1** | Critical - Revenue loss | Page immediately | Payment system down, database offline |
| **SEV2** | Major - Degraded service | 15 minutes | API latency 10x higher, 10% error rate |
| **SEV3** | Minor - Partial impact | 1 hour | Single pod failing, non-critical feature down |
| **SEV4** | Cosmetic - No user impact | Next business day | UI typo, log noise |

### Blameless Postmortem Template

```markdown
# Incident Postmortem: Payment Service Outage

**Date**: 2026-01-20
**Duration**: 47 minutes
**Severity**: SEV1
**Impact**: $125K revenue lost, 15K failed transactions

## Timeline

09:42 - Alert fired: Payment API error rate 50%
09:43 - On-call engineer acknowledged
09:45 - Incident Commander assigned
09:47 - Root cause identified: Database connection pool exhausted
09:50 - Mitigation: Restarted API pods
09:55 - Error rate dropped to 5%
10:00 - Mitigation: Increased connection pool size
10:15 - Error rate back to normal (< 0.1%)
10:29 - Incident resolved

## Root Cause

Database connection pool configured for 100 connections.
Traffic spike (Black Friday sale) increased to 5,000 RPS.
Each request held connection for 500ms average.
Required connections: 5,000 * 0.5 = 2,500.

Result: Connection pool exhausted ‚Üí requests failed.

## What Went Well

‚úÖ Alert fired within 30 seconds
‚úÖ Clear escalation path
‚úÖ Team mobilized quickly
‚úÖ Mitigation identified in < 5 minutes

## What Went Wrong

‚ùå No capacity planning for Black Friday
‚ùå Connection pool size not monitored
‚ùå No autoscaling for connection pool
‚ùå Runbook didn't cover this scenario

## Action Items

1. [P0] Increase connection pool to 5,000 (@alice, Due: Jan 21)
2. [P0] Add connection pool saturation alert (@bob, Due: Jan 21)
3. [P1] Implement autoscaling for connection pool (@charlie, Due: Jan 25)
4. [P1] Update runbook with connection pool troubleshooting (@david, Due: Jan 27)
5. [P2] Load test before major events (@team, Due: Feb 1)

## Lessons Learned

- Monitor resource saturation, not just errors
- Load test before high-traffic events
- Connection pools are a finite resource
```

---

## 8. On-Call Best Practices

### On-Call Rotation

```
Team of 6 engineers
Rotation: 1 week on-call, 5 weeks off

Primary On-Call:
- Responds to all alerts
- Pages: 5-10 per week expected
- Compensation: $500/week stipend

Secondary On-Call:
- Backup if primary unreachable
- Takes over for escalations
- Compensation: $250/week stipend

Schedule:
Week 1: Alice (primary), Bob (secondary)
Week 2: Charlie (primary), David (secondary)
Week 3: Eve (primary), Frank (secondary)
...
```

### On-Call Expectations

**Response Times**:
- SEV1: 5 minutes
- SEV2: 15 minutes
- SEV3: 1 hour

**Escalation Path**:
```
1. Primary on-call (5 min)
   ‚Üì
2. Secondary on-call (10 min)
   ‚Üì
3. Engineering Manager (15 min)
   ‚Üì
4. VP Engineering (20 min)
```

**Handoff**:
```markdown
## On-Call Handoff: Alice ‚Üí Charlie

**Open Incidents**:
- INC-1234: Database latency spikes (SEV3, investigating)

**Ongoing Issues**:
- Redis cache hit rate dropped to 60% (normal: 90%)
- Monitoring this, no action needed yet

**Upcoming Maintenance**:
- SQL failover test scheduled for Wednesday 2am

**Tips**:
- If you see "connection timeout" errors, restart worker pods
- Database runbook: https://wiki.company.com/db-runbook
- Slack #oncall-help if you need guidance
```

---

## 9. Load Testing & Performance Testing

Testing your system under load is critical for understanding scalability limits and preventing outages.

### Load Testing vs Performance Testing

| Type | Purpose | When | Tools |
|------|---------|------|-------|
| **Load Testing** | Verify system handles expected load | Before launch, before traffic spikes | k6, JMeter, Azure Load Testing |
| **Stress Testing** | Find breaking point | Capacity planning | k6, Locust |
| **Spike Testing** | Handle sudden traffic bursts | Black Friday prep | k6, Gatling |
| **Soak Testing** | Detect memory leaks over time | After deployments | k6 (24+ hours) |
| **Performance Testing** | Measure response times | Every release | Azure Load Testing |

![k6 Load Test Stages](/images/azure/17-k6-load-stages.svg)

---

### Azure Load Testing

**Azure Load Testing** is a fully managed service for generating high-scale load.

#### Create Load Test

```bash
# Create Azure Load Testing resource
az load create \
  --name myloadtest \
  --resource-group rg-prod \
  --location eastus

# Upload JMeter test plan
az load test create \
  --load-test-resource myloadtest \
  --test-id checkout-load-test \
  --display-name "Checkout API Load Test" \
  --description "Test 1000 concurrent users" \
  --test-plan checkout-test.jmx \
  --engine-instances 10
```

#### JMeter Test Plan Example

```xml
<?xml version="1.0" encoding="UTF-8"?>
<jmeterTestPlan version="1.2">
  <hashTree>
    <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="Checkout Load Test">
      <elementProp name="TestPlan.user_defined_variables" elementType="Arguments">
        <collectionProp name="Arguments.arguments">
          <elementProp name="TARGET_URL" elementType="Argument">
            <stringProp name="Argument.name">TARGET_URL</stringProp>
            <stringProp name="Argument.value">${__P(TARGET_URL,https://api.example.com)}</stringProp>
          </elementProp>
        </collectionProp>
      </elementProp>
    </TestPlan>
    <hashTree>
      <ThreadGroup guiclass="ThreadGroupGui" testclass="ThreadGroup" testname="Users">
        <intProp name="ThreadGroup.num_threads">1000</intProp>
        <intProp name="ThreadGroup.ramp_time">60</intProp>
        <longProp name="ThreadGroup.duration">600</longProp>
      </ThreadGroup>
      <hashTree>
        <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="POST /checkout">
          <stringProp name="HTTPSampler.domain">${TARGET_URL}</stringProp>
          <stringProp name="HTTPSampler.path">/api/checkout</stringProp>
          <stringProp name="HTTPSampler.method">POST</stringProp>
          <boolProp name="HTTPSampler.auto_redirects">true</boolProp>
          <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
        </HTTPSamplerProxy>
      </hashTree>
    </hashTree>
  </hashTree>
</jmeterTestPlan>
```

---

### k6 Load Testing

**k6** is a modern, developer-friendly load testing tool.

#### Basic Load Test

```javascript
// load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 100 },   // Ramp up to 100 users
    { duration: '5m', target: 100 },   // Stay at 100 users
    { duration: '2m', target: 200 },   // Spike to 200 users
    { duration: '5m', target: 200 },   // Stay at 200 users
    { duration: '2m', target: 0 },     // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'],  // 95% of requests < 500ms
    http_req_failed: ['rate<0.01'],     // Error rate < 1%
  },
};

export default function () {
  const payload = JSON.stringify({
    userId: `user${__VU}`,
    cartId: `cart${__VU}`,
    items: [{ productId: 'prod123', quantity: 2 }],
  });

  const params = {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${__ENV.API_TOKEN}`,
    },
  };

  const res = http.post('https://api.example.com/checkout', payload, params);

  check(res, {
    'status is 200': (r) => r.status === 200,
    'checkout successful': (r) => r.json('success') === true,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });

  sleep(1);  // Think time: 1 second between requests
}
```

#### Run k6 Test

```bash
# Local test
k6 run load-test.js

# Cloud test (distributed load)
k6 cloud load-test.js

# With environment variables
k6 run -e API_TOKEN=$TOKEN load-test.js

# Output to InfluxDB + Grafana
k6 run --out influxdb=http://localhost:8086/k6 load-test.js
```

---

### Performance Testing Scenarios

#### Scenario 1: Black Friday Load Test

```javascript
// black-friday.js
export const options = {
  scenarios: {
    // Normal traffic (1000 RPS)
    normal_traffic: {
      executor: 'constant-arrival-rate',
      rate: 1000,
      timeUnit: '1s',
      duration: '1h',
      preAllocatedVUs: 500,
      maxVUs: 2000,
    },
    // Black Friday spike (10x traffic for 30 min)
    black_friday_spike: {
      executor: 'constant-arrival-rate',
      rate: 10000,
      timeUnit: '1s',
      duration: '30m',
      startTime: '1h',  // Start after normal traffic
      preAllocatedVUs: 5000,
      maxVUs: 20000,
    },
  },
  thresholds: {
    'http_req_duration{scenario:normal_traffic}': ['p(99)<1000'],
    'http_req_duration{scenario:black_friday_spike}': ['p(99)<2000'],
    'http_req_failed': ['rate<0.05'],  // 5% error budget during spike
  },
};
```

#### Scenario 2: Soak Test (Memory Leak Detection)

```javascript
// soak-test.js
export const options = {
  stages: [
    { duration: '5m', target: 100 },    // Ramp up
    { duration: '24h', target: 100 },   // Run for 24 hours
    { duration: '5m', target: 0 },      // Ramp down
  ],
};

export default function () {
  http.get('https://api.example.com/products');
  sleep(3);  // 3 seconds think time
}
```

**What to Monitor**:
- Memory usage (should be flat, not increasing)
- Connection pool size (should stabilize)
- Database connections (no leaks)
- HTTP response times (should not degrade)

---

### Azure Application Insights Performance Testing

#### Custom Telemetry for Load Tests

```csharp
// Program.cs
services.AddApplicationInsightsTelemetry(options =>
{
    options.EnableAdaptiveSampling = false;  // Disable during load tests
    options.DeveloperMode = false;
});

// LoadTestController.cs
[HttpGet("health")]
public IActionResult Health()
{
    var telemetry = new TelemetryClient();

    // Track custom metric during load test
    telemetry.TrackMetric("DatabaseConnectionPoolSize",
        GetConnectionPoolSize());

    telemetry.TrackMetric("MemoryUsageMB",
        GC.GetTotalMemory(false) / 1024 / 1024);

    return Ok(new { status = "healthy" });
}
```

#### KQL Query: Analyze Load Test Results

```kusto
// Performance during load test
requests
| where timestamp between(datetime(2026-01-21 10:00) .. datetime(2026-01-21 11:00))
| summarize
    P50 = percentile(duration, 50),
    P95 = percentile(duration, 95),
    P99 = percentile(duration, 99),
    RequestRate = count() / (max(timestamp) - min(timestamp)) / 1s,
    FailureRate = countif(success == false) * 100.0 / count()
    by bin(timestamp, 1m)
| project timestamp, P50, P95, P99, RequestRate, FailureRate
| render timechart
```

---

### Performance Testing Best Practices

#### 1. Realistic Test Data

```javascript
// Bad: Same user every time
const userId = 'user123';

// Good: Random users from pool
const users = ['user1', 'user2', 'user3', ...];  // 10,000 users
const userId = users[Math.floor(Math.random() * users.length)];

// Better: Unique user per VU
const userId = `user${__VU}`;
```

#### 2. Think Time (Realistic User Behavior)

```javascript
export default function () {
  // User lands on homepage
  http.get('https://example.com/');
  sleep(2);  // User reads content

  // User searches for product
  http.get('https://example.com/search?q=laptop');
  sleep(3);  // User browses results

  // User clicks product
  http.get('https://example.com/products/laptop-123');
  sleep(5);  // User reads reviews

  // User adds to cart
  http.post('https://example.com/cart/add', { productId: 'laptop-123' });
  sleep(1);
}
```

#### 3. Connection Pooling

```javascript
export const options = {
  batch: 20,  // Send 20 requests in parallel per VU
  batchPerHost: 6,  // Max 6 parallel connections per host (HTTP/1.1)
};
```

#### 4. Gradual Ramp-Up

```javascript
// Bad: Instant spike (thundering herd)
export const options = {
  vus: 1000,
  duration: '10m',
};

// Good: Gradual ramp-up
export const options = {
  stages: [
    { duration: '2m', target: 100 },
    { duration: '2m', target: 500 },
    { duration: '2m', target: 1000 },
    { duration: '10m', target: 1000 },
  ],
};
```

---

### Interpreting Load Test Results

#### Metrics to Track

| Metric | Good | Warning | Critical |
|--------|------|---------|----------|
| **P95 Latency** | &lt;500ms | 500-1000ms | &gt;1000ms |
| **Error Rate** | &lt;0.1% | 0.1-1% | &gt;1% |
| **Throughput** | Meets SLO | 80% of SLO | &lt;80% of SLO |
| **CPU Usage** | &lt;70% | 70-85% | &gt;85% |
| **Memory Usage** | Stable | Slow increase | Rapid increase |

#### Bottleneck Identification

```kusto
// Find slowest dependencies
dependencies
| where timestamp > ago(1h)
| summarize
    AvgDuration = avg(duration),
    P95Duration = percentile(duration, 95),
    Count = count()
    by target
| order by P95Duration desc
| take 10
```

**Common Bottlenecks**:
1. **Database queries** (most common) - Add indexes, optimize queries
2. **External API calls** - Add caching, circuit breakers
3. **CPU-intensive operations** - Move to background jobs
4. **Network latency** - Use CDN, co-locate services
5. **Memory allocation** - Reduce allocations, use object pooling

---

### Load Testing Checklist

Before running production load tests:

- [ ] **Test environment matches production** (same VM SKU, database tier, autoscaling rules)
- [ ] **Disable sampling** in Application Insights (100% telemetry during tests)
- [ ] **Notify team** (Slack #on-call: "Load test starting at 10 AM")
- [ ] **Monitor dashboards** open (Grafana, Application Insights Live Metrics)
- [ ] **Rollback plan ready** (know how to stop the test immediately)
- [ ] **Test data prepared** (realistic user IDs, product IDs, payment methods)
- [ ] **Success criteria defined** (P95 &lt;500ms, error rate &lt;1%, no memory leaks)
- [ ] **Baseline captured** (run test before changes, compare after)

> [!WARNING]
> **Gotcha: Testing in Production**
> Load testing in production is risky but sometimes necessary. If you must:
> - Use **synthetic users** (test-user-1, test-user-2) to isolate test data
> - Route test traffic to a **separate backend pool** (blue-green)
> - Start with **5% of production load**, increase gradually
> - Have **instant rollback** ready (kill switch)
> - **Never** test payment processing or critical user actions in production

---

## 10. Production Readiness Checklist

Before launching a new service to production:

```
‚úÖ Observability
  ‚úÖ Metrics instrumented (Golden Signals)
  ‚úÖ Logs centralized (Application Insights / Log Analytics)
  ‚úÖ Distributed tracing enabled
  ‚úÖ Dashboards created
  ‚úÖ Alerts configured with runbooks

‚úÖ Reliability
  ‚úÖ SLOs defined and measured
  ‚úÖ Error budget policy in place
  ‚úÖ Circuit breakers implemented
  ‚úÖ Retries with exponential backoff
  ‚úÖ Timeouts configured
  ‚úÖ Health checks (liveness & readiness)

‚úÖ Scalability
  ‚úÖ Load tested at 2x peak traffic
  ‚úÖ Autoscaling configured
  ‚úÖ Database connection pooling
  ‚úÖ Caching strategy
  ‚úÖ Rate limiting for external APIs

‚úÖ Security
  ‚úÖ Secrets in Key Vault (not in code)
  ‚úÖ Managed Identity for authentication
  ‚úÖ Network isolation (Private Endpoints)
  ‚úÖ WAF enabled
  ‚úÖ Security scanning in CI/CD

‚úÖ Operational
  ‚úÖ Runbooks documented
  ‚úÖ On-call rotation defined
  ‚úÖ Disaster recovery tested
  ‚úÖ Backup and restore verified
  ‚úÖ Chaos engineering experiments run

‚úÖ Cost
  ‚úÖ Cost estimation completed
  ‚úÖ Budgets and alerts set
  ‚úÖ Autoscaling to reduce waste
  ‚úÖ Reserved Instances for steady load
```

---

## 10. Interview Questions

### Beginner Level

<AccordionGroup>
  <Accordion title="Q1: What is the difference between SLI, SLO, and SLA?">
    **Answer**:

    **SLI (Service Level Indicator)**:
    - What you measure
    - Example: "p95 latency = 250ms"

    **SLO (Service Level Objective)**:
    - Your internal target
    - Example: "p95 latency < 300ms"
    - Guides engineering decisions

    **SLA (Service Level Agreement)**:
    - Customer promise with consequences
    - Example: "99.9% uptime or 10% refund"
    - Legal/financial agreement

    **Relationship**: SLI ‚â§ SLO ‚â§ SLA
  </Accordion>

  <Accordion title="Q2: What is an error budget?">
    **Answer**:

    Error budget = How much unreliability you can tolerate.

    **Calculation**:
    ```
    SLO: 99.9% availability
    Error budget: 100% - 99.9% = 0.1%

    Monthly: 30 days √ó 24 hours √ó 60 min = 43,200 min
    Error budget: 43,200 √ó 0.001 = 43.2 minutes downtime allowed
    ```

    **Use**:
    - Budget remaining ‚Üí ship features fast
    - Budget exhausted ‚Üí focus on reliability
  </Accordion>
</AccordionGroup>

### Intermediate Level

<AccordionGroup>
  <Accordion title="Q3: How do you measure and reduce toil?">
    **Answer**:

    **Measure**:
    1. Time audit: Track manual, repetitive tasks
    2. Calculate percentage of time spent on toil
    3. Goal: Keep toil < 50% (ideally < 30%)

    **Reduce**:
    1. Automate high-frequency tasks first
    2. Examples:
       - Manual deployments ‚Üí CI/CD
       - Manual scaling ‚Üí Autoscaling
       - Log searching ‚Üí Centralized logging
    3. Measure improvement quarterly

    **Not Toil**:
    - Novel problem solving
    - Architecture design
    - Incident response (new incidents)
  </Accordion>

  <Accordion title="Q4: Design an on-call rotation for a team of 6">
    **Answer**:

    **Rotation**:
    - Primary on-call: 1 week
    - Secondary on-call: 1 week
    - Rotation: 6 weeks per person

    **Schedule**:
    ```
    Week 1: Alice (P), Bob (S)
    Week 2: Charlie (P), David (S)
    Week 3: Eve (P), Frank (S)
    Week 4: Bob (P), Alice (S)
    Week 5: David (P), Charlie (S)
    Week 6: Frank (P), Eve (S)
    ```

    **Compensation**:
    - Primary: $500/week
    - Secondary: $250/week

    **Handoff Process**:
    - Written handoff document
    - 15-minute sync call
    - Slack thread with context
  </Accordion>
</AccordionGroup>

### Advanced Level

<AccordionGroup>
  <Accordion title="Q5: Implement an error budget policy">
    **Answer**:

    ```
    Service: E-Commerce Checkout
    SLO: 99.95% availability (21.6 min downtime/month)

    Error Budget Policy:

    GREEN (> 80% budget remaining):
    - Ship features daily
    - Canary deployments (10% traffic)
    - Automated rollouts
    - Chaos experiments allowed

    YELLOW (30-80% budget remaining):
    - Ship features 2x per week
    - Extended canary period (24 hours)
    - Manual approval for risky changes
    - Pause chaos experiments

    ORANGE (10-30% budget remaining):
    - Feature freeze (critical fixes only)
    - Extended testing period
    - Manual deployments
    - Root cause analysis required

    RED (< 10% budget remaining):
    - FULL STOP on features
    - Emergency fixes only
    - Incident review with leadership
    - Mandatory postmortems
    - Focus: Pay down reliability debt

    Reset: Monthly (first day of month)

    Escalation:
    - Budget hits YELLOW ‚Üí Notify team
    - Budget hits ORANGE ‚Üí Notify manager
    - Budget hits RED ‚Üí Notify VP Engineering
    ```
  </Accordion>

  <Accordion title="Q6: Design a chaos engineering program">
    **Answer**:

    ```text
    **Phase 1: Foundation (Month 1-2)**
    - Set up Azure Chaos Studio
    - Define blast radius (start with dev/staging)
    - Create baseline metrics dashboard
    - Get stakeholder buy-in

    **Phase 2: Simple Experiments (Month 3-4)**
    - Pod failures (10% ‚Üí 30% ‚Üí 50%)
    - Network latency (100ms ‚Üí 500ms ‚Üí 1s)
    - CPU stress (50% ‚Üí 80% ‚Üí 100%)
    - Run in staging, measure impact

    **Phase 3: Production Testing (Month 5-6)**
    - Start with off-peak hours
    - Small blast radius (single AZ)
    - Gradual rollout to full production
    - Always have kill switch

    **Phase 4: GameDays (Month 7+)**
    - Quarterly exercises
    - Simulate real outages
    - Test incident response
    - Cross-team coordination

    **Metrics to Track**:
    - MTTR (Mean Time To Recovery)
    - Blast radius (how many users affected)
    - Learnings per experiment
    - Action items completed

    **Culture**:
    - Blameless
    - Learning-focused
    - Celebrate finding weaknesses
    - Share results company-wide
    ```
  </Accordion>
</AccordionGroup>

---

## 11. Key Takeaways

<CardGroup cols={2}>
  <Card title="SLOs Drive Decisions" icon="bullseye">
    Define clear SLOs. Use error budgets to balance velocity and reliability.
  </Card>

  <Card title="Eliminate Toil" icon="robot">
    Automate everything repetitive. Spend time on engineering, not firefighting.
  </Card>

  <Card title="Break Things on Purpose" icon="hammer">
    Chaos engineering builds confidence. Test in production before customers do.
  </Card>

  <Card title="Blameless Culture" icon="heart">
    Focus on systems, not people. Learn from failures without punishment.
  </Card>

  <Card title="Monitor What Matters" icon="chart-line">
    User experience > server metrics. Latency, errors, saturation.
  </Card>

  <Card title="Operational Excellence" icon="star">
    Production readiness checklist before launch. On-call with clear expectations.
  </Card>
</CardGroup>

---

## Next Steps

<Card title="Back to Course Overview" icon="arrow-left" href="/courses/azure-cloud-engineering/00-overview">
  Return to course overview or continue to the Capstone Project
</Card>

<Card title="Continue to Chapter 15" icon="arrow-right" href="/courses/azure-cloud-engineering/15-capstone-project">
  Apply everything you've learned in the enterprise e-commerce capstone
</Card>
