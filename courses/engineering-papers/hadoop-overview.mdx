---
title: "Apache Hadoop"
sidebarTitle: "Hadoop Overview"
description: "Master the distributed computing framework that revolutionized big data processing and enabled the modern data ecosystem"
icon: "database"
---

# Apache Hadoop

A comprehensive deep-dive into Apache Hadoop—the open-source distributed computing framework that democratized big data processing and built upon the foundations laid by Google's GFS and MapReduce.

<Info>
**Course Duration**: 14-18 hours
**Level**: Intermediate to Advanced
**Prerequisites**: Basic distributed systems knowledge, understanding of MapReduce concepts
**Outcome**: Deep understanding of Hadoop architecture, HDFS, YARN, MapReduce, and ecosystem
</Info>

---

## Why Study Hadoop?

<CardGroup cols={2}>
  <Card title="Industry Standard" icon="industry">
    De facto big data platform. Powers data infrastructure at thousands of companies worldwide.
  </Card>
  <Card title="Interview Essential" icon="briefcase">
    Critical for data engineering and backend roles. Understanding Hadoop is essential for system design interviews.
  </Card>
  <Card title="Ecosystem Foundation" icon="lightbulb">
    Foundation for Spark, Hive, HBase, and modern data tools. Understanding Hadoop helps you master the entire ecosystem.
  </Card>
  <Card title="Real-World Scale" icon="chart-line">
    Learn how Yahoo, Facebook, and others process petabytes of data daily with commodity hardware.
  </Card>
</CardGroup>

---

## What You'll Learn

```text
┌─────────────────────────────────────────────────────────────┐
│                  HADOOP MASTERY                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│ Chapter 1: Introduction & Origins                          │
│ • Hadoop's relationship to GFS and MapReduce               │
│ • Why Hadoop was created                                    │
│ • Design goals and philosophy                              │
│ • Ecosystem overview                                        │
│                                                             │
│ Chapter 2: HDFS Architecture                                │
│ • NameNode and DataNode design                             │
│ • How HDFS differs from GFS                                │
│ • Block replication and placement                          │
│ • Read and write data flows                                │
│                                                             │
│ Chapter 3: MapReduce Framework                              │
│ • Programming model deep dive                               │
│ • Job execution and task management                        │
│ • Shuffle and sort mechanisms                              │
│ • Optimization techniques                                   │
│                                                             │
│ Chapter 4: YARN Resource Management                         │
│ • ResourceManager and NodeManager                           │
│ • Container-based execution                                 │
│ • Scheduling algorithms                                     │
│ • Multi-tenancy support                                     │
│                                                             │
│ Chapter 5: Hadoop Ecosystem                                 │
│ • Hive: SQL on Hadoop                                       │
│ • Pig: Data flow language                                   │
│ • HBase: NoSQL database                                     │
│ • Spark integration and beyond                             │
│                                                             │
│ Chapter 6: Fault Tolerance                                  │
│ • HDFS failure handling                                     │
│ • MapReduce task recovery                                   │
│ • NameNode HA configurations                               │
│ • Data integrity and checksums                             │
│                                                             │
│ Chapter 7: Performance & Tuning                             │
│ • Performance bottlenecks                                   │
│ • Optimization strategies                                   │
│ • Benchmarks and best practices                            │
│ • JVM tuning and compression                               │
│                                                             │
│ Chapter 8: Production Deployment                            │
│ • Cluster sizing and planning                               │
│ • Monitoring and alerting                                   │
│ • Security (Kerberos, encryption)                          │
│ • Real-world use cases                                      │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Key Concepts Covered

<AccordionGroup>
  <Accordion title="HDFS: Distributed File System" icon="hard-drive">
    Master HDFS architecture, how it implements GFS concepts in Java, and the differences between NameNode/DataNode and GFS Master/Chunkserver. Learn block replication strategies and fault tolerance mechanisms.
  </Accordion>

  <Accordion title="MapReduce: Programming Model" icon="code">
    Deep dive into the MapReduce programming paradigm, how jobs are executed across clusters, the shuffle and sort phase, and how to write efficient MapReduce applications.
  </Accordion>

  <Accordion title="YARN: Resource Management" icon="server">
    Understand how YARN decouples resource management from data processing, enabling Hadoop to run diverse workloads beyond MapReduce including Spark, Flink, and custom applications.
  </Accordion>

  <Accordion title="Fault Tolerance at Scale" icon="shield">
    Study how Hadoop handles failures in a cluster of thousands of nodes, including NameNode HA, automatic block re-replication, and speculative execution of tasks.
  </Accordion>

  <Accordion title="Data Locality" icon="location-dot">
    Learn how Hadoop optimizes computation by moving processing to data rather than data to processing, reducing network bandwidth and improving performance dramatically.
  </Accordion>

  <Accordion title="Ecosystem Integration" icon="puzzle-piece">
    Explore how Hive, Pig, HBase, Spark, and other tools build on Hadoop's foundation to provide SQL, streaming, real-time processing, and more.
  </Accordion>
</AccordionGroup>

---

## Who This Course Is For

<Tabs>
  <Tab title="Data Engineers">
    **Big Data Engineers**

    - Build and maintain Hadoop clusters
    - Design data processing pipelines
    - Optimize MapReduce and Spark jobs
    - Implement ETL workflows at scale

    **What You'll Gain**:
    - Production-ready Hadoop knowledge
    - Performance tuning expertise
    - Troubleshooting skills
  </Tab>

  <Tab title="Backend Engineers">
    **Software Engineers**

    - Understand distributed data processing
    - Design scalable data systems
    - Integrate with Hadoop ecosystem
    - Make architectural decisions

    **What You'll Gain**:
    - Distributed systems patterns
    - Data processing fundamentals
    - System design capabilities
  </Tab>

  <Tab title="Interview Prep">
    **Job Seekers**

    - Hadoop is frequently asked at tech companies
    - Essential for data engineering roles
    - Common in system design questions
    - Demonstrates big data expertise

    **Interview Topics**:
    - "Design a distributed file system"
    - "Explain MapReduce execution"
    - "HDFS vs GFS differences"
    - "Optimize a slow Hadoop job"
  </Tab>

  <Tab title="Architects">
    **System Architects**

    - Design big data architectures
    - Choose appropriate technologies
    - Plan cluster capacity
    - Implement data governance

    **Practical Value**:
    - Technology selection criteria
    - Scalability planning
    - Cost optimization
    - Security implementation
  </Tab>
</Tabs>

---

## Prerequisites

<Warning>
**Recommended Background**:

- Basic understanding of distributed systems
- Familiarity with MapReduce concepts
- Knowledge of Java programming (helpful)
- Understanding of Linux file systems

**Not Required But Helpful**:
- GFS paper knowledge
- Experience with cloud computing
- SQL and database concepts
- Python or Scala programming
</Warning>

---

## Course Structure

Each chapter includes:

<CardGroup cols={3}>
  <Card title="Theory" icon="book">
    Deep conceptual explanations with comprehensive diagrams
  </Card>
  <Card title="Architecture" icon="diagram-project">
    Component interactions and data flow analysis
  </Card>
  <Card title="Interview Prep" icon="comments">
    4-5 questions per chapter at various difficulty levels
  </Card>
  <Card title="Real-World Examples" icon="building">
    Production insights from Yahoo, Facebook, and others
  </Card>
  <Card title="Visual Learning" icon="image">
    ASCII diagrams, flowcharts, and architectural visuals
  </Card>
  <Card title="Key Takeaways" icon="star">
    Summary sections highlighting critical concepts
  </Card>
</CardGroup>

---

## Learning Path

<Steps>
  <Step title="Understand the Origins">
    Start with **Chapter 1** to learn how Hadoop evolved from Google's GFS and MapReduce papers and why it became open source.
  </Step>

  <Step title="Master HDFS">
    **Chapter 2** covers the distributed file system. Learn how HDFS stores petabytes reliably and efficiently.
  </Step>

  <Step title="Learn MapReduce">
    **Chapter 3** dives into the programming model. Understand how to process massive datasets in parallel.
  </Step>

  <Step title="Explore YARN">
    **Chapter 4** covers resource management. Learn how Hadoop 2.0 evolved beyond MapReduce-only processing.
  </Step>

  <Step title="Discover the Ecosystem">
    **Chapter 5** explores tools built on Hadoop. See how Hive, Pig, HBase, and Spark extend capabilities.
  </Step>

  <Step title="Handle Failures">
    **Chapter 6** covers fault tolerance. Learn how Hadoop maintains reliability despite constant failures.
  </Step>

  <Step title="Optimize Performance">
    **Chapter 7** analyzes tuning techniques. Understand bottlenecks and optimization strategies.
  </Step>

  <Step title="Deploy to Production">
    **Chapter 8** covers real-world deployment. Learn cluster management, monitoring, and operational best practices.
  </Step>
</Steps>

---

## Chapter-by-Chapter Deep Overview

The rest of this course is structured like a research-style walkthrough of Hadoop’s major subsystems. This section gives you **paper-level notes** for each chapter so you can treat this track almost like reading a series of system design papers.

---

### Chapter 2: HDFS Architecture – Storage for Web-Scale Data

**Goals of HDFS**:

- **Scale**: Store petabytes of data across thousands of commodity machines.
- **Reliability**: Survive constant hardware failures without losing data.
- **Throughput over latency**: Optimize for large sequential reads/writes, not single-record lookups.
- **Simplicity**: Provide a small set of operations (create, append, list, delete) with relaxed semantics.

**Core Components**:

```text
                HDFS HIGH-LEVEL VIEW

   ┌───────────────────────────────────────────────┐
   │                   Client                     │
   └───────────────┬──────────────────────────────┘
                   │ 1. metadata ops (open, create,
                   │    list, delete)
                   │
           ┌───────▼───────┐
           │   NameNode    │
           │  (metadata)   │
           └───────┬───────┘
                   │ 2. block locations
                   │
    ┌───────────────▼───────────────┐
    │         DataNodes (N)         │
    │  (actual block data, 3x rep) │
    └───────────────────────────────┘
```

- **NameNode**: In-memory namespace tree (`/user/alice/logs/...`) and block mapping (`file → [block IDs] → [DataNodes]`).
- **DataNodes**: Store block replicas as local files; periodically send **block reports** and **heartbeats** to the NameNode.
- **Secondary NameNode / Checkpoint Node**: Periodically merges the NameNode’s edit log with the fsimage to keep metadata compact.

**Block Management**:

- Files are split into large blocks (often 128MB/256MB+).
- Each block is **replicated** (default replication factor = 3) across different nodes and racks.
- NameNode tracks **which DataNodes** host replicas; DataNodes are largely stateless.

```text
BLOCK PLACEMENT (SIMPLE HEURISTIC)

- Replica 1: Same rack as writer, local node when possible
- Replica 2: Different rack
- Replica 3: Same as Replica 2’s rack, different node

Goal: Balance reliability (rack-awareness) and write bandwidth.
```

**Read Path (HDFS vs GFS)**:

- Client asks NameNode for block locations.
- Client then streams from the **nearest** DataNode (data path does not go through the NameNode).
- For multi-block files, the client will **pipeline** across DataNodes in sequence.

**Write Path**:

- Client requests a new block from NameNode.
- NameNode chooses a **replica pipeline** (e.g., DN1 → DN2 → DN3).
- Client writes to DN1, which streams the same bytes to DN2, which streams to DN3.
- If any DataNode fails, the pipeline is reconfigured and the block is re-replicated later.

---

### Chapter 3: MapReduce Framework – Batch Processing at Scale

**Problem**: Let ordinary developers process terabytes of data on a cluster without managing threads, failures, or data distribution.

**Programming Model**:

- **Map**: Transform input records into intermediate `(key, value)` pairs.
- **Shuffle**: Group all values by key across the cluster.
- **Reduce**: Aggregate values for each key to produce final results.

```text
MAPREDUCE DATA FLOW (LOGICAL)

Input Splits → map() → shuffle/sort → reduce() → Output
```

**Execution Architecture (Classic MRv1)**:

- **JobTracker**: Central coordinator; schedules tasks, tracks progress, handles retries.
- **TaskTrackers**: Run map and reduce tasks inside fixed “slots”.
- **InputFormat / OutputFormat**: Decide how to slice data into splits and how to write results.

**Data Locality in Map Phase**:

- Each map task is ideally scheduled on a node that has the relevant HDFS block.
- If not possible, scheduler falls back to same-rack or remote execution.

**Shuffle and Sort**:

- Map tasks buffer intermediate key-value pairs, partition them by reducer, and spill to local disk.
- A background thread merges and **sorts** these segments.
- Reduce tasks pull data from multiple map outputs over the network, merge sort them, and feed sorted groups into the `reduce()` function.

```text
SHUFFLE BOTTLENECKS

- Too many small map outputs → many tiny network transfers.
- Skewed keys → one reducer becomes a straggler.
- Solutions: combiner functions, custom partitioners, skew-aware partitioning.
```

**Fault Tolerance**:

- If a map or reduce task fails, JobTracker re-schedules it on another TaskTracker.
- Task outputs are immutable; re-running a task doesn’t corrupt others.
- Completed task outputs are stored on local disk; if that disk fails, the task is recomputed from HDFS.

---

### Chapter 4: YARN – Decoupling Resource Management

MapReduce-as-the-only-workload became a limitation. YARN generalizes the cluster to run **multiple processing engines**.

**Key Ideas**:

- Separate **resource management** from **application logic**.
- Support many frameworks (MapReduce v2, Tez, Spark, Flink) sharing the same cluster.

**Core Components**:

- **ResourceManager (RM)**: Global scheduler and resource arbitrator.
- **NodeManager (NM)**: Per-node agent reporting resource usage, launching containers.
- **ApplicationMaster (AM)**: Framework-specific orchestrator (one per application).
- **Containers**: Resource bundles (CPU, memory, etc.) allocated to run tasks.

```text
YARN CONTROL FLOW

1. Client → RM: "Start application X".
2. RM allocates first container → launches ApplicationMaster (AM).
3. AM negotiates with RM for more containers to run its tasks.
4. AM talks to NodeManagers to start/stop containers.
5. When done, AM unregisters from RM.
```

**Scheduling Policies**:

- **Capacity Scheduler**: Multi-tenant, hierarchical queues with guaranteed capacities.
- **Fair Scheduler**: Aims to give equal shares of the cluster to active users/jobs.

**Why This Matters**:

- Enables **mixed workloads**: long-running services, interactive SQL, streaming, and batch.
- Prevents one MapReduce job from monopolizing the cluster.

---

### Chapter 5: Hadoop Ecosystem – Beyond HDFS and MapReduce

Hadoop’s long-term impact comes from the **ecosystem** built on top of its storage and resource layers.

**Storage Models**:

- **HDFS**: Write-once, read-many file storage.
- **HBase**: Random-access, sparse, column-family store (inspired by Bigtable).
- **Columnar formats (Parquet/ORC)**: Highly compressed, analytic-friendly layout.

**Query Engines**:

- **Hive**: SQL over HDFS; rewrites queries into MapReduce/Tez/Spark jobs.
- **Impala / Presto / Trino**: Low-latency, MPP-style query engines.

**Processing Engines**:

- **Original MapReduce**: Disk-bound batch processing.
- **Tez**: DAG-structured jobs with fewer materializations.
- **Spark**: In-memory RDD/DataFrame abstractions; batch, streaming, ML.

**Coordination & Workflow**:

- **ZooKeeper**: Coordination, leader election, configuration.
- **Oozie / Airflow**: DAG-based workflow schedulers orchestrating many jobs.
- **Kafka**: Durable event log feeding Hadoop-based batch and stream pipelines.

The chapter focuses less on "API usage" and more on **how these systems compose** into a coherent data platform and what invariants they rely on from HDFS and YARN.

---

### Chapter 6: Fault Tolerance – Designing for Constant Failure

At Hadoop scale, failure is continuous, not exceptional.

**HDFS Fault Tolerance**:

- **Replication**: Lose up to `replicationFactor - 1` nodes without losing data.
- **Heartbeats & Block Reports**: NameNode continuously monitors DataNode liveness.
- **Re-replication**: On DataNode failure, blocks are automatically cloned to new nodes.

**NameNode Resilience**:

- **Edit log + fsimage**: Write-ahead log of metadata mutations plus periodic snapshots.
- **Checkpointing**: Merges edit log into fsimage to bound replay time.
- **HA Mode** (HDFS 2.x+): Active and Standby NameNodes coordinated via ZooKeeper.

**MapReduce / YARN Fault Tolerance**:

- **Task retry**: Failed tasks are re-run elsewhere; job fails only after exceeding retry limits.
- **Speculative execution**: Duplicate slow tasks to mitigate stragglers.
- **AM recovery** (YARN): In some setups, ApplicationMasters can be restarted and resume work.

The chapter reads like a reliability engineering document: it enumerates **failure modes** (disk loss, node loss, rack loss, network partitions, NameNode crashes) and shows concretely how Hadoop responds in each case.

---

### Chapter 7: Performance & Tuning – Pushing the Cluster to Its Limits

This chapter focuses on **bottleneck analysis** and **tuning strategies**.

**Where Time Goes in a Typical Job**:

- HDFS read/write throughput (disk + network).
- Map-side parsing and serialization.
- Shuffle (network + disk merges).
- Reduce-side aggregation and output formatting.

**Key Tuning Levers**:

- **Data layout**: Prefer columnar formats (Parquet/ORC) for analytical queries.
- **Compression**: Use block-level compression (e.g., Snappy, LZO) to reduce I/O and network.
- **Parallelism**: Tune number of mappers/reducers, container sizes, and YARN queue capacities.
- **Locality**: Monitor locality rates; low locality often signals cluster imbalance.

```text
EXAMPLE TRADE-OFF

- More reducers:
  + Fewer keys per reducer → less skew
  - More small output files and overhead

- Fewer reducers:
  + Larger outputs per reducer → fewer files
  - Risk of stragglers and hotspots
```

**Cluster-Level Considerations**:

- Network topology (oversubscription ratios, rack layout).
- Disk configuration (number of spindles per node, SSD caches).
- Garbage collection tuning for long-lived JVM processes.

---

### Chapter 8: Production Deployment – From Lab to Data Platform

This chapter treats Hadoop as a **product you operate**, not a library you call.

**Cluster Design**:

- **Node roles**: Edge nodes vs masters vs workers.
- **High availability**: Redundant masters, NameNode HA, multiple RMs (dependent on distro).
- **Security**: Kerberos authentication, HDFS encryption zones, network ACLs.

**Operational Playbooks**:

- Adding/removing nodes without downtime.
- Rolling upgrades and configuration changes.
- Handling NameNode failover events.
- Capacity planning (storage growth vs compute demand).

**Monitoring & Observability**:

- Metrics: HDFS capacity, block replication health, job latencies, YARN queue utilization.
- Logs: Centralized log aggregation for HDFS, YARN, and ecosystem services.
- Alerting: Threshold-based alerts for under-replicated blocks, full disks, failing nodes.

By the end of this chapter, Hadoop is framed the way SREs view any large-scale distributed system: in terms of **SLIs/SLOs, failure modes, and runbooks**.

---

## Key Design Principles

<Note>
**Core Hadoop Principles to Remember**:

1. **Hardware Failure is Common**
   → Detect and recover from failures automatically

2. **Moving Computation is Cheaper Than Moving Data**
   → Data locality is key to performance

3. **Simple and Robust Beats Complex and Fragile**
   → Prefer straightforward designs that work reliably

4. **Scale Out, Not Up**
   → Add more machines rather than bigger machines

5. **Write Once, Read Many**
   → Optimize for append-only workloads

6. **Portability Across Platforms**
   → Run on commodity hardware with Linux

7. **Open Source and Community-Driven**
   → Benefit from thousands of contributors worldwide
</Note>

---

## What Makes This Course Different?

<CardGroup cols={2}>
  <Card title="Complete Coverage" icon="check">
    Covers HDFS, MapReduce, YARN, and ecosystem. Not just theory—real implementation details.
  </Card>

  <Card title="Interview Focused" icon="target">
    36+ interview questions across all chapters. Practice explaining complex concepts clearly.
  </Card>

  <Card title="Visual Learning" icon="eye">
    Extensive diagrams showing data flows, architecture, and component interactions.
  </Card>

  <Card title="Production Insights" icon="industry">
    Real-world examples from Yahoo (created Hadoop), Facebook, LinkedIn, and others.
  </Card>

  <Card title="GFS Comparison" icon="code-compare">
    Understand how Hadoop implements and improves upon Google's original designs.
  </Card>

  <Card title="Ecosystem Context" icon="network-wired">
    Learn how Hadoop fits into the broader big data landscape and modern alternatives.
  </Card>
</CardGroup>

---

## Expected Outcomes

After completing this course, you will be able to:

```text
TECHNICAL SKILLS:
────────────────
✓ Explain Hadoop architecture comprehensively
✓ Design HDFS storage strategies
✓ Write and optimize MapReduce jobs
✓ Understand YARN resource management
✓ Work with Hadoop ecosystem tools
✓ Compare Hadoop with alternatives (Spark, etc.)

INTERVIEW SKILLS:
────────────────
✓ Answer "How does Hadoop work?"
✓ Explain HDFS vs GFS differences
✓ Discuss MapReduce execution flow
✓ Analyze performance bottlenecks
✓ Compare distributed processing frameworks
✓ Design big data solutions

PRACTICAL SKILLS:
────────────────
✓ Deploy and configure Hadoop clusters
✓ Optimize job performance
✓ Troubleshoot common issues
✓ Plan capacity and resources
✓ Implement security and governance
✓ Choose appropriate ecosystem tools
```

---

## Related Systems

Understanding Hadoop provides foundation for these systems:

<Tabs>
  <Tab title="Core Technologies">
    **Built on Hadoop**:

    - **Apache Hive**: SQL query engine
    - **Apache Pig**: Data flow scripting
    - **Apache HBase**: NoSQL database
    - **Apache Spark**: In-memory processing
    - **Apache Flink**: Stream processing
    - **Presto/Trino**: Distributed SQL engine

    These tools leverage HDFS and YARN infrastructure.
  </Tab>

  <Tab title="Cloud Evolution">
    **Cloud-Native Alternatives**:

    - **Amazon EMR**: Managed Hadoop service
    - **Google Dataproc**: Cloud Hadoop/Spark
    - **Azure HDInsight**: Hadoop in Azure
    - **Databricks**: Unified analytics platform
    - **Snowflake**: Cloud data warehouse

    Understanding Hadoop helps transition to cloud platforms.
  </Tab>

  <Tab title="Foundational Papers">
    **Research Foundations**:

    - **GFS**: Google's file system (HDFS inspiration)
    - **MapReduce**: Google's processing model
    - **Bigtable**: Inspired HBase
    - **Dremel**: Inspired Impala/Drill
    - **Spanner**: Global consistency

    Hadoop brought these concepts to the open-source world.
  </Tab>
</Tabs>

---

## Study Tips

<AccordionGroup>
  <Accordion title="Hands-On Practice" icon="laptop-code">
    Install Hadoop locally or use cloud sandbox environments. Running actual jobs solidifies understanding better than reading alone.
  </Accordion>

  <Accordion title="Compare with GFS" icon="code-compare">
    As you learn HDFS, constantly compare with GFS. Understanding differences deepens knowledge of both systems.
  </Accordion>

  <Accordion title="Draw Data Flows" icon="diagram-project">
    Sketch how data moves through MapReduce shuffle, HDFS replication, YARN scheduling. Visual understanding aids retention.
  </Accordion>

  <Accordion title="Read Source Code" icon="code">
    Hadoop is open source. Reading actual implementation provides insights no documentation can match.
  </Accordion>

  <Accordion title="Practice Interview Questions" icon="comments">
    Don't skip questions. Practice explaining concepts aloud. Being able to teach demonstrates true understanding.
  </Accordion>
</AccordionGroup>

---

## Time Commitment

<CardGroup cols={2}>
  <Card title="Full Deep Dive" icon="clock">
    **14-18 hours**

    - Read all chapters thoroughly
    - Work through all examples
    - Answer all interview questions
    - Experiment with Hadoop locally
  </Card>

  <Card title="Interview Prep Focus" icon="rocket">
    **8-10 hours**

    - Focus on Chapters 1, 2, 3, 6
    - Practice interview questions
    - Understand core concepts
    - Compare with modern alternatives
  </Card>

  <Card title="Quick Overview" icon="gauge">
    **4-5 hours**

    - Chapter 1: Origins
    - Chapter 2: HDFS basics
    - Chapter 3: MapReduce overview
    - Skim ecosystem chapters
  </Card>

  <Card title="Mastery Path" icon="trophy">
    **25+ hours**

    - All chapters in depth
    - Install and configure cluster
    - Write sample MapReduce jobs
    - Explore ecosystem tools
    - Compare multiple systems
  </Card>
</CardGroup>

---

## Additional Resources

<CardGroup cols={2}>
  <Card title="GFS Paper" icon="file-pdf">
    "The Google File System" (2003)
    Foundation for HDFS design
  </Card>

  <Card title="MapReduce Paper" icon="file-pdf">
    "MapReduce: Simplified Data Processing" (2004)
    Original programming model
  </Card>

  <Card title="Hadoop: The Definitive Guide" icon="book">
    Tom White's comprehensive book
    Industry-standard reference
  </Card>

  <Card title="Apache Hadoop Docs" icon="book-open">
    Official documentation
    Latest features and APIs
  </Card>
</CardGroup>

---

## Get Started

Ready to master Apache Hadoop?

<Card title="Start with Chapter 1" icon="play" href="./hadoop-chapter-1-introduction">
  Begin your journey with **Introduction & Origins** to understand how Hadoop evolved from Google's research papers.
</Card>

<Tip>
**Learning Strategy**: Hadoop is a complex ecosystem. Take time to understand each component individually before seeing how they work together. The investment pays off in deep big data expertise.
</Tip>

---

## Course Map

```text
START HERE
    ↓
┌─────────────────────────────────────────┐
│ Chapter 1: Introduction & Origins       │ ← Understand the history
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Chapter 2: HDFS Architecture            │ ← Learn the storage layer
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Chapter 3: MapReduce Framework          │ ← Master the processing model
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Chapter 4: YARN Resource Management     │ ← Understand orchestration
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Chapter 5: Hadoop Ecosystem             │ ← Explore the tools
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Chapter 6: Fault Tolerance              │ ← Handle failures
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Chapter 7: Performance & Tuning         │ ← Optimize performance
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│ Chapter 8: Production Deployment        │ ← Deploy at scale
└─────────────────────────────────────────┘
    ↓
MASTER LEVEL: Comprehensive big data expertise
```

## How This Paper-Level Course Fits Into the Bigger Picture

This Hadoop "engineering paper" track is designed to complement more hands-on tooling courses.

- **If you're reading this from the engineering-papers course**: Treat it as your *theoretical backbone*—understand *why* Hadoop looks the way it does before you worry about commands.
- **If you're also following the Hadoop tools modules** (e.g., HDFS, MapReduce, YARN deep dives): Use this course to connect implementation details back to the original design goals and trade-offs.
- **If you're coming from the GFS paper track**: Continuously map concepts (`Master` → `NameNode`, `Chunkserver` → `DataNode`, record appends, relaxed consistency) and note where Hadoop intentionally diverged.

```text
HOW TO STUDY EFFECTIVELY

1. Pick one path:
   - GFS paper → Hadoop paper → Hadoop tools
   - or Hadoop paper → GFS paper (for historical contrast)

2. For each major concept:
   - Read the design motivation here
   - Skim the corresponding research paper sections
   - Then see how it is implemented in Hadoop code/docs

3. As you progress:
   - Maintain a personal "concept map" of storage, compute, and scheduling
   - Note recurring patterns: replication, data locality, failure recovery
```

This structure is intentional: by the end of the Hadoop paper track, you should be able to read production postmortems, design reviews, and research papers about large-scale data systems with ease.

Let's begin the journey into the distributed computing framework that democratized big data!
