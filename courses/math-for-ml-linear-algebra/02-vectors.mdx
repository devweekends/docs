---
title: "Vectors & Vector Spaces"
sidebarTitle: "Vectors"
description: "Understanding vectors through house price prediction - from intuition to implementation"
icon: "arrow-right"
---

# Vectors & Vector Spaces

**The Big Question**: How do we teach a computer to predict house prices?

The answer starts with **vectors** - the fundamental language of machine learning. By the end of this module, you'll understand how representing a house as a vector of numbers unlocks the power of mathematical prediction.

<Info>
**Estimated Time**: 3-4 hours  
**Difficulty**: Beginner  
**Prerequisites**: Basic algebra  
**Main Example**: House price prediction  
**Supporting Examples**: Document similarity, user recommendations
</Info>

---

## The Problem: Predicting House Prices

Imagine you're a real estate agent. A client asks: *"How much is this house worth?"*

You look at the house and consider:
- **3 bedrooms**
- **2,000 square feet**
- **15 years old**
- **5 km from city center**

How do you turn these features into a price prediction? This is where vectors come in.

---

## What Is a Vector?

### The House as a Vector

![House as Vector](/images/courses/math-for-ml-linear-algebra/house-as-vector.svg)

A vector is simply an **ordered list of numbers**. For our house:

```python
import numpy as np

# House features as a vector
house = np.array([3, 2000, 15, 5])
#                 ↑   ↑    ↑   ↑
#              beds sqft age dist
```

**Why is this powerful?** Because now we can use math to:
- Compare houses
- Find similar properties
- Predict prices
- Cluster neighborhoods

### Two Ways to Think About Vectors

#### 1. **Algebraic View**: A List of Numbers

This is the practical view for coding:

```python
# Different houses as vectors
house_1 = np.array([3, 2000, 15, 5])  # Suburban family home
house_2 = np.array([2, 1200, 25, 2])  # Urban apartment
house_3 = np.array([5, 3500, 5, 15])  # New luxury home
```

Each house is a point in **4-dimensional space** (4 features).

#### 2. **Geometric View**: An Arrow in Space

For 2D/3D vectors, we can visualize them as arrows:

![Vector as Arrow](/images/courses/math-for-ml-linear-algebra/vector-geometric.svg)

A vector has:
- **Magnitude** (length): How "big" is the house? (total value of features)
- **Direction**: What type of house? (luxury vs. budget, urban vs. suburban)

```python
# Visualize a 2D vector (bedrooms, sqft)
v = np.array([3, 2000])

import matplotlib.pyplot as plt
plt.figure(figsize=(6, 6))
plt.arrow(0, 0, v[0], v[1]/1000, head_width=0.2, fc='blue', ec='blue')
plt.xlim(0, 5)
plt.ylim(0, 3)
plt.xlabel('Bedrooms')
plt.ylabel('Sqft (thousands)')
plt.title('House as 2D Vector')
plt.grid(True)
plt.show()
```

**Key Insight**: Even though we can't visualize 4D space, the math works the same way!

---

## Why Vectors? The ML Perspective

### Everything Becomes Numbers

Machine learning algorithms need numbers, not descriptions. Vectors let us convert **anything** into numbers. Let's see three relatable examples:

### Example 1: Houses → Vectors (Real Estate)

```python
# Represent a house as a vector of features
house = np.array([
    3,      # bedrooms
    2000,   # square feet
    15,     # age in years
    5       # km from city center
])

# Now we can compare houses mathematically!
house_1 = np.array([3, 2000, 15, 5])
house_2 = np.array([4, 2200, 8, 2])
```

**Why this matters**: Real estate agents can find similar properties, predict prices, and recommend houses to buyers.

---

### Example 2: Students → Vectors (Education)

```python
# Represent a student's performance as a vector
student = np.array([
    85,     # math score (0-100)
    92,     # reading score (0-100)
    78,     # science score (0-100)
    12      # hours studied per week
])

# Compare students
student_A = np.array([85, 92, 78, 12])  # Strong in reading
student_B = np.array([95, 75, 88, 15])  # Strong in math
student_C = np.array([87, 90, 80, 13])  # Similar to A
```

**Why this matters**: 
- Find students with similar learning patterns
- Predict final exam scores
- Recommend personalized study plans
- Identify students who need help

**Real application**: Khan Academy uses this to recommend practice problems!

---

### Example 3: Movies → Vectors (Entertainment)

```python
# Represent a movie by its features
movie = np.array([
    8.5,    # IMDb rating (0-10)
    120,    # runtime in minutes
    2020,   # release year
    0.8,    # action score (0-1)
    0.2,    # romance score (0-1)
    0.6     # comedy score (0-1)
])

# Movie database
inception = np.array([8.8, 148, 2010, 0.9, 0.1, 0.3])
titanic = np.array([7.9, 195, 1997, 0.3, 0.9, 0.2])
avengers = np.array([8.4, 143, 2012, 0.95, 0.1, 0.5])
```

**Why this matters**:
- Netflix recommends movies similar to what you watched
- Spotify finds songs with similar "vibes"
- YouTube suggests videos you'll like

**How it works**: If you liked Inception (action-heavy), the system finds movies with similar vectors (high action score).

---

### The Universal Pattern

```
Real-World Object → Vector of Numbers → Math Operations → Predictions
```

**Key Insight**: The same math works for houses, students, movies, images, text, users, products... everything!

---

## Vector Operations: The Building Blocks

Now that we can represent houses as vectors, what can we do with them?

### 1. Vector Addition: Combining Features

**The Question**: What if we want to combine two house profiles?

![Vector Addition](/images/courses/math-for-ml-linear-algebra/vector-addition.svg)

**Geometric Intuition**: Place vectors tip-to-tail. The result is the diagonal.

**Algebraic Definition**: Add corresponding components.

```python
# Two house feature vectors
house_1 = np.array([3, 2000, 15, 5])
house_2 = np.array([2, 1500, 10, 3])

# Average house in the neighborhood
average_house = (house_1 + house_2) / 2
print(average_house)  # [2.5, 1750, 12.5, 4]
```

**Why This Matters**: 
- **Feature engineering**: Combine features to create new ones
- **Gradient descent**: Update model parameters by adding gradients
- **Ensemble methods**: Average predictions from multiple models

**Real-World Example**: User preferences

```python
# User's historical preferences
past_prefs = np.array([0.8, 0.2, 0.5])  # [action, comedy, drama]

# Recent viewing behavior
recent = np.array([0.1, 0.3, 0.2])

# Updated preferences (weighted sum)
new_prefs = 0.7 * past_prefs + 0.3 * recent
print(new_prefs)  # [0.59, 0.23, 0.41]
```

---

### 2. Scalar Multiplication: Scaling Features

**The Question**: What if all house prices in a neighborhood increase by 20%?

![Scalar Multiplication](/images/courses/math-for-ml-linear-algebra/scalar-multiplication.svg)

**Geometric Intuition**: Stretch or shrink the vector. Direction stays the same.

**Algebraic Definition**: Multiply each component by a number (scalar).

```python
house = np.array([3, 2000, 15, 5])

# Scale by 1.2 (20% increase)
scaled_house = 1.2 * house
print(scaled_house)  # [3.6, 2400, 18, 6]
```

**Why This Matters**:
- **Normalization**: Scale features to same range
- **Learning rate**: Control how much to update parameters
- **Feature weighting**: Emphasize important features

**ML Application**: Gradient descent

```python
# Current model parameters
weights = np.array([50000, 120, -5000, -8000])

# Gradient (direction to improve)
gradient = np.array([100, 0.5, -20, -30])

# Learning rate (how far to move)
learning_rate = 0.01

# Update parameters
weights = weights - learning_rate * gradient
#                    ↑ scalar multiplication!
```

**Key Insight**: The learning rate controls the step size. Too large → overshoot. Too small → slow learning.

---

### 3. Dot Product: Measuring Similarity

**The Big Question**: How do we measure if two things are similar?

This is THE most important operation in machine learning! Let's see why through three examples.

![Dot Product with Houses](/images/courses/math-for-ml-linear-algebra/dot-product-houses.svg)

**Algebraic Definition**: Multiply corresponding components and sum.

**Mathematical Formula**:

$$
\mathbf{v} \cdot \mathbf{w} = \sum_{i=1}^{n} v_i w_i = v_1w_1 + v_2w_2 + \ldots + v_nw_n
$$

**Alternative Formula** (geometric):

$$
\mathbf{v} \cdot \mathbf{w} = \|\mathbf{v}\| \|\mathbf{w}\| \cos(\theta)
$$

Where $\theta$ is the angle between vectors.

---

### Example 1: Comparing Houses

```python
house_1 = np.array([3, 2000, 10, 3])  # Suburban family home
house_2 = np.array([4, 2200, 8, 2])   # Similar house
house_3 = np.array([1, 800, 50, 20])  # Old studio apartment

# Compute dot products
sim_1_2 = np.dot(house_1, house_2)
sim_1_3 = np.dot(house_1, house_3)

print(f"House 1 · House 2 = {sim_1_2}")  # 4,400,098 (large, positive)
print(f"House 1 · House 3 = {sim_1_3}")  # 41,803 (much smaller)
```

**Interpretation**: 
- Large dot product = similar houses
- Small dot product = different houses
- **Why?** Similar houses have similar feature values, so products are large

**Real application**: Zillow uses this to find "similar homes" when you're browsing!

---

### Example 2: Matching Students for Study Groups

```python
# Student profiles: [math_score, reading_score, science_score, study_hours]
alice = np.array([85, 92, 78, 12])    # Strong in reading
bob = np.array([95, 75, 88, 15])      # Strong in math
charlie = np.array([87, 90, 80, 13])  # Similar to Alice

# Who should Alice study with?
alice_bob = np.dot(alice, bob)
alice_charlie = np.dot(alice, charlie)

print(f"Alice · Bob = {alice_bob}")        # 23,265
print(f"Alice · Charlie = {alice_charlie}") # 24,021 (higher!)
```

**Interpretation**: Alice and Charlie have more similar learning patterns!

**Why this matters**:
- Form effective study groups (similar students help each other)
- Pair struggling students with successful ones who had similar challenges
- Predict who will benefit from group work

**Real application**: Educational platforms use this for peer matching!

---

### Example 3: Movie Recommendations

```python
# Movie features: [rating, runtime, year, action, romance, comedy]
inception = np.array([8.8, 148, 2010, 0.9, 0.1, 0.3])
interstellar = np.array([8.6, 169, 2014, 0.7, 0.2, 0.2])
titanic = np.array([7.9, 195, 1997, 0.3, 0.9, 0.2])

# You just watched Inception. What should Netflix recommend?
inception_interstellar = np.dot(inception, interstellar)
inception_titanic = np.dot(inception, titanic)

print(f"Inception · Interstellar = {inception_interstellar}")  # 26,847
print(f"Inception · Titanic = {inception_titanic}")            # 24,143
```

**Recommendation**: Watch Interstellar! (Higher similarity)

**Why it works**: Both are:
- High-rated sci-fi films
- Similar runtime
- Recent releases
- Action-heavy with minimal romance

**Real application**: This is literally how Netflix, Spotify, and YouTube work!

---

### Understanding the Dot Product Geometrically

**Key Insights**:

```python
# Parallel vectors (same direction) → large positive dot product
v1 = np.array([2, 0])
v2 = np.array([3, 0])
print(np.dot(v1, v2))  # 6 (positive, large)

# Perpendicular vectors (90°) → dot product = 0
v3 = np.array([1, 0])
v4 = np.array([0, 1])
print(np.dot(v3, v4))  # 0 (orthogonal = independent!)

# Opposite vectors (180°) → negative dot product
v5 = np.array([1, 1])
v6 = np.array([-1, -1])
print(np.dot(v5, v6))  # -2 (opposite)
```

**What this means**:
- **Positive dot product**: Vectors point in similar directions (similar items)
- **Zero dot product**: Vectors are perpendicular (completely different items)
- **Negative dot product**: Vectors point in opposite directions (opposite items)

---

### Why Dot Product is Everywhere in ML

**1. Neural Networks**: Every layer computes dot products!

```python
# A neuron computes: output = weights · inputs + bias
weights = np.array([0.5, -0.3, 0.8])
inputs = np.array([1.0, 2.0, 3.0])

output = np.dot(weights, inputs) + 0.1
# = (0.5×1.0) + (-0.3×2.0) + (0.8×3.0) + 0.1
# = 0.5 - 0.6 + 2.4 + 0.1 = 2.4
```

**2. Similarity Search**: Find similar items

```python
# Find products similar to what user just bought
user_purchase = np.array([1, 0, 1, 0, 1])  # Product features
all_products = np.array([
    [1, 0, 1, 1, 0],  # Product A
    [1, 0, 1, 0, 1],  # Product B (identical!)
    [0, 1, 0, 1, 0],  # Product C (different)
])

similarities = [np.dot(user_purchase, product) for product in all_products]
print(similarities)  # [2, 3, 0] → Recommend Product B!
```

**3. Attention Mechanisms**: How transformers (GPT, BERT) work

```python
# Simplified: How much should we "attend" to each word?
query = np.array([0.8, 0.2, 0.5])  # Current word
key1 = np.array([0.9, 0.1, 0.4])   # Word 1
key2 = np.array([0.2, 0.8, 0.1])   # Word 2

attention_1 = np.dot(query, key1)  # 0.94 (high attention!)
attention_2 = np.dot(query, key2)  # 0.37 (low attention)
```

---

### 4. Vector Magnitude: Measuring "Size"

**The Question**: How "big" is a house (in feature space)?

**Geometric Intuition**: The length of the arrow.

**Algebraic Definition**: Square root of dot product with itself.

```python
house = np.array([3, 2000, 15, 5])

magnitude = np.linalg.norm(house)
# = sqrt(3² + 2000² + 15² + 5²)
# = sqrt(9 + 4,000,000 + 225 + 25)
# = sqrt(4,000,259)
# ≈ 2000.06
```

**Mathematical Formula**:

$$
\|\mathbf{v}\| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 + v_2^2 + \ldots + v_n^2}
$$

**Why This Matters**: Normalization!

```python
# Problem: sqft dominates (2000 vs 3 bedrooms)
house = np.array([3, 2000, 15, 5])

# Solution: Normalize to unit length
normalized = house / np.linalg.norm(house)
print(normalized)  # [0.0015, 0.9999, 0.0075, 0.0025]
print(np.linalg.norm(normalized))  # 1.0 (unit vector)
```

**Key Insight**: After normalization, all features contribute equally. Sqft no longer dominates!

---

## Similarity Measures: Finding Similar Items

### Cosine Similarity: Direction-Based

**The Problem with Dot Product**: It's affected by magnitude!

```python
# Two houses with same type, different size
small_house = np.array([2, 1000, 10, 3])
large_house = np.array([4, 2000, 20, 6])  # 2× small_house

# Dot product is very different
print(np.dot(small_house, small_house))  # 1,001,113
print(np.dot(large_house, large_house))  # 4,004,452 (4× larger!)
```

**The Solution**: Cosine similarity ignores magnitude, only cares about direction (type).

![Cosine Similarity](/images/courses/math-for-ml-linear-algebra/cosine-similarity.svg)

**Formula**:

$$
\text{similarity}(\mathbf{v}, \mathbf{w}) = \frac{\mathbf{v} \cdot \mathbf{w}}{\|\mathbf{v}\| \|\mathbf{w}\|} = \cos(\theta)
$$

**Range**: -1 (opposite) to +1 (identical direction)

```python
def cosine_similarity(v, w):
    """Compute cosine similarity between two vectors."""
    return np.dot(v, w) / (np.linalg.norm(v) * np.linalg.norm(w))
```

---

### Example 1: House Type Matching (Ignoring Size)

```python
# Find houses of similar TYPE, regardless of size
small_suburban = np.array([2, 1000, 10, 5])   # Small suburban
large_suburban = np.array([4, 2000, 20, 10])  # Large suburban (2× size)
urban_apartment = np.array([1, 800, 5, 1])    # Urban apartment

# Cosine similarity
print(f"Small vs Large suburban: {cosine_similarity(small_suburban, large_suburban):.3f}")  # 1.000!
print(f"Small suburban vs Urban: {cosine_similarity(small_suburban, urban_apartment):.3f}")  # 0.997
```

**Key Insight**: The two suburban houses are identical in TYPE (cosine = 1.0), even though one is twice the size!

**Why this matters**: 
- A family looking for a suburban house doesn't care if it's 2000 or 4000 sqft
- They care about the TYPE: suburban, family-friendly, good schools
- Cosine similarity captures this!

**Real application**: Zillow's "similar homes" feature uses cosine similarity to find homes of similar style, not just similar size.

---

### Example 2: Student Learning Style (Not Just Scores)

```python
# Student profiles: [math, reading, science, study_hours]
alice = np.array([85, 92, 78, 12])      # Strong reader, moderate study
alice_2x = np.array([170, 184, 156, 24]) # Alice with 2× scores (impossible, but illustrative)
bob = np.array([95, 75, 88, 15])        # Strong in math

# Cosine similarity
print(f"Alice vs Alice_2x: {cosine_similarity(alice, alice_2x):.3f}")  # 1.000 (same learning style!)
print(f"Alice vs Bob: {cosine_similarity(alice, bob):.3f}")            # 0.991 (different style)
```

**Interpretation**: 
- Alice and Alice_2x have IDENTICAL learning patterns (cosine = 1.0)
- The magnitude doesn't matter - it's the PATTERN that counts
- Alice is strong in reading, Bob is strong in math (different patterns)

**Why this matters**:
- Match students with similar learning STYLES, not just similar scores
- A student who scores 60/70/65 has the same pattern as one who scores 80/93/87
- Recommend study materials based on learning style, not absolute performance

**Real application**: Khan Academy matches students with similar learning patterns to suggest effective study paths.

---

### Example 3: Movie Taste (Not Just Ratings)

```python
# Movie preferences: [action, romance, comedy, horror, sci-fi]
user_A = np.array([5, 1, 3, 0, 4])      # Loves action & sci-fi
user_A_harsh = np.array([3, 0, 2, 0, 2]) # Same taste, harsher ratings
user_B = np.array([1, 5, 2, 4, 0])      # Loves romance & horror

# Cosine similarity
print(f"User A vs A_harsh: {cosine_similarity(user_A, user_A_harsh):.3f}")  # 0.998 (same taste!)
print(f"User A vs B: {cosine_similarity(user_A, user_B):.3f}")              # 0.385 (different taste)
```

**Key Insight**: User A and User A_harsh have the SAME TASTE, just different rating scales!
- User A rates generously (5, 4, 3)
- User A_harsh rates strictly (3, 2, 1)
- But they like the SAME TYPES of movies!

**Why this matters**:
- Some users rate everything 5 stars, others are harsh critics
- Cosine similarity finds users with similar TASTE, not similar rating scales
- Recommend movies based on taste, not rating magnitude

**Real application**: Netflix uses cosine similarity because users have different rating behaviors, but similar tastes should get similar recommendations.

---

### When to Use Cosine vs. Euclidean Distance

**Use Cosine Similarity when**:
- ✅ Direction matters more than magnitude
- ✅ Different scales (harsh vs. generous raters)
- ✅ Text similarity (document length doesn't matter)
- ✅ Recommendation systems (taste, not intensity)

**Use Euclidean Distance when**:
- ✅ Absolute position matters
- ✅ Same scale for all features
- ✅ Clustering (K-means)
- ✅ Anomaly detection (how far from normal?)

```python
# Example: Anomaly detection
normal_house = np.array([3, 2000, 15, 5])
similar_house = np.array([3, 2100, 14, 4])
anomaly = np.array([10, 8000, 2, 50])  # Weird house!

# Euclidean distance (absolute difference)
print(f"Normal vs Similar: {euclidean_distance(normal_house, similar_house):.1f}")  # 100.5
print(f"Normal vs Anomaly: {euclidean_distance(normal_house, anomaly):.1f}")        # 6007.0 (huge!)

# Cosine similarity (direction)
print(f"Normal vs Similar: {cosine_similarity(normal_house, similar_house):.3f}")  # 0.999
print(f"Normal vs Anomaly: {cosine_similarity(normal_house, anomaly):.3f}")        # 0.996 (still high!)
```

**Interpretation**: Euclidean distance catches the anomaly better because it cares about MAGNITUDE!

---

## Real-World Application: Finding Similar Houses

Let's build a simple house recommendation system!

```python
import numpy as np

# Database of houses (bedrooms, sqft, age, distance)
houses = np.array([
    [3, 2000, 15, 5],   # House 0
    [4, 2200, 8, 2],    # House 1
    [2, 1200, 25, 3],   # House 2
    [3, 1900, 12, 6],   # House 3
    [5, 3500, 5, 15],   # House 4
])

# Prices (in thousands)
prices = np.array([320, 380, 250, 310, 550])

# Query: User likes this house
query_house = np.array([3, 2000, 10, 4])

# Find 3 most similar houses
similarities = []
for i, house in enumerate(houses):
    sim = cosine_similarity(query_house, house)
    similarities.append((i, sim, prices[i]))

# Sort by similarity
similarities.sort(key=lambda x: x[1], reverse=True)

print("Top 3 similar houses:")
for i, (idx, sim, price) in enumerate(similarities[:3], 1):
    print(f"{i}. House {idx}: similarity={sim:.3f}, price=${price}k")
```

**Output**:
```
Top 3 similar houses:
1. House 0: similarity=0.999, price=$320k
2. House 3: similarity=0.998, price=$310k
3. House 1: similarity=0.997, price=$380k
```

**Prediction**: Based on similar houses, estimated price ≈ $337k (average of top 3)

---

## Supporting Example 1: Document Similarity

The same vector concepts apply to text!

```python
from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "machine learning is awesome",
    "deep learning is a subset of machine learning",
    "neural networks are powerful",
    "python is great for machine learning"
]

# Convert to vectors
vectorizer = CountVectorizer()
doc_vectors = vectorizer.fit_transform(documents).toarray()

print("Vocabulary:", vectorizer.get_feature_names_out())
print("\nDocument vectors:")
print(doc_vectors)

# Find similar documents to "machine learning"
query = "machine learning"
query_vector = vectorizer.transform([query]).toarray()[0]

for i, doc_vec in enumerate(doc_vectors):
    sim = cosine_similarity(query_vector, doc_vec)
    print(f"Doc {i}: {sim:.3f} - {documents[i]}")
```

**Key Insight**: Same math, different domain!

---

## Supporting Example 2: User Recommendations

```python
# User-movie rating matrix
ratings = np.array([
    [5, 4, 0, 0, 1],  # User 0: likes action/comedy
    [4, 5, 0, 0, 2],  # User 1: similar to User 0
    [0, 0, 5, 4, 5],  # User 2: likes drama/romance
    [5, 4, 0, 1, 1],  # User 3: similar to User 0
])

# Find users similar to User 0
user_0 = ratings[0]
for i in range(1, len(ratings)):
    sim = cosine_similarity(user_0, ratings[i])
    print(f"User {i}: similarity = {sim:.3f}")

# Output:
# User 1: similarity = 0.987 (recommend same movies!)
# User 2: similarity = 0.140 (different taste)
# User 3: similarity = 0.989 (very similar)
```

---

## Practice Exercises

### Exercise 1: House Price Estimation

```python
# Given these houses and prices
houses = np.array([
    [3, 1800, 20, 5],  # $280k
    [4, 2400, 10, 3],  # $360k
    [2, 1200, 30, 8],  # $220k
])
prices = np.array([280, 360, 220])

# Predict price for this house
new_house = np.array([3, 2000, 15, 4])

# TODO: Find 2 most similar houses and average their prices
```

<details>
<summary>Solution</summary>

```python
similarities = []
for i, house in enumerate(houses):
    sim = cosine_similarity(new_house, house)
    similarities.append((i, sim, prices[i]))

similarities.sort(key=lambda x: x[1], reverse=True)

# Top 2 similar houses
top_2_prices = [similarities[0][2], similarities[1][2]]
predicted_price = np.mean(top_2_prices)

print(f"Predicted price: ${predicted_price}k")
# Output: Predicted price: $320k
```
</details>

---

## Key Takeaways

✅ **Vectors represent data** - Houses, images, text all become vectors  
✅ **Dot product measures similarity** - Foundation of neural networks  
✅ **Cosine similarity** - Direction-based (ignores magnitude)  
✅ **Euclidean distance** - Position-based (includes magnitude)  
✅ **Normalization matters** - Prevent one feature from dominating  
✅ **Same math, different domains** - Vectors work everywhere!  

---

## What's Next?

You now understand how to represent houses as vectors and measure similarity. But how do we actually **predict** the price?

That's where **matrices** come in. A matrix is a function that transforms input (house features) into output (price prediction). This is exactly how neural networks work!

<Card title="Next: Matrices & Transformations" icon="arrow-right" href="/courses/math-for-ml-linear-algebra/03-matrices">
  Learn how matrices transform house features into price predictions
</Card>
