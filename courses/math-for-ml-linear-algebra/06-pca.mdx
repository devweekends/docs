---
title: "Principal Component Analysis (PCA)"
sidebarTitle: "PCA"
description: "Reduce dimensions while keeping the important information - from 10 features to 3"
icon: "compress"
---

# Principal Component Analysis (PCA)

**The Big Question**: Can we predict house prices with fewer features?

You have 10 house features, but your model is slow and overfitting. Can you reduce to 3 features while keeping 95% of the information?

**PCA (Principal Component Analysis)** is the answer - it's dimensionality reduction using eigenvectors!

<Info>
**Estimated Time**: 3-4 hours  
**Difficulty**: Intermediate  
**Prerequisites**: Eigenvalues module  
**Main Example**: House feature reduction  
**Supporting Examples**: Student profile compression, Movie recommendation optimization
</Info>

---

## The Core Idea

### From 10 Features to 3

**Problem**: Too many features cause:
- Slow training
- Overfitting
- Hard to visualize
- Expensive to collect

**Solution**: Find the 3 **most important directions** (principal components) and project data onto them!

```python
# Before PCA
X = np.array([[3, 2000, 15, 5, 8, 7, 2, 95, 3, 1500]])  # 10 features

# After PCA
X_reduced = pca.transform(X)  # 3 features
# Still captures 95% of variance!
```

---

## Example 1: House Price Prediction with Fewer Features

### The Dataset

```python
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1000 houses × 10 features
# [beds, baths, sqft, age, distance, school, crime, walk, parking, yard]
np.random.seed(42)
n_houses = 1000

houses = np.random.randn(n_houses, 10)
# Add correlations (realistic data)
houses[:, 2] = houses[:, 0] * 500 + 1500  # sqft correlates with beds
houses[:, 1] = houses[:, 0] * 0.5 + 1.5   # baths correlate with beds

print(f"Original shape: {houses.shape}")  # (1000, 10)
```

### Apply PCA

```python
# Step 1: Standardize (important!)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
houses_scaled = scaler.fit_transform(houses)

# Step 2: Apply PCA
pca = PCA(n_components=3)  # Keep top 3 components
houses_pca = pca.fit_transform(houses_scaled)

print(f"Reduced shape: {houses_pca.shape}")  # (1000, 3)
print(f"Variance explained: {pca.explained_variance_ratio_}")
# [0.45, 0.28, 0.15] = 88% total!
```

### What Do the Components Mean?

```python
# Component loadings (how original features contribute)
components = pca.components_

print("First component (PC1):")
features = ['beds', 'baths', 'sqft', 'age', 'dist', 'school', 'crime', 'walk', 'park', 'yard']
for i, feature in enumerate(features):
    print(f"  {feature}: {components[0, i]:.3f}")

# Output:
# beds: 0.450
# baths: 0.420
# sqft: 0.480  ← Dominates!
# age: -0.120
# ...
```

**Interpretation**:
- **PC1** (45% variance): "House size" (beds + baths + sqft)
- **PC2** (28% variance): "Location quality" (school + walk - crime)
- **PC3** (15% variance): "Age vs. modernization"

### Visualize in 2D

```python
# Plot first 2 principal components
plt.figure(figsize=(10, 6))
plt.scatter(houses_pca[:, 0], houses_pca[:, 1], alpha=0.5)
plt.xlabel('PC1: House Size (45% variance)')
plt.ylabel('PC2: Location Quality (28% variance)')
plt.title('Houses in PCA Space')
plt.grid(True)
plt.show()
```

### Predict Prices with Reduced Features

```python
# Train model on original 10 features
from sklearn.linear_model import LinearRegression

prices = np.random.randn(n_houses) * 50000 + 300000  # Simulated prices

model_full = LinearRegression()
model_full.fit(houses_scaled, prices)
score_full = model_full.score(houses_scaled, prices)

# Train model on 3 PCA features
model_pca = LinearRegression()
model_pca.fit(houses_pca, prices)
score_pca = model_pca.score(houses_pca, prices)

print(f"R² with 10 features: {score_full:.3f}")
print(f"R² with 3 features: {score_pca:.3f}")
# Almost the same performance with 70% fewer features!
```

**Key Insight**: PCA reduced features from 10 → 3 while maintaining 88% of information and similar prediction accuracy!

**Real Application**: Zillow uses PCA to compress house features for faster recommendations.

---

## Example 2: Student Performance - Simplifying Profiles

### The Dataset

```python
# 500 students × 8 features
# [study_hrs, prev_gpa, attendance, sleep, exercise, social, stress, motivation]
students = np.random.randn(500, 8)

# Standardize
students_scaled = scaler.fit_transform(students)

# Apply PCA
pca = PCA(n_components=3)
students_pca = pca.fit_transform(students_scaled)

print(f"Variance explained: {pca.explained_variance_ratio_}")
# [0.38, 0.25, 0.18] = 81% total
```

### Interpret Components

```python
features = ['study', 'gpa', 'attend', 'sleep', 'exercise', 'social', 'stress', 'motiv']
components = pca.components_

print("\\nPrincipal Components:")
for i in range(3):
    print(f"\\nPC{i+1} ({pca.explained_variance_ratio_[i]:.1%} variance):")
    # Find top 3 contributing features
    top_idx = np.argsort(np.abs(components[i]))[-3:][::-1]
    for idx in top_idx:
        print(f"  {features[idx]}: {components[i, idx]:.3f}")
```

**Interpretation**:
- **PC1** (38%): "Academic engagement" (study + GPA + attendance)
- **PC2** (25%): "Well-being" (sleep + exercise - stress)
- **PC3** (18%): "Social balance" (social + motivation)

### Identify Student Clusters

```python
from sklearn.cluster import KMeans

# Cluster students in PCA space
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(students_pca)

# Visualize
plt.scatter(students_pca[:, 0], students_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)
plt.xlabel('PC1: Academic Engagement')
plt.ylabel('PC2: Well-being')
plt.title('Student Clusters')
plt.colorbar(label='Cluster')
plt.show()
```

**Clusters Found**:
- **Cluster 0**: High academic, low well-being (burnout risk!)
- **Cluster 1**: Balanced students
- **Cluster 2**: Low academic, high social (need intervention)

**Real Application**: Khan Academy uses PCA to group students and personalize learning paths!

---

## Example 3: Movie Recommendations - Faster Matching

### The Dataset

```python
# 1000 movies × 20 features
# [action, romance, comedy, horror, sci-fi, drama, thriller, mystery, 
#  animation, documentary, rating, runtime, year, budget, revenue, ...]
movies = np.random.randn(1000, 20)

# Standardize
movies_scaled = scaler.fit_transform(movies)

# Apply PCA
pca = PCA(n_components=5)  # 20 → 5 features
movies_pca = pca.fit_transform(movies_scaled)

print(f"Variance explained: {pca.explained_variance_ratio_.sum():.1%}")
# 85% with just 5 components!
```

### Interpret Movie "Factors"

```python
# PC1: "Blockbuster factor"
# High: action, sci-fi, budget, revenue
# Low: documentary, indie

# PC2: "Emotional factor"  
# High: romance, drama
# Low: action, horror

# PC3: "Comedy factor"
# High: comedy, animation
# Low: thriller, horror
```

### Fast Similarity Search

```python
# Find similar movies in PCA space (much faster!)
from scipy.spatial.distance import cdist

# Query movie (in PCA space)
query_movie = movies_pca[0]

# Compute distances (5D instead of 20D!)
distances = cdist([query_movie], movies_pca, metric='cosine')[0]

# Top 5 similar movies
similar_idx = np.argsort(distances)[1:6]  # Exclude self
print(f"Similar movies: {similar_idx}")
```

**Speed Comparison**:
```python
import time

# Original 20D space
start = time.time()
for _ in range(1000):
    distances_full = cdist([movies_scaled[0]], movies_scaled, metric='cosine')
time_full = time.time() - start

# PCA 5D space
start = time.time()
for _ in range(1000):
    distances_pca = cdist([movies_pca[0]], movies_pca, metric='cosine')
time_pca = time.time() - start

print(f"Full: {time_full:.3f}s")
print(f"PCA: {time_pca:.3f}s")
print(f"Speedup: {time_full/time_pca:.1f}x")
# 4x faster with PCA!
```

**Real Application**: Netflix uses PCA to compress movie features for real-time recommendations to millions of users!

---

## How PCA Works: Step-by-Step

### Step 1: Standardize Data

```python
# Why? Features have different scales
# beds: 1-5, sqft: 500-5000, age: 0-100

X = np.array([[3, 2000, 15], [4, 2200, 8]])

# Standardize: mean=0, std=1
X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_scaled = (X - X_mean) / X_std
```

### Step 2: Compute Covariance Matrix

```python
# Covariance shows how features vary together
cov_matrix = np.cov(X_scaled.T)
print(cov_matrix.shape)  # (3, 3)
```

### Step 3: Find Eigenvectors

```python
# Eigenvectors = principal directions
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort by eigenvalue (largest first)
idx = eigenvalues.argsort()[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]
```

### Step 4: Project Data

```python
# Keep top k eigenvectors
k = 2
principal_components = eigenvectors[:, :k]

# Project data onto principal components
X_pca = X_scaled @ principal_components
print(X_pca.shape)  # (2, 2) - reduced from 3 to 2!
```

---

## Choosing Number of Components

### Method 1: Variance Threshold

```python
# Keep components that explain 95% of variance
pca = PCA(n_components=0.95)  # Automatic!
X_pca = pca.fit_transform(X_scaled)
print(f"Kept {pca.n_components_} components")
```

### Method 2: Scree Plot

```python
# Fit PCA with all components
pca_full = PCA()
pca_full.fit(X_scaled)

# Plot explained variance
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),
         pca_full.explained_variance_ratio_, 'bo-')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.title('Scree Plot')
plt.grid(True)
plt.show()

# Look for "elbow" - where curve flattens
```

### Method 3: Cumulative Variance

```python
cumsum = np.cumsum(pca_full.explained_variance_ratio_)
n_components = np.argmax(cumsum >= 0.95) + 1
print(f"Need {n_components} components for 95% variance")
```

---

## Practice Exercises

### Exercise 1: House Data Compression

```python
# Given 1000 houses with 15 features
X = np.random.randn(1000, 15)

# TODO:
# 1. Standardize the data
# 2. Apply PCA to keep 95% variance
# 3. How many components needed?
# 4. What do the top 3 components represent?
```

<details>
<summary>Solution</summary>

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Standardize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA with 95% variance
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

print(f"Components needed: {pca.n_components_}")
print(f"Variance explained: {pca.explained_variance_ratio_[:3]}")
print(f"Total variance: {pca.explained_variance_ratio_.sum():.1%}")
```
</details>

---

## Key Takeaways

✅ **PCA** = dimensionality reduction using eigenvectors  
✅ **Principal components** = directions of maximum variance  
✅ **Standardize first** = essential for PCA to work correctly  
✅ **Choose k** = keep components explaining 95% variance  
✅ **Faster models** = fewer features → faster training  
✅ **Visualization** = reduce to 2D/3D for plotting  

---

## What's Next?

PCA is great for reducing features, but what if you want to find **hidden patterns** in your data? Like discovering that users who like action movies also like sci-fi?

That's **Singular Value Decomposition (SVD)** - the most powerful matrix decomposition!

<Card title="Next: Singular Value Decomposition (SVD)" icon="arrow-right" href="/courses/math-for-ml-linear-algebra/07-svd">
  Build a movie recommendation system using matrix factorization
</Card>
