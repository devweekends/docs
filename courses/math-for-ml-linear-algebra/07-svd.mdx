---
title: "Singular Value Decomposition (SVD)"
sidebarTitle: "SVD"
description: "The most powerful matrix decomposition - from Netflix recommendations to image compression"
icon: "layer-group"
---

# Singular Value Decomposition (SVD)

**The Big Question**: How does Netflix recommend movies you'll love?

You've rated 50 movies. Netflix has 10,000 movies. How do they predict your ratings for the 9,950 you haven't seen?

**SVD (Singular Value Decomposition)** is the answer - it discovers hidden patterns in data!

<Info>
**Estimated Time**: 4-5 hours  
**Difficulty**: Intermediate to Advanced  
**Prerequisites**: Eigenvalues and PCA modules  
**Main Example**: Movie recommendation system  
**Supporting Examples**: House price patterns, Student performance prediction
</Info>

---

## The Core Idea

### Matrix Factorization

**Key Insight**: Any matrix can be decomposed into 3 simpler matrices!

$$
A = U \Sigma V^T
$$

Where:
- $U$ = left singular vectors (users/rows)
- $\Sigma$ = singular values (importance)
- $V^T$ = right singular vectors (items/columns)

```python
import numpy as np

# Any matrix
A = np.array([
    [4, 0, 2],
    [0, 3, 0],
    [2, 0, 1]
])

# SVD decomposition
U, S, VT = np.linalg.svd(A)

print(f"U shape: {U.shape}")    # (3, 3)
print(f"S shape: {S.shape}")    # (3,)
print(f"VT shape: {VT.shape}")  # (3, 3)

# Reconstruct
A_reconstructed = U @ np.diag(S) @ VT
print(np.allclose(A, A_reconstructed))  # True!
```

---

## Example 1: Movie Recommendation System

### The Problem

```python
# User-Movie rating matrix (5 users Ã— 6 movies)
# Rows = users, Columns = movies
# 0 = not rated
ratings = np.array([
    [5, 3, 0, 1, 0, 0],  # User 0: likes action
    [4, 0, 0, 1, 0, 0],  # User 1: likes action
    [1, 1, 0, 5, 4, 5],  # User 2: likes romance
    [0, 0, 0, 4, 5, 4],  # User 3: likes romance
    [0, 1, 5, 4, 0, 0],  # User 4: mixed taste
])

print(f"Matrix shape: {ratings.shape}")  # (5, 6)
print(f"Sparsity: {(ratings == 0).sum() / ratings.size:.1%}")  # 50% missing!
```

**Challenge**: Predict the missing ratings!

### Apply SVD

```python
# SVD
U, S, VT = np.linalg.svd(ratings, full_matrices=False)

print(f"U (users): {U.shape}")    # (5, 5)
print(f"S (importance): {S.shape}")  # (5,)
print(f"VT (movies): {VT.shape}")  # (5, 6)

print(f"\\nSingular values: {S}")
# [12.48, 9.51, 1.35, 0.89, 0.12]
# First 2 values dominate!
```

### Low-Rank Approximation

```python
# Keep only top 2 components (hidden factors)
k = 2
U_k = U[:, :k]
S_k = np.diag(S[:k])
VT_k = VT[:k, :]

# Reconstruct with reduced rank
ratings_approx = U_k @ S_k @ VT_k

print("\\nOriginal ratings:")
print(ratings)

print("\\nPredicted ratings:")
print(ratings_approx.round(1))
```

**Output**:
```
Original:
[[5 3 0 1 0 0]
 [4 0 0 1 0 0]
 [1 1 0 5 4 5]
 [0 0 0 4 5 4]
 [0 1 5 4 0 0]]

Predicted:
[[4.8 2.9 0.8 1.2 0.4 0.5]
 [3.9 2.3 0.6 1.0 0.3 0.4]
 [0.9 0.9 0.3 4.8 4.2 4.6]
 [0.5 0.5 0.2 4.3 4.7 4.1]
 [1.8 1.5 4.2 3.5 1.2 1.4]]
```

**Key Insight**: SVD filled in the missing ratings! User 0 would rate Movie 2 as 0.8 (low), Movie 4 as 0.4 (low).

### Interpret Hidden Factors

```python
# What do the 2 hidden factors represent?
print("\\nMovie factors (VT):")
print(VT_k)

# Factor 1: [high, medium, low, low, low, low]
# â†’ Action movies (Movies 0, 1)

# Factor 2: [low, low, low, high, high, high]
# â†’ Romance movies (Movies 3, 4, 5)
```

**Interpretation**:
- **Factor 1**: "Action preference"
- **Factor 2**: "Romance preference"

Users are combinations of these factors!

```python
print("\\nUser factors (U):")
print(U_k)

# User 0: [high action, low romance]
# User 2: [low action, high romance]
# User 4: [medium action, medium romance] - mixed taste!
```

**Real Application**: This is exactly how Netflix works! They use ~50 hidden factors instead of 2.

---

## Example 2: House Price Patterns

### The Problem

```python
# House-Feature matrix (100 houses Ã— 10 features)
# Discover hidden patterns in house data

np.random.seed(42)
n_houses = 100
n_features = 10

# Simulate house data with hidden patterns
houses = np.random.randn(n_houses, n_features)

# Add pattern 1: "Luxury" (beds, baths, sqft high together)
luxury_factor = np.random.randn(n_houses)
houses[:, 0] += luxury_factor * 0.8  # beds
houses[:, 1] += luxury_factor * 0.7  # baths
houses[:, 2] += luxury_factor * 0.9  # sqft

# Add pattern 2: "Location" (school, walk, crime related)
location_factor = np.random.randn(n_houses)
houses[:, 5] += location_factor * 0.6  # school
houses[:, 7] += location_factor * 0.5  # walkability
houses[:, 6] -= location_factor * 0.4  # crime (inverse)
```

### Apply SVD

```python
# SVD
U, S, VT = np.linalg.svd(houses, full_matrices=False)

print(f"Singular values: {S[:5]}")
# [12.3, 10.8, 3.2, 2.9, 2.1]
# Top 2 dominate!

# Keep top 2 patterns
k = 2
houses_compressed = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# Compression ratio
original_size = houses.size
compressed_size = U[:, :k].size + k + VT[:k, :].size
print(f"Compression: {compressed_size / original_size:.1%}")  # 24% of original!
```

### Interpret Patterns

```python
# What do the patterns represent?
features = ['beds', 'baths', 'sqft', 'age', 'dist', 'school', 'crime', 'walk', 'park', 'yard']

print("\\nPattern 1 (Luxury):")
for i, feature in enumerate(features):
    print(f"  {feature}: {VT[0, i]:.3f}")

# High: beds, baths, sqft
# â†’ "Luxury/Size" pattern

print("\\nPattern 2 (Location):")
for i, feature in enumerate(features):
    print(f"  {feature}: {VT[1, i]:.3f}")

# High: school, walkability
# Low: crime
# â†’ "Location Quality" pattern
```

**Real Application**: Zillow uses SVD to discover hidden house "types" (luxury urban, suburban family, etc.)

---

## Example 3: Student Performance Prediction

### The Problem

```python
# Student-Subject matrix (200 students Ã— 8 subjects)
# Predict grades in subjects students haven't taken yet

students = np.random.randn(200, 8)

# Add pattern 1: "STEM aptitude"
stem_factor = np.random.randn(200)
students[:, 0] += stem_factor * 0.9  # Math
students[:, 2] += stem_factor * 0.8  # Physics
students[:, 4] += stem_factor * 0.7  # CS

# Add pattern 2: "Humanities aptitude"
humanities_factor = np.random.randn(200)
students[:, 1] += humanities_factor * 0.8  # English
students[:, 3] += humanities_factor * 0.7  # History
students[:, 5] += humanities_factor * 0.6  # Art
```

### Apply SVD

```python
U, S, VT = np.linalg.svd(students, full_matrices=False)

print(f"Singular values: {S[:5]}")
# [18.5, 15.2, 4.3, 3.8, 2.9]

# Keep top 2 aptitudes
k = 2
students_approx = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]
```

### Predict Missing Grades

```python
# Simulate missing grades (set some to 0)
students_incomplete = students.copy()
mask = np.random.rand(*students.shape) < 0.3  # 30% missing
students_incomplete[mask] = 0

# Predict using SVD
U, S, VT = np.linalg.svd(students_incomplete, full_matrices=False)
students_predicted = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# Compare predictions
print("\\nActual grade:", students[0, 3])
print("Predicted grade:", students_predicted[0, 3])
# Close match!
```

**Real Application**: Coursera uses SVD to recommend courses based on your performance in similar courses!

---

## SVD vs PCA vs Eigenvalues

### Comparison

| Method | Input | Output | Use Case |
|--------|-------|--------|----------|
| **Eigenvalues** | Square matrix | Eigenvalues + eigenvectors | Feature importance |
| **PCA** | Data matrix | Principal components | Dimensionality reduction |
| **SVD** | Any matrix | 3 matrices (U, Î£, V) | Recommendation, compression |

### When to Use Each

**Use Eigenvalues when**:
- You have a square matrix (covariance, adjacency)
- You want to find important directions
- Example: PageRank, stability analysis

**Use PCA when**:
- You want to reduce features
- You want to visualize high-D data
- Example: Compress 10 features â†’ 3

**Use SVD when**:
- You have a rectangular matrix (users Ã— items)
- You want to fill missing values
- You want to discover hidden patterns
- Example: Recommendations, collaborative filtering

---

## SVD Applications

### 1. Image Compression

```python
from PIL import Image

# Load image
img = Image.open('photo.jpg').convert('L')  # Grayscale
img_array = np.array(img)

# SVD
U, S, VT = np.linalg.svd(img_array, full_matrices=False)

# Compress: keep top k singular values
k = 50  # Instead of 512
img_compressed = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

# Compression ratio
original_size = img_array.size
compressed_size = U[:, :k].size + k + VT[:k, :].size
print(f"Compression: {compressed_size / original_size:.1%}")  # 20%!

# Save compressed image
Image.fromarray(img_compressed.astype(np.uint8)).save('compressed.jpg')
```

### 2. Noise Reduction

```python
# Noisy data
data_noisy = data_clean + np.random.randn(*data_clean.shape) * 0.5

# SVD
U, S, VT = np.linalg.svd(data_noisy, full_matrices=False)

# Keep only large singular values (signal)
# Drop small ones (noise)
k = 10
data_denoised = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]
```

### 3. Latent Semantic Analysis (LSA)

```python
# Document-Term matrix
# Rows = documents, Columns = words
# Discover hidden topics!

from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    "machine learning is great",
    "deep learning neural networks",
    "python programming language",
    # ... more documents
]

vectorizer = TfidfVectorizer()
doc_term_matrix = vectorizer.fit_transform(documents).toarray()

# SVD
U, S, VT = np.linalg.svd(doc_term_matrix, full_matrices=False)

# Top 3 topics
k = 3
topics = VT[:k, :]

# Interpret topics
terms = vectorizer.get_feature_names_out()
for i in range(k):
    top_terms_idx = np.argsort(topics[i])[-5:][::-1]
    print(f"Topic {i}: {[terms[idx] for idx in top_terms_idx]}")
```

---

## Practice Exercises

### Exercise 1: Movie Recommendations

```python
# User-Movie ratings (sparse)
ratings = np.array([
    [5, 0, 0, 1],
    [4, 0, 0, 1],
    [0, 0, 5, 4],
    [0, 3, 4, 0],
])

# TODO:
# 1. Apply SVD
# 2. Use rank-2 approximation
# 3. Predict missing ratings
# 4. Recommend top movie for User 0
```

<details>
<summary>Solution</summary>

```python
U, S, VT = np.linalg.svd(ratings, full_matrices=False)

# Rank-2 approximation
k = 2
ratings_pred = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]

print("Predicted ratings:")
print(ratings_pred.round(1))

# Recommend for User 0
user_0_pred = ratings_pred[0]
unrated_movies = np.where(ratings[0] == 0)[0]
best_movie = unrated_movies[np.argmax(user_0_pred[unrated_movies])]
print(f"Recommend Movie {best_movie} to User 0")
```
</details>

---

## Key Takeaways

âœ… **SVD** = decompose any matrix into U, Î£, V  
âœ… **Low-rank approximation** = keep top k singular values  
âœ… **Collaborative filtering** = predict missing ratings  
âœ… **Hidden patterns** = discover latent factors  
âœ… **Compression** = reduce storage with minimal loss  
âœ… **Noise reduction** = keep signal, drop noise  

---

## Course Complete!

ðŸŽ‰ **Congratulations!** You've mastered Linear Algebra for Machine Learning!

You now understand:
- **Vectors**: Data representation and similarity
- **Matrices**: Transformations and neural networks
- **Eigenvalues**: Feature importance
- **PCA**: Dimensionality reduction
- **SVD**: Recommendation systems

<Card title="Next: Build Real Projects" icon="rocket" href="/courses/math-for-ml-linear-algebra/projects">
  Apply your knowledge to real-world ML projects
</Card>
