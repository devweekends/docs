---
title: "Matrices & Linear Transformations"
sidebarTitle: "Matrices"
description: "Understanding matrices as functions that transform data - from house prices to neural networks"
icon: "grid"
---

# Matrices & Linear Transformations

**The Big Question**: How do we predict house prices from features?

In the previous module, we learned to represent houses as vectors. Now we'll learn how **matrices** transform those feature vectors into predictions. This is exactly how neural networks work!

<Info>
**Estimated Time**: 4-5 hours  
**Difficulty**: Beginner to Intermediate  
**Prerequisites**: Vectors module  
**Main Example**: House price prediction model  
**Supporting Examples**: Student grade prediction, movie rating prediction
</Info>

---

## From Vectors to Predictions

### The Problem

We have house features as a vector:

```python
house = np.array([3, 2000, 15, 5])  # [beds, sqft, age, distance]
```

We want to predict the price. How?

**Answer**: Use a **weight matrix** to transform features into a price!

![Matrix Price Prediction](/images/courses/math-for-ml-linear-algebra/matrix-price-prediction.svg)

---

## What Is a Matrix?

### Two Perspectives

#### 1. **Algebraic View**: A Table of Numbers

```python
# A 2×3 matrix (2 rows, 3 columns)
A = np.array([
    [1, 2, 3],
    [4, 5, 6]
])

print(A.shape)  # (2, 3)
```

#### 2. **Functional View**: A Transformation

**Key Insight**: A matrix is a **function** that transforms vectors!

$$
\mathbf{y} = A\mathbf{x}
$$

- Input: vector $\mathbf{x}$
- Matrix: $A$ (the transformation)
- Output: vector $\mathbf{y}$

**This is how neural networks work!**

---

## Example 1: House Price Prediction

### The Model

```python
# House features (input vector)
house = np.array([
    3,      # bedrooms
    2000,   # sqft
    15,     # age
    5       # distance
])

# Weight matrix (learned from data)
# Each weight says "how much does this feature affect price?"
weights = np.array([
    50000,   # $50k per bedroom
    120,     # $120 per sqft
    -5000,   # -$5k per year of age (older = cheaper)
    -8000    # -$8k per km from city
])

# Prediction
price = np.dot(weights, house)
# = (50000×3) + (120×2000) + (-5000×15) + (-8000×5)
# = 150,000 + 240,000 - 75,000 - 40,000
# = $275,000

print(f"Predicted price: ${price:,}")
```

**What just happened?**
- Matrix multiplication transformed 4 features → 1 price
- Each weight captures the relationship between feature and price
- This is **linear regression**!

### Multiple Houses (Batch Processing)

```python
# Multiple houses (each row is a house)
houses = np.array([
    [3, 2000, 15, 5],   # House 1
    [4, 2200, 8, 2],    # House 2
    [2, 1200, 25, 8],   # House 3
])

# Predict all prices at once!
prices = houses @ weights  # Matrix multiplication
print(prices)  # [275000, 358400, 186000]
```

**Key Insight**: Matrix multiplication lets us process **many** houses simultaneously! This is why GPUs are so powerful for ML.

---

## Example 2: Student Grade Prediction

### The Model

```python
# Student performance features
student = np.array([
    12,     # hours studied per week
    85,     # previous exam score
    8,      # assignments completed
    90      # attendance %
])

# Weights (how much each factor affects final grade)
weights = np.array([
    2.5,    # +2.5 points per hour studied
    0.3,    # 30% weight on previous exam
    1.5,    # +1.5 points per assignment
    0.2     # 20% weight on attendance
])

# Predict final grade
final_grade = np.dot(weights, student)
# = (2.5×12) + (0.3×85) + (1.5×8) + (0.2×90)
# = 30 + 25.5 + 12 + 18
# = 85.5

print(f"Predicted final grade: {final_grade:.1f}")
```

### Batch Prediction for Entire Class

```python
# All students in class
students = np.array([
    [12, 85, 8, 90],   # Student A
    [8, 70, 6, 75],    # Student B
    [15, 95, 10, 95],  # Student C
])

# Predict all grades at once
grades = students @ weights
print(grades)  # [85.5, 63.5, 102.5]
```

**Real application**: Learning management systems use this to predict which students need help!

---

## Example 3: Movie Rating Prediction

### The Model

```python
# Movie features
movie = np.array([
    0.9,    # action score (0-1)
    0.1,    # romance score
    0.3,    # comedy score
    8.5,    # IMDb rating
    2020    # release year
])

# User preferences (weights)
user_weights = np.array([
    5,      # Loves action (+5 points)
    -2,     # Dislikes romance (-2 points)
    3,      # Likes comedy (+3 points)
    0.5,    # Considers IMDb rating
    -0.001  # Slight preference for newer movies
])

# Predict user's rating
predicted_rating = np.dot(user_weights, movie)
# = (5×0.9) + (-2×0.1) + (3×0.3) + (0.5×8.5) + (-0.001×2020)
# = 4.5 - 0.2 + 0.9 + 4.25 - 2.02
# = 7.43

print(f"Predicted rating: {predicted_rating:.1f}/10")
```

### Recommend Movies to User

```python
# Movie database
movies = np.array([
    [0.9, 0.1, 0.3, 8.5, 2020],  # Action movie
    [0.2, 0.9, 0.1, 7.5, 2019],  # Romance movie
    [0.7, 0.2, 0.8, 8.0, 2021],  # Action-comedy
])

# Predict ratings for all movies
ratings = movies @ user_weights
print(ratings)  # [7.43, 2.73, 8.88]

# Recommend highest-rated movie
best_movie = np.argmax(ratings)
print(f"Recommended: Movie {best_movie}")  # Movie 2 (action-comedy)
```

**Real application**: Netflix uses matrices to predict your ratings for millions of movies!

---

## Matrix Operations

### 1. Matrix Addition

**When to use**: Combining multiple models, updating parameters

```python
# Two prediction models
model_1 = np.array([[1, 2], [3, 4]])
model_2 = np.array([[5, 6], [7, 8]])

# Ensemble: average the models
ensemble = (model_1 + model_2) / 2
print(ensemble)
# [[3, 4],
#  [5, 6]]
```

### 2. Scalar Multiplication

**When to use**: Scaling predictions, learning rates

```python
# Original weights
weights = np.array([[50000, 120, -5000, -8000]])

# Scale down for regularization
scaled_weights = 0.9 * weights
```

### 3. Matrix Multiplication

**The Most Important Operation!**

**Rule**: (m×n) matrix × (n×p) matrix = (m×p) matrix

The **inner dimensions must match**!

```python
# Example: Neural network layer
# Input: 3 features → Output: 2 neurons

# Weight matrix (2 neurons × 3 features)
W = np.array([
    [0.5, -0.3, 0.8],  # Weights for neuron 1
    [0.2, 0.6, -0.4]   # Weights for neuron 2
])

# Input vector (3 features)
x = np.array([1.0, 2.0, 3.0])

# Output (2 neurons)
y = W @ x
# Neuron 1: (0.5×1.0) + (-0.3×2.0) + (0.8×3.0) = 2.3
# Neuron 2: (0.2×1.0) + (0.6×2.0) + (-0.4×3.0) = 0.2

print(y)  # [2.3, 0.2]
```

![Neural Network Layer](/images/courses/math-for-ml-linear-algebra/neural-network-layer.svg)

**Key Insight**: Every neural network layer is just matrix multiplication!

---

## Matrix Multiplication: Three Examples

### Example 1: Multi-Output House Prediction

Predict **multiple** outputs from house features:

```python
# House features
house = np.array([3, 2000, 15, 5])

# Weight matrix (3 outputs × 4 features)
W = np.array([
    [50000, 120, -5000, -8000],   # Price prediction
    [0.8, 0.0002, -0.01, -0.02],  # Desirability score (0-1)
    [2, 0.001, -0.5, -0.3]        # Market time (months)
])

# Predict all outputs
outputs = W @ house
print(f"Price: ${outputs[0]:,.0f}")
print(f"Desirability: {outputs[1]:.2f}")
print(f"Time to sell: {outputs[2]:.1f} months")
```

**Output**:
```
Price: $275,000
Desirability: 0.73
Time to sell: 2.8 months
```

### Example 2: Student Performance Across Subjects

Predict grades in multiple subjects:

```python
# Student features
student = np.array([12, 85, 8, 90])  # [study_hours, prev_score, assignments, attendance]

# Weight matrix (3 subjects × 4 features)
W = np.array([
    [2.5, 0.3, 1.5, 0.2],   # Math
    [2.0, 0.4, 1.0, 0.3],   # English
    [3.0, 0.2, 2.0, 0.1]    # Science
])

# Predict all subject grades
grades = W @ student
print(f"Math: {grades[0]:.1f}")
print(f"English: {grades[1]:.1f}")
print(f"Science: {grades[2]:.1f}")
```

### Example 3: Multi-User Movie Recommendations

Predict ratings for multiple users:

```python
# Movie features
movie = np.array([0.9, 0.1, 0.3, 8.5, 2020])

# User preference matrix (3 users × 5 features)
U = np.array([
    [5, -2, 3, 0.5, -0.001],    # User 1: loves action
    [-1, 5, 2, 0.3, -0.002],    # User 2: loves romance
    [3, 1, 4, 0.4, 0]           # User 3: balanced
])

# Predict ratings for all users
ratings = U @ movie
print(f"User 1 rating: {ratings[0]:.1f}")
print(f"User 2 rating: {ratings[1]:.1f}")
print(f"User 3 rating: {ratings[2]:.1f}")
```

---

## Matrix Transpose

**Definition**: Flip rows and columns

```python
A = np.array([
    [1, 2, 3],
    [4, 5, 6]
])

A_T = A.T
print(A_T)
# [[1, 4],
#  [2, 5],
#  [3, 6]]
```

**Why it matters**: Used in backpropagation!

```python
# Forward pass
X = np.array([[1, 2, 3]])  # (1×3) input
W = np.array([[0.5], [0.3], [0.8]])  # (3×1) weights
y = X @ W  # (1×1) output

# Backward pass (gradient)
dy = np.array([[1.0]])  # Gradient from loss
dW = X.T @ dy  # (3×1) gradient for weights
```

---

## Identity Matrix

**Definition**: Matrix that doesn't change vectors

$$
I\mathbf{v} = \mathbf{v}
$$

```python
I = np.eye(3)  # 3×3 identity
print(I)
# [[1, 0, 0],
#  [0, 1, 0],
#  [0, 0, 1]]

v = np.array([5, 10, 15])
result = I @ v
print(result)  # [5, 10, 15] (unchanged!)
```

**Why it matters**: Used in regularization, initialization, and matrix inversion.

---

## Matrix Inverse

**Definition**: Matrix that "undoes" another matrix

$$
A A^{-1} = I
$$

```python
A = np.array([
    [4, 7],
    [2, 6]
])

A_inv = np.linalg.inv(A)
print(A_inv)

# Verify
print(A @ A_inv)  # ≈ Identity matrix
```

**Application**: Solving linear equations

```python
# Solve: Ax = b for x
A = np.array([[2, 1], [1, 3]])
b = np.array([5, 7])

# Solution: x = A^(-1) b
x = np.linalg.inv(A) @ b
print(x)  # [1, 3]

# Verify
print(A @ x)  # [5, 7] ✓
```

---

## Practice Exercises

### Exercise 1: House Price Model

```python
# Given these houses and prices
houses = np.array([
    [3, 1800, 20, 5],  # $280k
    [4, 2400, 10, 3],  # $360k
    [2, 1200, 30, 8],  # $220k
])
prices = np.array([280000, 360000, 220000])

# TODO: Find the best weights using least squares
# weights = ?
```

<details>
<summary>Solution</summary>

```python
# Least squares solution: w = (X^T X)^(-1) X^T y
X = houses
y = prices

weights = np.linalg.inv(X.T @ X) @ X.T @ y
print(weights)

# Predict new house
new_house = np.array([3, 2000, 15, 4])
predicted_price = new_house @ weights
print(f"Predicted: ${predicted_price:,.0f}")
```
</details>

---

## Key Takeaways

✅ **Matrices are functions** that transform vectors  
✅ **Matrix multiplication** = applying transformations  
✅ **Neural networks** = stacked matrix multiplications  
✅ **Batch processing** = process many inputs at once  
✅ **Transpose** = used in backpropagation  
✅ **Inverse** = solving equations, undoing transformations  

---

## What's Next?

You now understand how matrices transform data. But which transformations are most important? Which directions in your data carry the most information?

That's where **eigenvalues and eigenvectors** come in - they reveal the "natural axes" of your data!

<Card title="Next: Eigenvalues & Eigenvectors" icon="arrow-right" href="/courses/math-for-ml-linear-algebra/05-eigenvalues">
  Discover which house features matter most for price prediction
</Card>
