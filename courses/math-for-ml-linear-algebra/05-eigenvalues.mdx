---
title: "Eigenvalues & Eigenvectors"
sidebarTitle: "Eigenvalues"
description: "Discovering which features matter most - the hidden structure in your data"
icon: "compass"
---

# Eigenvalues & Eigenvectors

**The Big Question**: Which house features actually matter for price prediction?

You have 10 features (bedrooms, bathrooms, sqft, age, distance, school rating, crime rate, walkability, parking, yard size). But which ones really drive the price? Which can you ignore?

**Eigenvalues and eigenvectors** reveal the answer - they show you the "natural axes" of your data!

<Info>
**Estimated Time**: 3-4 hours  
**Difficulty**: Intermediate  
**Prerequisites**: Vectors and Matrices modules  
**Main Example**: House feature importance  
**Supporting Examples**: Student success factors, Movie genre patterns
</Info>

---

## The Intuition: Special Directions

### What Are Eigenvectors?

**Imagine**: You have a transformation (matrix) that stretches, rotates, and skews space. But there are **special directions** that only get stretched, never rotated!

These special directions are **eigenvectors**.

```python
import numpy as np

# A transformation matrix
A = np.array([
    [3, 1],
    [0, 2]
])

# A special vector (eigenvector)
v = np.array([1, 0])

# Apply transformation
result = A @ v
print(result)  # [3, 0] = 3 * v

# The vector only got scaled by 3 (the eigenvalue)!
# Direction unchanged!
```

**Key Insight**: 
- **Eigenvector**: Special direction that doesn't change
- **Eigenvalue**: How much it gets scaled

$$
A\mathbf{v} = \lambda\mathbf{v}
$$

Where:
- $A$ = transformation matrix
- $\mathbf{v}$ = eigenvector (direction)
- $\lambda$ = eigenvalue (scaling factor)

---

## Example 1: House Features - What Really Matters?

### The Problem

You have house data with many features. Which features explain most of the variation in prices?

```python
# House data (100 houses × 4 features)
# Features: [bedrooms, sqft, age, distance]
houses = np.array([
    [3, 2000, 15, 5],
    [4, 2200, 8, 2],
    [2, 1200, 25, 8],
    # ... 97 more houses
])

# Compute covariance matrix (how features vary together)
cov_matrix = np.cov(houses.T)
print(cov_matrix.shape)  # (4, 4)
```

### Finding Eigenvectors

```python
# Find eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort by eigenvalue (largest first)
idx = eigenvalues.argsort()[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

print("Eigenvalues:", eigenvalues)
# [1,200,000, 150, 80, 20]

print("First eigenvector:", eigenvectors[:, 0])
# [0.02, 0.99, 0.05, 0.08]
```

**Interpretation**:

1. **First eigenvector** (largest eigenvalue = 1,200,000):
   - Components: [0.02, 0.99, 0.05, 0.08]
   - **Sqft dominates** (0.99)! This is the most important direction
   - Explains most price variation

2. **Second eigenvector** (eigenvalue = 150):
   - Might be "age" or "distance"
   - Less important

3. **Third & Fourth**: Even less important

**Key Insight**: You can drop the last 2 features and keep 95% of the information!

### Visualizing Principal Directions

```python
import matplotlib.pyplot as plt

# Plot houses (using first 2 features for visualization)
plt.scatter(houses[:, 0], houses[:, 1], alpha=0.5)

# Plot eigenvectors (scaled by eigenvalues)
origin = np.mean(houses[:, :2], axis=0)
for i in range(2):
    direction = eigenvectors[:2, i] * np.sqrt(eigenvalues[i]) / 10
    plt.arrow(origin[0], origin[1], direction[0], direction[1],
              head_width=50, head_length=100, fc=f'C{i}', ec=f'C{i}')

plt.xlabel('Bedrooms')
plt.ylabel('Sqft')
plt.title('Principal Directions of House Data')
plt.show()
```

**Real Application**: Zillow uses this to determine which features to prioritize in their pricing model!

---

## Example 2: Student Success - What Predicts Performance?

### The Problem

You track 5 factors for students:
- Study hours
- Previous GPA
- Attendance %
- Sleep hours
- Extracurriculars

Which factors actually predict final grades?

```python
# Student data (200 students × 5 factors)
students = np.array([
    [12, 3.5, 95, 7, 2],  # Student 1
    [8, 3.0, 80, 6, 1],   # Student 2
    # ... 198 more students
])

# Covariance matrix
cov_matrix = np.cov(students.T)

# Eigenanalysis
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort
idx = eigenvalues.argsort()[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

print("Eigenvalues:", eigenvalues)
# [45.2, 12.8, 5.3, 2.1, 0.8]

print("First eigenvector:", eigenvectors[:, 0])
# [0.35, 0.62, 0.48, 0.25, 0.15]
```

**Interpretation**:

1. **First principal component** (eigenvalue = 45.2):
   - Previous GPA (0.62) + Attendance (0.48) + Study hours (0.35)
   - This is the "**academic dedication**" factor
   - Explains 60% of variance in final grades

2. **Second component** (eigenvalue = 12.8):
   - Sleep hours (high) + Extracurriculars (moderate)
   - This is the "**work-life balance**" factor
   - Explains 20% of variance

3. **Remaining components**: Less important (20% total)

**Key Insight**: Focus interventions on "academic dedication" factors (GPA, attendance, study hours) - they matter most!

**Real Application**: Educational platforms use this to identify at-risk students and recommend targeted interventions.

---

## Example 3: Movies - Hidden Genre Patterns

### The Problem

Movies have explicit genres (action, romance, comedy, horror, sci-fi), but are there **hidden patterns** in how these combine?

```python
# Movie data (500 movies × 5 genre scores)
# Each score 0-1 indicating genre strength
movies = np.array([
    [0.9, 0.1, 0.3, 0.0, 0.8],  # Action sci-fi
    [0.2, 0.9, 0.1, 0.0, 0.1],  # Romance
    [0.1, 0.1, 0.8, 0.0, 0.2],  # Comedy
    # ... 497 more movies
])

# Covariance matrix
cov_matrix = np.cov(movies.T)

# Eigenanalysis
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

idx = eigenvalues.argsort()[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

print("Eigenvalues:", eigenvalues)
# [0.85, 0.42, 0.28, 0.15, 0.08]

print("First eigenvector:", eigenvectors[:, 0])
# [0.65, -0.15, 0.25, -0.10, 0.68]
```

**Interpretation**:

1. **First hidden pattern** (eigenvalue = 0.85):
   - Action (0.65) + Sci-fi (0.68) - Romance (-0.15)
   - This is the "**blockbuster**" pattern
   - High-budget action sci-fi films

2. **Second pattern** (eigenvalue = 0.42):
   - Comedy (high) + Romance (moderate)
   - This is the "**rom-com**" pattern

3. **Third pattern**: Horror + Thriller combination

**Key Insight**: Movies naturally cluster into these hidden patterns, not just explicit genres!

**Real Application**: Netflix uses eigenvectors to create "micro-genres" like "Cerebral Sci-Fi Dramas" or "Feel-Good Rom-Coms"!

---

## Computing Eigenvalues & Eigenvectors

### The Math

For a matrix $A$, find $\mathbf{v}$ and $\lambda$ such that:

$$
A\mathbf{v} = \lambda\mathbf{v}
$$

Rearrange:

$$
(A - \lambda I)\mathbf{v} = 0
$$

For non-trivial solutions:

$$
\det(A - \lambda I) = 0
$$

This is the **characteristic equation**.

### Example: 2×2 Matrix

```python
A = np.array([
    [4, 2],
    [1, 3]
])

# Find eigenvalues
eigenvalues, eigenvectors = np.linalg.eig(A)

print("Eigenvalues:", eigenvalues)  # [5, 2]
print("Eigenvectors:")
print(eigenvectors)
# [[0.89, -0.71],
#  [0.45,  0.71]]

# Verify
v1 = eigenvectors[:, 0]
lambda1 = eigenvalues[0]

print(A @ v1)  # [4.47, 2.24]
print(lambda1 * v1)  # [4.47, 2.24] ✓
```

---

## Applications in Machine Learning

### 1. Principal Component Analysis (PCA)

**Goal**: Reduce dimensions while keeping most information

```python
# House data: 10 features → 3 features
from sklearn.decomposition import PCA

# Original data (100 houses × 10 features)
X = np.random.randn(100, 10)

# PCA: keep top 3 eigenvectors
pca = PCA(n_components=3)
X_reduced = pca.fit_transform(X)

print(f"Original: {X.shape}")  # (100, 10)
print(f"Reduced: {X_reduced.shape}")  # (100, 3)
print(f"Variance explained: {pca.explained_variance_ratio_.sum():.2%}")  # 85%
```

**Key Insight**: Eigenvectors with largest eigenvalues capture most variance!

### 2. PageRank (Google's Algorithm)

**Goal**: Rank web pages by importance

```python
# Web graph (pages link to each other)
# Eigenvector of transition matrix = page importance!

# Simplified example
links = np.array([
    [0, 1, 1, 0],  # Page 0 links to 1, 2
    [1, 0, 1, 0],  # Page 1 links to 0, 2
    [1, 1, 0, 1],  # Page 2 links to 0, 1, 3
    [0, 0, 1, 0]   # Page 3 links to 2
])

# Normalize (transition probabilities)
P = links / links.sum(axis=1, keepdims=True)

# Find dominant eigenvector
eigenvalues, eigenvectors = np.linalg.eig(P.T)
pagerank = np.abs(eigenvectors[:, 0])
pagerank = pagerank / pagerank.sum()

print("PageRank scores:", pagerank)
# [0.28, 0.24, 0.38, 0.10]
# Page 2 is most important!
```

### 3. Spectral Clustering

**Goal**: Find natural clusters in data

```python
# Similarity matrix → Eigenvectors → Clusters
from sklearn.cluster import SpectralClustering

# House data
X = np.random.randn(100, 4)

# Spectral clustering (uses eigenvectors!)
clustering = SpectralClustering(n_clusters=3)
labels = clustering.fit_predict(X)

print("Cluster labels:", labels)
```

---

## Practice Exercises

### Exercise 1: House Feature Importance

```python
# Given house data
houses = np.array([
    [3, 2000, 15, 5, 8],  # beds, sqft, age, dist, school
    [4, 2200, 8, 2, 9],
    [2, 1200, 25, 8, 6],
    # ... more houses
])

# TODO: 
# 1. Compute covariance matrix
# 2. Find eigenvalues and eigenvectors
# 3. Which feature is most important?
# 4. Can you drop any features?
```

<details>
<summary>Solution</summary>

```python
# Covariance matrix
cov = np.cov(houses.T)

# Eigenanalysis
eigenvalues, eigenvectors = np.linalg.eig(cov)

# Sort
idx = eigenvalues.argsort()[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

# Most important feature
first_eigenvector = eigenvectors[:, 0]
most_important_idx = np.argmax(np.abs(first_eigenvector))
features = ['beds', 'sqft', 'age', 'dist', 'school']
print(f"Most important: {features[most_important_idx]}")

# Variance explained
variance_explained = eigenvalues / eigenvalues.sum()
print(f"First 3 components: {variance_explained[:3].sum():.2%}")
# If > 95%, can drop last 2 features!
```
</details>

---

## Key Takeaways

✅ **Eigenvectors** = special directions that don't rotate  
✅ **Eigenvalues** = how much eigenvectors get scaled  
✅ **Large eigenvalues** = important directions (keep these!)  
✅ **Small eigenvalues** = unimportant directions (can drop)  
✅ **PCA** = dimensionality reduction using eigenvectors  
✅ **PageRank** = eigenvector of web graph  

---

## What's Next?

You now understand which directions in your data matter most. But how do we actually **use** this for dimensionality reduction?

That's **Principal Component Analysis (PCA)** - the most important application of eigenvalues!

<Card title="Next: Principal Component Analysis (PCA)" icon="arrow-right" href="/courses/math-for-ml-linear-algebra/06-pca">
  Learn to reduce 10 house features to 3 while keeping 95% of information
</Card>
