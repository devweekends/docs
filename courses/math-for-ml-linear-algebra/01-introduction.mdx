---
title: "Introduction to Linear Algebra for ML"
sidebarTitle: "Introduction"
description: "Why linear algebra is the language of machine learning"
icon: "rocket"
---

<Frame>
  <img src="/images/courses/math-for-ml-linear-algebra/linear-algebra-intro-concept.svg" alt="Linear Algebra for Machine Learning" />
</Frame>

# Linear Algebra for Machine Learning

## Have You Ever Wondered...

- How does **Spotify** know that if you like Coldplay, you might also like Imagine Dragons?
- How does **Instagram** apply those fancy filters to your photos in milliseconds?
- How does **Netflix** predict you'll rate a movie 4.2 stars before you've even watched it?
- How does **Google Photos** find all pictures of your dog without you tagging them?

**The answer to ALL of these is Linear Algebra.**

Not calculus. Not statistics. Linear Algebra. The math of lists, tables, and transformations.

<Warning>
**Real Talk**: You probably took linear algebra in college, got confused by abstract proofs about "vector spaces" and "linear independence," passed the exam, and forgot everything.

This time is different. We're going to make you **see** linear algebra, **use** it, and actually **enjoy** it.
</Warning>

<Info>
**Estimated Time**: 16-20 hours  
**Difficulty**: Beginner-friendly (we assume you forgot everything)  
**Prerequisites**: Basic Python, willingness to experiment  
**What You'll Build**: Spotify-style song recommender, Instagram-style filters, Netflix-style rating predictor
</Info>

<Accordion title="üìã Prerequisite Self-Check" icon="clipboard-check">
**Before starting, make sure you can:**

‚úÖ **Python Basics**
- Create and manipulate lists: `my_list = [1, 2, 3]`
- Write simple loops: `for i in range(10)`
- Define and call functions: `def my_func(x): return x * 2`
- Use basic NumPy: `import numpy as np; arr = np.array([1, 2, 3])`

‚úÖ **Math Comfort Level**
- Basic arithmetic (you can use a calculator!)
- Understand coordinates on a graph (x, y)
- Comfortable with the idea that letters can represent numbers

‚ùå **You DON'T need:**
- Previous linear algebra (we start from zero)
- Calculus knowledge
- Matrix manipulation experience
- Any ML/AI background

**If you're missing Python basics**, check out our [Python Crash Course](/courses/python-crash-course) first (4-6 hours).
</Accordion>

<Tip>
**Career Impact**: Linear algebra is the most practical math you'll ever learn for tech. It's used in AI, graphics, data science, finance, and more. Engineers who truly understand it command $150K+ salaries because they can optimize, debug, and innovate where others can't.
</Tip>

---

## The "Aha!" Moment: Everything is a List of Numbers

Here's the secret that unlocks all of machine learning:

**Anything can be turned into a list of numbers. And once it's numbers, math can work magic.**

### Your Favorite Song ‚Üí Numbers

```python
# Spotify represents every song as ~12 numbers
billie_eilish_bad_guy = [
    0.70,   # danceability (0-1)
    0.43,   # energy (0-1)  
    0.56,   # speechiness (0-1)
    0.32,   # acousticness (0-1)
    0.00,   # instrumentalness (0-1)
    0.36,   # liveness (0-1)
    0.68,   # valence/happiness (0-1)
    135.0,  # tempo (BPM)
    # ... more features
]

# This list IS a vector. That's it. A vector is just a list of numbers.
```

### Your Face ‚Üí Numbers

```python
# A 100x100 pixel selfie = 10,000 numbers (brightness of each pixel)
# A neural network can compress this to just 128 numbers that capture "you-ness"

your_face_embedding = [0.23, -0.45, 0.89, ..., 0.12]  # 128 numbers

# Similar faces have similar numbers!
```

### A Netflix Movie ‚Üí Numbers

```python
# Every movie can be described by hidden factors
inception = [
    0.95,   # "mind-bending" factor
    0.80,   # "action" factor  
    0.20,   # "romance" factor
    0.60,   # "visual spectacle" factor
    # ...
]
```

**This is the core insight**: Once everything is numbers, we can:
- **Compare** things (how similar are two songs?)
- **Transform** things (apply a filter to a photo)
- **Find patterns** (what do users who liked X also like?)
- **Compress** things (store a 10MB image in 100KB)

![Everything is Numbers](/images/courses/math-for-ml-linear-algebra/everything-is-numbers.svg)

<Note>
**üîó ML Connection**: This "everything is numbers" insight is the foundation of ALL machine learning:

| ML Concept | Linear Algebra Foundation |
|------------|---------------------------|
| **Word Embeddings** (GPT, BERT) | Words ‚Üí vectors of 768+ numbers |
| **Neural Network Layers** | Matrix multiplication transforms |
| **Attention Mechanism** | Dot products measure relevance |
| **Image Recognition** | Pixels ‚Üí feature vectors ‚Üí classification |
| **Recommendation Systems** | Users & items as vectors in shared space |

Every module in this course connects directly to these ML applications!
</Note>

---

## Who Uses This (Companies & Salaries)

<CardGroup cols={3}>
  <Card title="OpenAI" icon="robot">
    GPT-4 does 100+ trillion matrix operations per prompt. Every AI breakthrough is linear algebra at scale.
  </Card>
  <Card title="Pixar/Disney" icon="wand-magic-sparkles">
    Every frame of Toy Story involves millions of matrix transformations for 3D rendering.
  </Card>
  <Card title="Google Search" icon="magnifying-glass">
    PageRank uses eigenvalues to rank websites. It's why Google won the search wars.
  </Card>
</CardGroup>

| Role | How They Use Linear Algebra | Median Salary |
|------|---------------------------|---------------|
| **ML Engineer** | Neural network weights, transformations, embeddings | $175K |
| **Data Scientist** | PCA, clustering, recommendation systems | $150K |
| **Graphics Engineer** | 3D transformations, shaders, physics | $180K |
| **Quantitative Analyst** | Portfolio optimization, risk modeling | $250K+ |
| **Robotics Engineer** | Kinematics, sensor fusion, SLAM | $165K |

---

## Mathematical Notation Quick Reference

Before we dive in, here's a cheat sheet of the notation you'll encounter. Don't memorize it ‚Äî just come back here when you see something unfamiliar.

<AccordionGroup>
  <Accordion title="Vectors" icon="arrow-right">
    | Symbol | Meaning | Example |
    |--------|---------|---------|
    | $\mathbf{v}$ or $\vec{v}$ | A vector (bold or arrow) | $\mathbf{v} = [3, 4, 5]$ |
    | $v_i$ | The $i$-th element of vector $\mathbf{v}$ | $v_2 = 4$ |
    | $\|\mathbf{v}\|$ | Length (magnitude) of vector | $\|\mathbf{v}\| = \sqrt{3^2 + 4^2 + 5^2} = \sqrt{50}$ |
    | $\mathbf{a} \cdot \mathbf{b}$ | Dot product | $[1,2] \cdot [3,4] = 1(3) + 2(4) = 11$ |
    | $\mathbf{a}^T$ | Transpose (row ‚Üî column) | $[1, 2, 3]^T = \begin{bmatrix}1\\2\\3\end{bmatrix}$ |
  </Accordion>

  <Accordion title="Matrices" icon="grid">
    | Symbol | Meaning | Example |
    |--------|---------|---------|
    | $A$, $B$, $M$ | Matrices (capital letters) | $A = \begin{bmatrix}1 & 2\\3 & 4\end{bmatrix}$ |
    | $A_{ij}$ or $a_{ij}$ | Element at row $i$, column $j$ | $A_{12} = 2$ |
    | $A^T$ | Transpose (flip rows/columns) | $\begin{bmatrix}1 & 2\\3 & 4\end{bmatrix}^T = \begin{bmatrix}1 & 3\\2 & 4\end{bmatrix}$ |
    | $A^{-1}$ | Inverse of matrix $A$ | $AA^{-1} = I$ |
    | $I$ | Identity matrix | $\begin{bmatrix}1 & 0\\0 & 1\end{bmatrix}$ |
    | $\det(A)$ or $|A|$ | Determinant | $\det\begin{bmatrix}a & b\\c & d\end{bmatrix} = ad - bc$ |
  </Accordion>

  <Accordion title="Operations & Summations" icon="calculator">
    | Symbol | Meaning | Example |
    |--------|---------|---------|
    | $\sum_{i=1}^{n}$ | Sum from $i=1$ to $n$ | $\sum_{i=1}^{3} i = 1 + 2 + 3 = 6$ |
    | $\prod_{i=1}^{n}$ | Product from $i=1$ to $n$ | $\prod_{i=1}^{3} i = 1 \times 2 \times 3 = 6$ |
    | $\mathbb{R}$ | Real numbers | $x \in \mathbb{R}$ means $x$ is a real number |
    | $\mathbb{R}^n$ | $n$-dimensional real space | $\mathbf{v} \in \mathbb{R}^3$ is a 3D vector |
  </Accordion>

  <Accordion title="Special Notation" icon="star">
    | Symbol | Meaning | ML Context |
    |--------|---------|------------|
    | $\lambda$ (lambda) | Eigenvalue | How much a direction stretches |
    | $\sigma$ (sigma) | Singular value | Importance of a pattern in SVD |
    | $\nabla$ (nabla/del) | Gradient operator | Direction of steepest change |
    | $\theta$ (theta) | Model parameters | Weights in neural networks |
    | $\approx$ | Approximately equal | $\pi \approx 3.14$ |
  </Accordion>
</AccordionGroup>

### Quick Math Examples

**Vector Addition** ‚Äî Add component by component:
$$
\begin{bmatrix}1\\2\\3\end{bmatrix} + \begin{bmatrix}4\\5\\6\end{bmatrix} = \begin{bmatrix}1+4\\2+5\\3+6\end{bmatrix} = \begin{bmatrix}5\\7\\9\end{bmatrix}
$$

**Scalar Multiplication** ‚Äî Multiply each component:
$$
3 \times \begin{bmatrix}1\\2\\3\end{bmatrix} = \begin{bmatrix}3\\6\\9\end{bmatrix}
$$

**Dot Product** ‚Äî Multiply corresponding elements and sum:
$$
\begin{bmatrix}1\\2\\3\end{bmatrix} \cdot \begin{bmatrix}4\\5\\6\end{bmatrix} = (1)(4) + (2)(5) + (3)(6) = 4 + 10 + 18 = 32
$$

**Matrix √ó Vector** ‚Äî Each output is a dot product:
$$
\begin{bmatrix}1 & 2\\3 & 4\end{bmatrix} \begin{bmatrix}5\\6\end{bmatrix} = \begin{bmatrix}(1)(5)+(2)(6)\\(3)(5)+(4)(6)\end{bmatrix} = \begin{bmatrix}17\\39\end{bmatrix}
$$

<Tip>
**Pro Tip**: When you see scary-looking equations in ML papers, break them down into these simple operations. Most "complex" formulas are just combinations of addition, multiplication, and dot products!
</Tip>

---

## What You'll Actually Learn (And Why You'll Care)

<AccordionGroup>
  <Accordion title="Module 1: Vectors" icon="arrow-right">
    **Real-World Examples You Already Know:**
    - **GPS Navigation**: Your location is a vector `[latitude, longitude]`. Distance between two places? Vector math.
    - **Fitness Trackers**: Your daily stats `[steps, calories, heart_rate, sleep_hours]` ‚Äî that's a vector describing your day.
    - **Job Matching**: LinkedIn represents you as `[skills, experience, education, location]` and finds similar candidates.
    - **Dating Apps**: Tinder/Hinge match you based on preference vectors. Similar vectors = potential match.
    
    **What You'll Build**: A similarity search engine (works for songs, jobs, or anything).
  </Accordion>
  
  <Accordion title="Module 2: Matrices" icon="grid">
    **Real-World Examples You Already Know:**
    - **Photo Editing**: Every Instagram filter is a matrix multiplication. Brightness, contrast, blur ‚Äî all matrix operations.
    - **Video Games**: When you rotate your character, move the camera, or zoom in ‚Äî that's matrix math happening 60 times per second.
    - **Spreadsheets**: Excel pivot tables, VLOOKUP across sheets ‚Äî you're doing matrix operations without knowing it.
    - **Maps/GPS**: Transforming GPS coordinates to screen pixels involves matrix multiplication.
    
    **What You'll Build**: Your own photo filter app and a 2D game transformation engine.
  </Accordion>
  
  <Accordion title="Module 3: Eigenvalues & PCA" icon="compress">
    **Real-World Examples You Already Know:**
    - **Surveys**: 50 questions reduce to 3-4 "personality types" ‚Äî that's PCA finding the key dimensions.
    - **Stock Market**: Hundreds of stocks move together because of 5-10 hidden factors (economy, interest rates, oil prices).
    - **Customer Segments**: Millions of customers cluster into 5-6 types based on purchasing patterns.
    - **Compression**: JPEG images keep 90% quality with 10% file size by keeping only the important eigenvectors.
    
    **What You'll Build**: Image compressor and customer segmentation system.
  </Accordion>
  
  <Accordion title="Module 4: SVD & Recommendations" icon="stars">
    **Real-World Examples You Already Know:**
    - **"Customers who bought X also bought Y"**: Amazon uses matrix factorization to find these patterns.
    - **YouTube Recommendations**: "Because you watched X" ‚Äî they decomposed your viewing history.
    - **Spell Check**: "Did you mean...?" often uses SVD to find similar words.
    - **Fraud Detection**: Normal transactions form patterns; fraud breaks those patterns.
    
    **What You'll Build**: A working recommendation engine using real MovieLens data.
  </Accordion>
</AccordionGroup>

---

## Your Learning Journey

<Steps>
  <Step title="Week 1-2: Vectors">
    Learn to see everything as vectors. Build a song/image similarity search engine.
  </Step>
  <Step title="Week 2-3: Matrices">
    Master transformations. Build Instagram-style photo filters from scratch.
  </Step>
  <Step title="Week 3-4: Eigenvalues & PCA">
    Find hidden patterns. Compress images, reduce dimensions, and understand what your data really contains.
  </Step>
  <Step title="Week 4-5: SVD & Recommendations">
    The crown jewel. Build a Netflix-style recommendation engine that predicts ratings.
  </Step>
</Steps>

---

## Why Most Math Courses Fail (And How This One's Different)

<Tabs>
  <Tab title="Traditional Course">
    1. Definition of a vector space
    2. Axioms of vector addition  
    3. Proof of linear independence
    4. Abstract theorem
    5. *"Exercise left to the reader"*
    6. **Student falls asleep**
  </Tab>
  <Tab title="This Course">
    1. **"How does your GPS calculate the fastest route?"**
    2. Locations are vectors. Distances are vector operations.
    3. Here's how Google Maps actually works.
    4. **Here's working code you can run**
    5. **Now modify it for your own project**
    6. *"Oh, THAT's what a dot product does!"*
  </Tab>
</Tabs>

<Tip>
**Our Promise**: Every concept will be:
- Explained with a **real-world app** you use daily
- Visualized with **clear diagrams**
- Coded in **Python you can run**
- Practiced with **projects you'll want to show off**
</Tip>

---

## Prerequisites (Honestly, Not Much)

**What You Need:**
- Basic Python: Variables, lists, loops, functions  
- Willingness to experiment: Run code, break things, learn  
- Curiosity: Wonder how apps work under the hood  

**What You DON'T Need:**
- Previous linear algebra knowledge (we start from scratch)  
- Mathematical proofs (we focus on intuition and code)  
- Perfect grades in math (many engineers struggle with math ‚Äî that's okay!)  

---

## Setup (5 Minutes)

```bash
# Create a new environment and install what we need
pip install numpy matplotlib jupyter scikit-learn pillow plotly ipywidgets

# Start Jupyter to follow along
jupyter notebook
```

That's it. No complex setup. Let's go.

<Tip>
**üéÆ Interactive Learning**: Throughout this course, you'll find interactive visualizations marked with üéÆ. These let you:
- Drag vectors and see transformations in real-time
- Adjust parameters with sliders to build intuition
- Experiment without breaking anything

We've added `plotly` and `ipywidgets` for these interactive elements. They're optional but highly recommended!
</Tip>

---

## The Projects You'll Build

By the end of this course, you'll have a portfolio of **real, working projects**:

<CardGroup cols={2}>
  <Card title="Song Recommender" icon="music">
    Find similar songs using vector similarity. Input: a song you like. Output: 10 songs you'll probably love.
  </Card>
  <Card title="Photo Filter App" icon="camera">
    Apply blur, sharpen, edge detection, and custom effects using matrix operations.
  </Card>
  <Card title="Image Compressor" icon="compress">
    Compress images to 10% of their size while keeping them recognizable. Understand how JPEG works.
  </Card>
  <Card title="Movie Recommender" icon="film">
    Predict user ratings for movies they haven't seen. The actual technique Netflix uses.
  </Card>
</CardGroup>

---

## Quick Taste: Vector Similarity in Action

Before we dive deep, let's see the magic in action. This is what you'll fully understand by the end of Module 1:

```python
import numpy as np

# Three songs represented as vectors [energy, danceability, acousticness]
blinding_lights = np.array([0.73, 0.51, 0.00])  # The Weeknd
levitating = np.array([0.69, 0.70, 0.03])        # Dua Lipa
someone_like_you = np.array([0.34, 0.50, 0.75])  # Adele

def similarity(song_a, song_b):
    """Cosine similarity - how similar are two vectors?"""
    return np.dot(song_a, song_b) / (np.linalg.norm(song_a) * np.linalg.norm(song_b))

print(f"Blinding Lights vs Levitating: {similarity(blinding_lights, levitating):.2f}")
print(f"Blinding Lights vs Someone Like You: {similarity(blinding_lights, someone_like_you):.2f}")

# Output:
# Blinding Lights vs Levitating: 0.94  (very similar! both are upbeat pop)
# Blinding Lights vs Someone Like You: 0.54  (less similar - different vibes)
```

**That's it.** That's the core of how Spotify recommendations work. Vectors + similarity.

Now imagine doing this with 100 dimensions instead of 3, and millions of songs. That's what you'll build.  

---

## By the End of This Course

You will:

‚úÖ **See** vectors and matrices everywhere (in apps, in data, in neural networks)  
‚úÖ **Build** 4 portfolio-worthy ML projects from scratch  
‚úÖ **Read** ML papers and actually understand the notation  
‚úÖ **Debug** ML models because you understand what's happening inside  
‚úÖ **Explain** to others why linear algebra powers AI  

Most importantly: You'll **stop being scared of math** in ML papers. When you see:

$$\mathbf{y} = W\mathbf{x} + \mathbf{b}$$

You'll think: *"Oh, that's just transforming a vector with a matrix. Like applying a filter to an image."*

---

## Ready?

Let's stop talking and start building. The next module introduces vectors by asking a simple question:

**"How does Spotify know what song to play next?"**

<Card title="Next: Vectors ‚Äî The Language of Similarity" icon="arrow-right" href="/courses/math-for-ml-linear-algebra/02-vectors">
  Learn what vectors really are, why everything is a vector, and how to measure similarity between any two things.
</Card>
