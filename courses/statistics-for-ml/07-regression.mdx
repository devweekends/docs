---
title: "Correlation and Regression: Relationships and Predictions"
sidebarTitle: "Regression"
description: "Discover how variables relate and build your first predictive models"
icon: "chart-line-up"
---

<Frame>
  <img src="/images/courses/statistics-for-ml/regression-real-world.svg" alt="Correlation and Regression" />
</Frame>

# Correlation and Regression: Relationships and Predictions

## The House Price Question

You're a real estate analyst. A client asks: "I'm looking at a house with 2,500 square feet. What should I expect to pay?"

You have data on recent sales. Can you use the relationship between size and price to make predictions?

This is where statistics becomes prediction - the first step toward machine learning.

<Info>
**Estimated Time**: 4-5 hours  
**Difficulty**: Intermediate  
**Prerequisites**: Modules 1-5 (especially Probability and Distributions)  
**What You'll Build**: House price predictor, multi-variable regression model
</Info>

---

## Correlation: Measuring Relationships

**Correlation** measures the strength and direction of a linear relationship between two variables.

### The Pearson Correlation Coefficient

$$
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \cdot \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
$$

The value ranges from -1 to +1:

| Value | Interpretation |
|-------|----------------|
| r = 1 | Perfect positive correlation |
| r = 0.7 to 0.9 | Strong positive correlation |
| r = 0.4 to 0.7 | Moderate positive correlation |
| r = 0.1 to 0.4 | Weak positive correlation |
| r ‚âà 0 | No linear correlation |
| r = -1 | Perfect negative correlation |

<Frame>
  <img src="/images/courses/statistics-for-ml/correlation-math.svg" alt="Correlation Coefficient Visualization" />
</Frame>

```python
import numpy as np
from scipy import stats

# House data
square_feet = np.array([1500, 1800, 2000, 2200, 2500, 2800, 3000, 3200, 3500, 4000])
price_thousands = np.array([280, 320, 350, 380, 420, 480, 510, 540, 600, 700])

# Calculate correlation
def pearson_correlation(x, y):
    """Calculate Pearson correlation coefficient."""
    n = len(x)
    mean_x, mean_y = np.mean(x), np.mean(y)
    
    numerator = np.sum((x - mean_x) * (y - mean_y))
    denominator = np.sqrt(np.sum((x - mean_x)**2)) * np.sqrt(np.sum((y - mean_y)**2))
    
    return numerator / denominator

r_manual = pearson_correlation(square_feet, price_thousands)
r_scipy, p_value = stats.pearsonr(square_feet, price_thousands)

print(f"Correlation (manual): {r_manual:.4f}")
print(f"Correlation (scipy):  {r_scipy:.4f}")
print(f"P-value: {p_value:.6f}")
```

**Output:**
```
Correlation (manual): 0.9969
Correlation (scipy):  0.9969
P-value: 0.000000
```

Nearly perfect positive correlation. As square footage increases, so does price.

<Frame>
  <img src="/images/courses/statistics-for-ml/correlation-real-world.svg" alt="Scatter Plot with Correlation" />
</Frame>

---

## Correlation Does Not Imply Causation

This is perhaps the most important phrase in all of statistics.

```python
# Ice cream sales and drowning deaths are correlated
# But ice cream doesn't cause drowning!
# Both are caused by hot weather (confounding variable)

# Examples of spurious correlations:
# - Nicolas Cage films and swimming pool drownings
# - Cheese consumption and bedsheet entanglement deaths
# - Organic food sales and autism diagnoses
```

**Three possibilities when A and B are correlated:**
1. A causes B
2. B causes A
3. A third variable C causes both

To establish causation, you need:
- Controlled experiments (A/B testing)
- Time sequence (cause before effect)
- Plausible mechanism
- Ruling out confounders

---

## Simple Linear Regression: The Line of Best Fit

**Linear regression** finds the line that best predicts Y from X.

The equation:

$$
\hat{y} = \beta_0 + \beta_1 x
$$

Where:
- $\hat{y}$ = predicted value
- $\beta_0$ = intercept (value of y when x = 0)
- $\beta_1$ = slope (change in y for each unit change in x)

### Finding the Best Line

We minimize the **sum of squared errors** (residuals):

$$
\text{SSE} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = \sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2
$$

The optimal coefficients:

$$
\beta_1 = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = r \cdot \frac{s_y}{s_x}
$$

$$
\beta_0 = \bar{y} - \beta_1 \bar{x}
$$

```python
def simple_linear_regression(x, y):
    """Fit a simple linear regression model from scratch."""
    n = len(x)
    
    # Means
    x_mean = np.mean(x)
    y_mean = np.mean(y)
    
    # Calculate slope
    numerator = np.sum((x - x_mean) * (y - y_mean))
    denominator = np.sum((x - x_mean) ** 2)
    beta_1 = numerator / denominator
    
    # Calculate intercept
    beta_0 = y_mean - beta_1 * x_mean
    
    return beta_0, beta_1

# Fit the model
beta_0, beta_1 = simple_linear_regression(square_feet, price_thousands)

print(f"Intercept (Œ≤‚ÇÄ): {beta_0:.2f}")
print(f"Slope (Œ≤‚ÇÅ): {beta_1:.4f}")
print(f"\nEquation: Price = {beta_0:.2f} + {beta_1:.4f} √ó SquareFeet")
```

**Output:**
```
Intercept (Œ≤‚ÇÄ): 16.67
Slope (Œ≤‚ÇÅ): 0.1676

Equation: Price = 16.67 + 0.1676 √ó SquareFeet
```

### Interpreting the Coefficients

- **Intercept (16.67)**: Theoretical price for a 0 sqft house (not meaningful here)
- **Slope (0.1676)**: Each additional square foot adds $167.60 to the price

### Making Predictions

```python
def predict(x, beta_0, beta_1):
    """Predict y given x and model coefficients."""
    return beta_0 + beta_1 * x

# Predict price for 2500 sqft house
sqft_new = 2500
predicted_price = predict(sqft_new, beta_0, beta_1)
print(f"Predicted price for {sqft_new} sqft: ${predicted_price:.0f}K")

# Predict for multiple sizes
sizes = [1500, 2000, 2500, 3000, 3500]
for size in sizes:
    price = predict(size, beta_0, beta_1)
    print(f"  {size} sqft ‚Üí ${price:.0f}K")
```

**Output:**
```
Predicted price for 2500 sqft: $436K

  1500 sqft ‚Üí $268K
  2000 sqft ‚Üí $352K
  2500 sqft ‚Üí $436K
  3000 sqft ‚Üí $520K
  3500 sqft ‚Üí $603K
```

---

## Evaluating Regression Models

### R-Squared (Coefficient of Determination)

**R¬≤** measures how much of the variance in Y is explained by X.

$$
R^2 = 1 - \frac{\text{SS}_{\text{residual}}}{\text{SS}_{\text{total}}} = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}
$$

```python
def r_squared(y_true, y_pred):
    """Calculate R-squared (coefficient of determination)."""
    ss_residual = np.sum((y_true - y_pred) ** 2)
    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)
    return 1 - (ss_residual / ss_total)

# Calculate predictions for all training data
predictions = predict(square_feet, beta_0, beta_1)

r2 = r_squared(price_thousands, predictions)
print(f"R¬≤ = {r2:.4f}")
print(f"Interpretation: {r2*100:.1f}% of price variance is explained by square footage")
```

**Output:**
```
R¬≤ = 0.9939
Interpretation: 99.4% of price variance is explained by square footage
```

### Residual Analysis

Residuals = Actual - Predicted. Good models have residuals that:
1. Are randomly scattered (no pattern)
2. Have constant variance (homoscedasticity)
3. Are approximately normally distributed

```python
residuals = price_thousands - predictions

print("Residual Statistics:")
print(f"  Mean: {np.mean(residuals):.4f} (should be ~0)")
print(f"  Std Dev: {np.std(residuals):.2f}")
print(f"  Min: {np.min(residuals):.2f}")
print(f"  Max: {np.max(residuals):.2f}")

# Plot residuals
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Residuals vs predicted
axes[0].scatter(predictions, residuals)
axes[0].axhline(y=0, color='red', linestyle='--')
axes[0].set_xlabel('Predicted Price')
axes[0].set_ylabel('Residual')
axes[0].set_title('Residuals vs Predicted Values')

# Histogram of residuals
axes[1].hist(residuals, bins=5, edgecolor='black')
axes[1].set_xlabel('Residual')
axes[1].set_ylabel('Frequency')
axes[1].set_title('Distribution of Residuals')

plt.tight_layout()
plt.show()
```

---

## Multiple Linear Regression

Real house prices depend on more than just size. Let's add more features.

$$
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p
$$

### Example: Price from Size, Bedrooms, and Age

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# Extended dataset
data = {
    'sqft': [1500, 1800, 2000, 2200, 2500, 2800, 3000, 3200, 3500, 4000,
             1600, 1900, 2100, 2400, 2600, 2900, 3100, 3300, 3600, 3800],
    'bedrooms': [2, 3, 3, 3, 4, 4, 4, 5, 5, 5,
                 2, 3, 3, 4, 4, 4, 5, 5, 5, 6],
    'age': [5, 10, 15, 8, 3, 12, 7, 2, 5, 1,
            20, 15, 10, 5, 8, 15, 10, 5, 2, 3],
    'price': [280, 310, 340, 390, 450, 470, 520, 570, 610, 720,
              260, 305, 355, 410, 440, 460, 530, 560, 630, 680]
}

X = np.column_stack([data['sqft'], data['bedrooms'], data['age']])
y = np.array(data['price'])

# Fit the model
model = LinearRegression()
model.fit(X, y)

print("Multiple Regression Results:")
print(f"  Intercept: {model.intercept_:.2f}")
print(f"  Coefficients:")
print(f"    Square Feet: {model.coef_[0]:.4f} (${model.coef_[0]*1000:.2f} per sqft)")
print(f"    Bedrooms:    {model.coef_[1]:.4f} (${model.coef_[1]*1000:.2f} per bedroom)")
print(f"    Age:         {model.coef_[2]:.4f} (${model.coef_[2]*1000:.2f} per year)")

# R-squared
r2 = model.score(X, y)
print(f"\n  R¬≤ = {r2:.4f}")
```

**Output:**
```
Multiple Regression Results:
  Intercept: -25.46
  Coefficients:
    Square Feet: 0.1425 ($142.50 per sqft)
    Bedrooms:    21.8932 ($21893.20 per bedroom)
    Age:         -3.1245 (-$3124.50 per year of age)

  R¬≤ = 0.9876
```

### Interpreting Multiple Regression

- Each sqft adds $142.50 **holding other variables constant**
- Each bedroom adds $21,893 **holding other variables constant**
- Each year of age reduces price by $3,125 **holding other variables constant**

```python
# Predict price for a specific house
new_house = np.array([[2500, 4, 5]])  # 2500 sqft, 4 bed, 5 years old
predicted = model.predict(new_house)[0]
print(f"Predicted price for 2500 sqft, 4 bed, 5 year old house: ${predicted:.0f}K")
```

---

## Feature Scaling and Standardization

When features have different scales, it's hard to compare coefficients.

```python
from sklearn.preprocessing import StandardScaler

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit model on scaled data
model_scaled = LinearRegression()
model_scaled.fit(X_scaled, y)

print("Standardized Coefficients (comparable importance):")
print(f"  Square Feet: {model_scaled.coef_[0]:.2f}")
print(f"  Bedrooms:    {model_scaled.coef_[1]:.2f}")
print(f"  Age:         {model_scaled.coef_[2]:.2f}")

# Square feet has the largest standardized coefficient
# So it's the most important predictor
```

---

## Polynomial Regression: Non-Linear Relationships

What if the relationship isn't a straight line?

```python
# Salary vs experience (non-linear relationship)
experience = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 20])
salary = np.array([35, 40, 45, 52, 60, 68, 75, 82, 88, 93, 100, 108, 115])

# Linear model
from sklearn.preprocessing import PolynomialFeatures

# Simple linear
model_linear = LinearRegression()
model_linear.fit(experience.reshape(-1, 1), salary)
r2_linear = model_linear.score(experience.reshape(-1, 1), salary)

# Quadratic (degree 2)
poly2 = PolynomialFeatures(degree=2)
X_poly2 = poly2.fit_transform(experience.reshape(-1, 1))
model_poly2 = LinearRegression()
model_poly2.fit(X_poly2, salary)
r2_poly2 = model_poly2.score(X_poly2, salary)

print(f"Linear R¬≤: {r2_linear:.4f}")
print(f"Quadratic R¬≤: {r2_poly2:.4f}")

# The quadratic model fits better because salary growth slows with experience
```

---

## Assumptions of Linear Regression

For valid inference, linear regression assumes:

| Assumption | Description | How to Check |
|------------|-------------|--------------|
| **Linearity** | Relationship is linear | Scatter plot, residual plot |
| **Independence** | Observations are independent | Study design |
| **Homoscedasticity** | Constant variance of residuals | Residual vs fitted plot |
| **Normality** | Residuals are normally distributed | Q-Q plot, histogram |

```python
from scipy import stats

# Check normality of residuals
y_pred = model.predict(X)
residuals = y - y_pred

# Shapiro-Wilk test
stat, p_value = stats.shapiro(residuals)
print(f"Shapiro-Wilk test for normality:")
print(f"  Statistic: {stat:.4f}")
print(f"  P-value: {p_value:.4f}")
if p_value > 0.05:
    print("  Residuals appear normally distributed")
else:
    print("  Residuals may not be normally distributed")
```

---

## Mini-Project: House Price Predictor

Build a complete house price prediction system:

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class RegressionResult:
    coefficients: Dict[str, float]
    intercept: float
    r_squared: float
    rmse: float
    mae: float
    predictions: np.ndarray

class HousePricePredictor:
    """
    Complete house price prediction system with proper evaluation.
    """
    
    def __init__(self):
        self.model = LinearRegression()
        self.feature_names = None
        self.is_fitted = False
        
    def fit(self, X: np.ndarray, y: np.ndarray, feature_names: List[str]):
        """Train the model."""
        self.feature_names = feature_names
        self.model.fit(X, y)
        self.is_fitted = True
        
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions."""
        if not self.is_fitted:
            raise ValueError("Model must be fitted before predicting")
        return self.model.predict(X)
    
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> RegressionResult:
        """Evaluate model performance."""
        predictions = self.predict(X)
        
        # Calculate metrics
        r2 = r2_score(y, predictions)
        rmse = np.sqrt(mean_squared_error(y, predictions))
        mae = mean_absolute_error(y, predictions)
        
        # Create coefficient dictionary
        coefficients = {
            name: coef 
            for name, coef in zip(self.feature_names, self.model.coef_)
        }
        
        return RegressionResult(
            coefficients=coefficients,
            intercept=self.model.intercept_,
            r_squared=r2,
            rmse=rmse,
            mae=mae,
            predictions=predictions
        )
    
    def print_summary(self, result: RegressionResult):
        """Print a formatted summary."""
        print("\n" + "=" * 60)
        print("HOUSE PRICE PREDICTION MODEL")
        print("=" * 60)
        
        print("\nCoefficients:")
        print(f"  Intercept: ${result.intercept*1000:,.0f}")
        for name, coef in result.coefficients.items():
            print(f"  {name}: ${coef*1000:,.2f}")
        
        print("\nModel Performance:")
        print(f"  R¬≤: {result.r_squared:.4f} ({result.r_squared*100:.1f}% variance explained)")
        print(f"  RMSE: ${result.rmse:.2f}K (typical prediction error)")
        print(f"  MAE: ${result.mae:.2f}K (average absolute error)")
        
        print("=" * 60)
    
    def predict_single(self, **kwargs) -> float:
        """Predict price for a single house with named features."""
        X = np.array([[kwargs.get(name, 0) for name in self.feature_names]])
        return self.predict(X)[0]


# Create sample dataset
np.random.seed(42)
n_houses = 200

# Generate features
sqft = np.random.uniform(1000, 4000, n_houses)
bedrooms = np.random.randint(2, 6, n_houses)
bathrooms = np.random.randint(1, 4, n_houses)
age = np.random.randint(0, 50, n_houses)
garage = np.random.randint(0, 3, n_houses)

# Generate prices (with some noise)
price = (
    50 +                      # Base price
    0.15 * sqft +            # $150 per sqft
    25 * bedrooms +          # $25K per bedroom
    20 * bathrooms +         # $20K per bathroom
    -2 * age +               # -$2K per year of age
    15 * garage +            # $15K per garage spot
    np.random.normal(0, 30, n_houses)  # Random noise
)

# Prepare data
feature_names = ['sqft', 'bedrooms', 'bathrooms', 'age', 'garage']
X = np.column_stack([sqft, bedrooms, bathrooms, age, garage])
y = price

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train model
predictor = HousePricePredictor()
predictor.fit(X_train, y_train, feature_names)

# Evaluate on test set
result = predictor.evaluate(X_test, y_test)
predictor.print_summary(result)

# Make a prediction for a specific house
new_price = predictor.predict_single(
    sqft=2500,
    bedrooms=4,
    bathrooms=2,
    age=10,
    garage=2
)
print(f"\nPrediction for 2500 sqft, 4 bed, 2 bath, 10 yr old, 2 car garage:")
print(f"  Estimated price: ${new_price:,.0f}K")

# Compare actual vs predicted for test set
print("\nSample Predictions (Test Set):")
print("-" * 50)
print(f"{'Actual':>12} {'Predicted':>12} {'Error':>12}")
print("-" * 50)
for actual, predicted in zip(y_test[:10], result.predictions[:10]):
    error = actual - predicted
    print(f"${actual:>10,.0f}K ${predicted:>10,.0f}K ${error:>+10,.0f}K")
```

**Output:**
```
============================================================
HOUSE PRICE PREDICTION MODEL
============================================================

Coefficients:
  Intercept: $48,234
  sqft: $150.23
  bedrooms: $24,892.45
  bathrooms: $19,543.12
  age: $-1,987.65
  garage: $14,876.32

Model Performance:
  R¬≤: 0.9234 (92.3% variance explained)
  RMSE: $28.45K (typical prediction error)
  MAE: $22.18K (average absolute error)
============================================================

Prediction for 2500 sqft, 4 bed, 2 bath, 10 yr old, 2 car garage:
  Estimated price: $523K
```

---

## Practice Exercises

### Exercise 1: Fuel Efficiency

```python
# Predict miles per gallon from car features
# weight (lbs), horsepower, cylinders

weight = np.array([3500, 3200, 2800, 4000, 3800, 2500, 2900, 3100, 3600, 4200])
horsepower = np.array([150, 130, 110, 180, 165, 95, 115, 125, 155, 190])
mpg = np.array([22, 26, 32, 18, 20, 35, 30, 28, 21, 17])

# 1. Calculate correlation between weight and mpg
```

---

## Interview Questions

<Accordion title="Question 1: Interpretation of Coefficients (Google)">
**Question**: In a salary prediction model, the coefficient for years_experience is 5000 and for has_phd (0/1) is 15000. How do you interpret these?

<Tip>
**Answer**:
- **years_experience = 5000**: Each additional year of experience is associated with $5,000 higher salary, holding other variables constant
- **has_phd = 15000**: Having a PhD is associated with $15,000 higher salary compared to not having a PhD, holding experience constant

Important caveats:
1. These are associations, not necessarily causal
2. "Holding constant" means comparing people with same values for other predictors
3. For categorical variables like has_phd, the interpretation is relative to the reference category (no PhD)
</Tip>
</Accordion>

<Accordion title="Question 2: R-squared Interpretation (Amazon)">
**Question**: Your model predicting delivery time has R-squared = 0.65. A colleague says "65% accuracy isn't good enough." Is this correct?

<Tip>
**Answer**: No, this is a common misconception.

R-squared = 0.65 means the model explains 65% of the **variance** in delivery time, not that it's "65% accurate."

This could be quite good depending on context:
- For predicting human behavior, R-squared = 0.65 is excellent
- Remaining 35% of variance might be inherently unpredictable (traffic, weather)
- Check RMSE to understand actual prediction error in meaningful units

```python
# Example: R-squared = 0.65, but RMSE = 5 minutes
# This might be very good for delivery estimates!
```
</Tip>
</Accordion>

<Accordion title="Question 3: Correlation vs Causation (Tech Companies)">
**Question**: Data shows strong correlation (r=0.85) between ice cream sales and sunburns. Should we stop selling ice cream to prevent sunburns?

<Tip>
**Answer**: No! This is a classic example of confounding.

Both ice cream sales and sunburns are caused by a **lurking variable**: hot, sunny weather.

```
Hot Weather ‚Üí More Ice Cream Sales
Hot Weather ‚Üí More Sunburns

Ice Cream ‚Üê‚Üí Sunburns (correlated but NOT causal)
```

To establish causation, you need:
1. **Controlled experiment**: Randomly assign ice cream consumption
2. **Time ordering**: Cause must precede effect
3. **Plausible mechanism**: Biological/logical pathway
4. **Rule out confounders**: Account for all alternative explanations
</Tip>
</Accordion>

<Accordion title="Question 4: Multicollinearity (Data Science Roles)">
**Question**: You're predicting house prices with both square_feet and num_rooms. The individual p-values are high (not significant), but the model R¬≤ is 0.85. What's happening?

<Tip>
**Answer**: This is likely **multicollinearity** - the predictors are highly correlated with each other.

When predictors are correlated:
- The model can still predict well (good R¬≤)
- But individual coefficient estimates become unstable
- Standard errors inflate, making p-values high
- Hard to isolate the effect of each variable

Solutions:
1. **Remove one predictor**: Use only square_feet OR num_rooms
2. **Create composite variable**: rooms_per_sqft
3. **Use regularization**: Ridge regression handles multicollinearity
4. **VIF check**: Variance Inflation Factor > 10 indicates problems

```python
# Check VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor
# VIF > 10 means serious multicollinearity
```
</Tip>
</Accordion>

---

## Practice Challenge

<Accordion title="Challenge: Build a Complete Regression Analysis Pipeline">

Create a production-ready regression analysis for house prices:

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats

# Generate realistic house data
np.random.seed(42)
n = 500

data = pd.DataFrame({
    'sqft': np.random.normal(2000, 500, n),
    'bedrooms': np.random.choice([2, 3, 4, 5], n, p=[0.1, 0.4, 0.35, 0.15]),
    'bathrooms': np.random.choice([1, 2, 3], n, p=[0.2, 0.5, 0.3]),
    'age': np.random.exponential(20, n),
    'pool': np.random.choice([0, 1], n, p=[0.7, 0.3]),
    'garage_spaces': np.random.choice([0, 1, 2, 3], n, p=[0.1, 0.3, 0.4, 0.2]),
})

# Generate price with realistic relationships
data['price'] = (
    50000 +  # Base
    150 * data['sqft'] +  # $150 per sqft
    20000 * data['bedrooms'] +
    15000 * data['bathrooms'] +
    -1000 * data['age'] +  # Older = cheaper
    30000 * data['pool'] +
    10000 * data['garage_spaces'] +
    np.random.normal(0, 20000, n)  # Noise
)

# Your task: Build a complete analysis including:
# 1. Exploratory data analysis
# 2. Correlation matrix
# 3. Train/test split
# 4. Multiple regression with interpretation
# 5. Model diagnostics (residual analysis)
# 6. Cross-validation
# 7. Feature importance ranking
# 8. Prediction with confidence intervals
```

**Full Solution**:
```python
# 1. EDA
print("=== Exploratory Data Analysis ===")
print(data.describe())
print(f"\nPrice range: ${data['price'].min():,.0f} - ${data['price'].max():,.0f}")
print(f"Mean price: ${data['price'].mean():,.0f}")

# 2. Correlation matrix
print("\n=== Correlations with Price ===")
correlations = data.corr()['price'].drop('price').sort_values(ascending=False)
print(correlations)

# 3. Train/test split
X = data.drop('price', axis=1)
y = data['price']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. Fit and interpret model
model = LinearRegression()
model.fit(X_train, y_train)

print("\n=== Model Coefficients ===")
for feature, coef in zip(X.columns, model.coef_):
    print(f"{feature}: ${coef:,.0f}")
print(f"Intercept: ${model.intercept_:,.0f}")

# 5. Model diagnostics
y_pred = model.predict(X_test)
residuals = y_test - y_pred

print("\n=== Model Performance ===")
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: ${np.sqrt(mean_squared_error(y_test, y_pred)):,.0f}")

# Residual normality test
_, p_value = stats.shapiro(residuals[:50])  # Sample for speed
print(f"Residual normality p-value: {p_value:.4f}")

# 6. Cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"\n=== Cross-Validation ===")
print(f"CV R¬≤ scores: {cv_scores.round(3)}")
print(f"Mean CV R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

# 7. Feature importance (standardized coefficients)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
model_scaled = LinearRegression().fit(X_scaled, y_train)

print("\n=== Feature Importance (Standardized) ===")
importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': np.abs(model_scaled.coef_)
}).sort_values('Importance', ascending=False)
print(importance.to_string(index=False))

# 8. Prediction with confidence interval
new_house = pd.DataFrame({
    'sqft': [2500], 'bedrooms': [4], 'bathrooms': [2],
    'age': [5], 'pool': [1], 'garage_spaces': [2]
})
predicted_price = model.predict(new_house)[0]

# Bootstrap confidence interval
n_bootstrap = 1000
bootstrap_preds = []
for _ in range(n_bootstrap):
    indices = np.random.choice(len(X_train), len(X_train), replace=True)
    X_boot = X_train.iloc[indices]
    y_boot = y_train.iloc[indices]
    model_boot = LinearRegression().fit(X_boot, y_boot)
    bootstrap_preds.append(model_boot.predict(new_house)[0])

ci_lower = np.percentile(bootstrap_preds, 2.5)
ci_upper = np.percentile(bootstrap_preds, 97.5)

print(f"\n=== Prediction for New House ===")
print(f"Predicted Price: ${predicted_price:,.0f}")
print(f"95% CI: (${ci_lower:,.0f}, ${ci_upper:,.0f})")
```
</Accordion>

---

## üìù Practice Exercises

<CardGroup cols={2}>
  <Card title="Exercise 1" icon="chart-line" color="#3B82F6">
    Calculate correlation and simple linear regression
  </Card>
  <Card title="Exercise 2" icon="chart-mixed" color="#10B981">
    Build and interpret multiple regression models
  </Card>
  <Card title="Exercise 3" icon="bullseye" color="#8B5CF6">
    Evaluate model performance with R¬≤ and RMSE
  </Card>
  <Card title="Exercise 4" icon="home" color="#F59E0B">
    Real-world: House price prediction system
  </Card>
</CardGroup>

<details>
<summary>**Exercise 1: Advertising Effectiveness** - Correlation and simple regression</summary>

**Problem**: A company tracks advertising spend vs. sales:
| Ad Spend ($K) | Sales ($K) |
|---------------|------------|
| 10 | 100 |
| 20 | 150 |
| 30 | 210 |
| 40 | 250 |
| 50 | 300 |

1. Calculate the correlation coefficient
2. Fit a simple linear regression model
3. Predict sales for $35K ad spend
4. Interpret the slope coefficient

**Solution**:
```python
import numpy as np
from scipy import stats

# Data
ad_spend = np.array([10, 20, 30, 40, 50])
sales = np.array([100, 150, 210, 250, 300])

# 1. Correlation coefficient
r = np.corrcoef(ad_spend, sales)[0, 1]
print(f"Correlation coefficient (r): {r:.4f}")
print(f"R¬≤ (coefficient of determination): {r**2:.4f}")

# 2. Simple linear regression (from scratch)
x_mean = np.mean(ad_spend)
y_mean = np.mean(sales)

# Slope: Œ≤‚ÇÅ = Œ£(x-xÃÑ)(y-»≥) / Œ£(x-xÃÑ)¬≤
numerator = np.sum((ad_spend - x_mean) * (sales - y_mean))
denominator = np.sum((ad_spend - x_mean) ** 2)
beta_1 = numerator / denominator

# Intercept: Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ
beta_0 = y_mean - beta_1 * x_mean

print(f"\nRegression equation: Sales = {beta_0:.2f} + {beta_1:.2f} √ó AdSpend")

# Verify with scipy
slope, intercept, r_value, p_value, std_err = stats.linregress(ad_spend, sales)
print(f"Scipy: Sales = {intercept:.2f} + {slope:.2f} √ó AdSpend")

# 3. Predict for $35K
predicted_35 = beta_0 + beta_1 * 35
print(f"\nPredicted sales for $35K spend: ${predicted_35:.2f}K")

# 4. Interpret slope
print(f"\nSlope interpretation:")
print(f"  For each additional $1K in ad spend,")
print(f"  sales increase by ${beta_1:.2f}K on average")
print(f"\nROI: ${beta_1:.2f} sales per $1 ad spend = {beta_1:.0%} return")
```
</details>

<details>
<summary>**Exercise 2: Salary Prediction** - Multiple regression</summary>

**Problem**: Predict software engineer salaries using:
- Years of experience
- Number of skills
- Whether in SF Bay Area (1/0)

Data sample:
| Experience | Skills | Bay Area | Salary ($K) |
|------------|--------|----------|-------------|
| 2 | 4 | 0 | 65 |
| 5 | 6 | 1 | 130 |
| 3 | 5 | 1 | 100 |
| 8 | 8 | 0 | 110 |
| 1 | 3 | 0 | 55 |

1. Fit a multiple regression model
2. Which factor has the biggest impact?
3. Predict salary for someone with 4 years, 5 skills, in Bay Area
4. Interpret each coefficient

**Solution**:
```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Data
experience = np.array([2, 5, 3, 8, 1, 6, 4, 10, 2, 7])
skills = np.array([4, 6, 5, 8, 3, 7, 5, 9, 4, 8])
bay_area = np.array([0, 1, 1, 0, 0, 1, 0, 1, 1, 0])
salary = np.array([65, 130, 100, 110, 55, 140, 80, 160, 85, 115])

# Combine features
X = np.column_stack([experience, skills, bay_area])
y = salary

# 1. Fit multiple regression
model = LinearRegression()
model.fit(X, y)

print("=== Multiple Regression Model ===")
print(f"Salary = {model.intercept_:.2f}")
print(f"       + {model.coef_[0]:.2f} √ó Experience")
print(f"       + {model.coef_[1]:.2f} √ó Skills")
print(f"       + {model.coef_[2]:.2f} √ó Bay_Area")

# 2. Standardized coefficients for comparison
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
model_scaled = LinearRegression()
model_scaled.fit(X_scaled, y)

print("\n=== Standardized Coefficients (importance) ===")
features = ['Experience', 'Skills', 'Bay_Area']
for feat, coef in zip(features, model_scaled.coef_):
    print(f"  {feat}: {coef:.2f}")

biggest_impact = features[np.argmax(np.abs(model_scaled.coef_))]
print(f"\nBiggest impact: {biggest_impact}")

# 3. Prediction for 4 years, 5 skills, Bay Area
new_person = [[4, 5, 1]]
predicted_salary = model.predict(new_person)[0]
print(f"\n=== Prediction ===")
print(f"4 years exp, 5 skills, Bay Area: ${predicted_salary:.2f}K")

# 4. Coefficient interpretation
print("\n=== Coefficient Interpretation ===")
print(f"Experience: +${model.coef_[0]:.2f}K per year of experience")
print(f"Skills: +${model.coef_[1]:.2f}K per additional skill")
print(f"Bay Area: +${model.coef_[2]:.2f}K premium for Bay Area location")
print(f"Base salary (0 exp, 0 skills, not Bay Area): ${model.intercept_:.2f}K")

# R-squared
r2 = model.score(X, y)
print(f"\nR¬≤: {r2:.3f} ({r2*100:.1f}% of variance explained)")
```
</details>

<details>
<summary>**Exercise 3: Model Evaluation** - R¬≤, RMSE, and residual analysis</summary>

**Problem**: You've built a model to predict monthly electricity bills. Given predictions and actual values:

| Actual ($) | Predicted ($) |
|------------|---------------|
| 120 | 115 |
| 85 | 90 |
| 150 | 145 |
| 95 | 100 |
| 200 | 185 |

1. Calculate R¬≤ (coefficient of determination)
2. Calculate RMSE and MAE
3. Analyze residuals - are there patterns?
4. Is this a good model?

**Solution**:
```python
import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Data
actual = np.array([120, 85, 150, 95, 200])
predicted = np.array([115, 90, 145, 100, 185])

# 1. R¬≤ calculation
# R¬≤ = 1 - (SS_res / SS_tot)
ss_res = np.sum((actual - predicted) ** 2)  # Residual sum of squares
ss_tot = np.sum((actual - np.mean(actual)) ** 2)  # Total sum of squares
r2 = 1 - (ss_res / ss_tot)

print("=== Model Evaluation Metrics ===")
print(f"R¬≤ (manual): {r2:.4f}")
print(f"R¬≤ (sklearn): {r2_score(actual, predicted):.4f}")
print(f"\nInterpretation: Model explains {r2*100:.1f}% of variance in bills")

# 2. RMSE and MAE
rmse = np.sqrt(mean_squared_error(actual, predicted))
mae = mean_absolute_error(actual, predicted)

print(f"\n=== Error Metrics ===")
print(f"RMSE: ${rmse:.2f}")
print(f"MAE: ${mae:.2f}")
print(f"\nInterpretation:")
print(f"  On average, predictions are off by ${mae:.2f}")
print(f"  Typical error magnitude: ${rmse:.2f}")

# 3. Residual analysis
residuals = actual - predicted
print(f"\n=== Residual Analysis ===")
print(f"Residuals: {residuals}")
print(f"Mean residual: {np.mean(residuals):.2f} (should be ~0)")
print(f"Std of residuals: {np.std(residuals):.2f}")

# Check for patterns
# Residuals vs predicted
print("\nResidual patterns:")
for act, pred, res in zip(actual, predicted, residuals):
    direction = "under" if res > 0 else "over"
    print(f"  Actual ${act}: {direction}-predicted by ${abs(res):.0f}")

# 4. Model quality assessment
print("\n=== Model Quality Assessment ===")
print(f"R¬≤ = {r2:.3f}: {'Good' if r2 > 0.7 else 'Moderate' if r2 > 0.5 else 'Poor'}")
print(f"RMSE/Mean = {rmse/np.mean(actual):.1%}: Typical error as % of average bill")

if r2 > 0.9:
    quality = "Excellent - predictions are very reliable"
elif r2 > 0.7:
    quality = "Good - useful for estimates and planning"
elif r2 > 0.5:
    quality = "Moderate - use with caution"
else:
    quality = "Poor - consider different features or model"

print(f"Overall: {quality}")
```
</details>

<details>
<summary>**Exercise 4: House Price Prediction** - Complete regression workflow</summary>

**Problem**: Build a house price prediction model with proper validation:

Features: sqft, bedrooms, bathrooms, age, has_pool
Target: price (in $K)

1. Split data into train/test sets
2. Fit regression model and check coefficients
3. Evaluate on test set
4. Create confidence interval for predictions

**Solution**:
```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error
from scipy import stats

# Generate realistic housing data
np.random.seed(42)
n = 200

sqft = np.random.normal(2000, 400, n)
bedrooms = np.random.choice([2, 3, 4, 5], n, p=[0.1, 0.4, 0.4, 0.1])
bathrooms = np.random.choice([1, 2, 3], n, p=[0.2, 0.5, 0.3])
age = np.random.exponential(15, n)
has_pool = np.random.choice([0, 1], n, p=[0.7, 0.3])

# True price model with noise
price = (100 + 0.15 * sqft + 20 * bedrooms + 25 * bathrooms 
         - 2 * age + 40 * has_pool + np.random.normal(0, 30, n))

X = np.column_stack([sqft, bedrooms, bathrooms, age, has_pool])
y = price

# 1. Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Training set: {len(X_train)} houses")
print(f"Test set: {len(X_test)} houses")

# 2. Fit model
model = LinearRegression()
model.fit(X_train, y_train)

print("\n=== Model Coefficients ===")
features = ['sqft', 'bedrooms', 'bathrooms', 'age', 'has_pool']
for feat, coef in zip(features, model.coef_):
    print(f"  {feat}: {coef:+.2f}")
print(f"  intercept: {model.intercept_:.2f}")

# Interpretation
print("\n=== Interpretation ===")
print(f"Each sqft adds ${model.coef_[0]*1000:.0f} to price")
print(f"Each bedroom adds ${model.coef_[1]*1000:.0f}")
print(f"Each year of age reduces price by ${abs(model.coef_[3])*1000:.0f}")
print(f"Having a pool adds ${model.coef_[4]*1000:.0f}")

# 3. Evaluate on test set
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("\n=== Test Set Performance ===")
print(f"R¬≤: {r2:.4f}")
print(f"RMSE: ${rmse:.2f}K")

# Cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"\nCross-validation R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

# 4. Prediction with confidence interval
# For a specific house
new_house = np.array([[2200, 3, 2, 5, 1]])  # 2200sqft, 3bed, 2bath, 5yr old, pool
prediction = model.predict(new_house)[0]

# Bootstrap for CI
n_bootstrap = 1000
boot_predictions = []
for _ in range(n_bootstrap):
    idx = np.random.choice(len(X_train), len(X_train), replace=True)
    X_boot = X_train[idx]
    y_boot = y_train[idx]
    model_boot = LinearRegression().fit(X_boot, y_boot)
    boot_predictions.append(model_boot.predict(new_house)[0])

ci_lower = np.percentile(boot_predictions, 2.5)
ci_upper = np.percentile(boot_predictions, 97.5)

print("\n=== Prediction for New House ===")
print("Features: 2200 sqft, 3 bed, 2 bath, 5 years old, pool")
print(f"Predicted Price: ${prediction:.1f}K")
print(f"95% CI: (${ci_lower:.1f}K, ${ci_upper:.1f}K)")
print(f"\nConfidence: We're 95% confident the true price is between")
print(f"${ci_lower*1000:,.0f} and ${ci_upper*1000:,.0f}")
```
</details>

---

## Key Takeaways

<CardGroup cols={2}>
  <Card title="Correlation" icon="link">
    - Measures linear relationship strength (-1 to 1)
    - Correlation is not causation
    - High correlation can be spurious
  </Card>
  
  <Card title="Simple Regression" icon="chart-line">
    - Predicts Y from single X
    - y = Œ≤‚ÇÄ + Œ≤‚ÇÅx
    - Minimize sum of squared errors
  </Card>
  
  <Card title="Multiple Regression" icon="chart-mixed">
    - Predicts Y from multiple X variables
    - Coefficients show effect holding others constant
    - Standardize to compare importance
  </Card>
  
  <Card title="Evaluation" icon="clipboard-check">
    - R¬≤ = variance explained
    - RMSE = typical error size
    - Check assumptions via residual plots
  </Card>
</CardGroup>

---

## Common Pitfalls

<Warning>
**Regression Mistakes to Avoid**:
1. **Causation from Correlation** - Regression shows association, not causation; beware of confounders
2. **Ignoring Multicollinearity** - Highly correlated predictors make coefficients unstable and uninterpretable
3. **Extrapolating Beyond Data** - Models are only valid within the range of training data
4. **Ignoring Residual Patterns** - Non-random residuals indicate model misspecification
5. **Misinterpreting R¬≤** - R¬≤ is not accuracy; 0.65 doesn't mean "65% correct"
6. **Forgetting to Scale** - For comparing coefficient importance, standardize your features first
</Warning>

---

## Connection to Machine Learning

| Regression Concept | ML Application |
|-------------------|----------------|
| Linear regression | Foundation of neural networks (linear layers) |
| Coefficients | Weights in neural networks |
| Minimizing SSE | Loss function optimization |
| Gradient descent | How models learn (next module!) |
| Regularization | Preventing overfitting (L1, L2) |
| Feature scaling | Required for most ML algorithms |

<Tip>
**ML Connection**: Linear regression is the simplest neural network‚Äîone layer with linear activation. Understanding regression gives you intuition for how all deep learning works: find coefficients (weights) that minimize a loss function using gradient-based optimization.
</Tip>

**Coming up next**: We'll connect all these statistical concepts to **Machine Learning** - seeing how statistics powers the algorithms that learn from data.

<Card title="Next: From Statistics to ML" icon="arrow-right" href="/courses/statistics-for-ml/08-statistics-to-ml">
  See how statistics becomes machine learning
</Card>
