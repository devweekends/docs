---
title: "Gradients & Multivariable Calculus"
sidebarTitle: "Gradients"
description: "Optimizing functions with many variables - the key to deep learning"
icon: "mountain"
---

# Gradients & Multivariable Calculus

## Your Challenge: The CEO's Dilemma

In the previous module, you optimized **one** thing (price). But in the real world, you rarely control just one variable.

Imagine you're the CEO of a tech startup. You have **two** powerful levers to pull:
1. **Price** ($x$): How much you charge
2. **Ad Spend** ($y$): How much you spend on marketing

**Your Goal**: Maximize Profit.

The problem is, these variables interact!
- High price + Low ads = No sales
- Low price + High ads = Lots of sales, but high costs
- High price + High ads = Premium brand? Or wasted money?

You are standing on a complex "Profit Landscape" with hills and valleys. You want to find the highest peak (maximum profit).

**The Catch**: You're blindfolded (or in a thick fog). You can't see the peak. You can only feel the slope under your feet.

### The Hiker in the Fog

![Hiker Gradient Analogy](/images/courses/math-for-ml-calculus/hiker-gradient-analogy.svg)

This is the classic intuition for **Gradients**.

Imagine you're hiking up a mountain in dense fog:
1. You can't see the summit.
2. You want to go up as fast as possible.
3. What do you do?

You feel the ground with your foot:
- **Step East ($x$)**: Is it going up or down? (Partial Derivative w.r.t $x$)
- **Step North ($y$)**: Is it going up or down? (Partial Derivative w.r.t $y$)

If East is steep uphill, and North is slightly uphill, you move **mostly East, slightly North**.

**The Gradient is your Compass.** It combines these two slopes into ONE arrow that points **steepest uphill**.

---

## What Is a Gradient?

### Intuitive Definition

**Gradient = The Direction of Steepest Ascent**

It answers: *"Which combination of changes ($x$ and $y$) will increase my output the fastest?"*

### Mathematical Definition

The gradient (symbol $\nabla$, pronounced "del") is just a vector holding all the partial derivatives:

$$
\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}
$$

- **Top number**: Slope in $x$ direction (Price)
- **Bottom number**: Slope in $y$ direction (Ad Spend)

### Let's Solve Your CEO Problem

Suppose your Profit function is:
$$ P(x, y) = -x^2 - y^2 + 10x + 8y $$

Let's find the gradient at your current position: Price = 2, Ad Spend = 3.

```python
import numpy as np

def profit_gradient(x, y):
    # Partial derivative for Price (x): -2x + 10
    # (Treat y as constant, derivative of -y¬≤ is 0)
    dp_dx = -2*x + 10
    
    # Partial derivative for Ad Spend (y): -2y + 8
    # (Treat x as constant, derivative of -x¬≤ is 0)
    dp_dy = -2*y + 8
    
    return np.array([dp_dx, dp_dy])

# Your current strategy
current_price = 2
current_ad_spend = 3

grad = profit_gradient(current_price, current_ad_spend)

print(f"Current Position: Price=${current_price}, Ads=${current_ad_spend}")
print(f"Gradient Vector: {grad}")
```

**Output**:
```
Current Position: Price=$2, Ads=$3
Gradient Vector: [6, 2]
```

**What this tells you**:
- **6 (x-component)**: Increasing Price is VERY profitable right now.
- **2 (y-component)**: Increasing Ad Spend is MILDLY profitable.
- **Decision**: You should increase BOTH, but focus 3x more effort on raising Price!

**Key Insight**: The gradient doesn't just tell you "up" - it tells you the **exact mix** of changes to make.

---

---

## Example 1: Optimizing Your Business

### The Problem

Let's formalize your CEO problem. You want to maximize Revenue based on two investments:
- $x$ = Advertising Budget ($1000s)
- $y$ = Product Quality Investment ($1000s)

**Your Revenue Function**:
$$ R(x, y) = 100x + 80y - x^2 - y^2 - 0.5xy $$

**Your Goal**: Find the perfect budget allocation ($x, y$) that maximizes $R$.

### Visualizing Your Landscape

Here is what your revenue landscape looks like. The gradient (red arrow) shows you the fastest way to the top.

![Steepest Ascent Visual](/images/courses/math-for-ml-calculus/steepest-ascent-visual.svg)

### Step 1: Compute the Gradient

You need to find the partial derivatives (the slope in each direction):

1. **Slope w.r.t Ad Budget ($x$)**:
   $$ \frac{\partial R}{\partial x} = 100 - 2x - 0.5y $$
   *(Treat $y$ as constant number)*

2. **Slope w.r.t Quality ($y$)**:
   $$ \frac{\partial R}{\partial y} = 80 - 2y - 0.5x $$
   *(Treat $x$ as constant number)*

**Your Gradient Vector**:
$$ \nabla R = \begin{bmatrix} 100 - 2x - 0.5y \\ 80 - 2y - 0.5x \end{bmatrix} $$

### Step 2: Check Your Current Strategy

Suppose you are currently spending:
- $x = 20$ ($20k on Ads)
- $y = 15$ ($15k on Quality)

Let's plug these into your gradient:

```python
import numpy as np

def revenue_gradient(x, y):
    dR_dx = 100 - 2*x - 0.5*y
    dR_dy = 80 - 2*y - 0.5*x
    return np.array([dR_dx, dR_dy])

# Your current allocation
x, y = 20, 15
grad = revenue_gradient(x, y)

print(f"Current allocation: Ad=${x}k, Quality=${y}k")
print(f"Gradient: {grad}")
```

**Output**:
```
Current allocation: Ad=$20k, Quality=$15k
Gradient: [52.5, 40.0]
```

**Interpretation**:
- **52.5**: Increasing Ad spend is HIGHLY profitable.
- **40.0**: Increasing Quality is ALSO profitable, but slightly less so.
- **Action**: Increase both, but prioritize Ads slightly more.

### Step 3: Find the Optimal Allocation

To find the absolute peak, you want the point where the slope is ZERO in all directions (flat top).

**Set Gradient to 0**:
$$
\begin{cases}
100 - 2x - 0.5y = 0 \\
80 - 2y - 0.5x = 0
\end{cases}
$$

Solving this system (using linear algebra or substitution):

```python
# Solve system of equations
# 2x + 0.5y = 100
# 0.5x + 2y = 80

A = np.array([[2, 0.5], [0.5, 2]])
b = np.array([100, 80])
optimal = np.linalg.solve(A, b)

x_opt, y_opt = optimal
print(f"Optimal Ad Budget: ${x_opt:.2f}k")
print(f"Optimal Quality Budget: ${y_opt:.2f}k")
```

**Output**:
```
Optimal Ad Budget: $42.67k
Optimal Quality Budget: $29.33k
```

**Result**: You found the perfect strategy! Spend $42.6k on ads and $29.3k on quality to maximize revenue.

**Real Application**: Google uses this exact math to optimize ad auctions, balancing multiple metrics (CTR, bid price, user relevance) simultaneously.

---

---

## Example 2: Optimizing Your Grades

### The Problem

You want to maximize your overall GPA across 3 subjects:
- $x$ = hours/week on Math
- $y$ = hours/week on English  
- $z$ = hours/week on Science

**Your GPA Function**:
$$ G(x, y, z) = \sqrt{x} + \sqrt{y} + \sqrt{z} - 0.01(x^2 + y^2 + z^2) $$
*(Square roots represent learning; squared terms represent burnout/fatigue)*

**Constraint**: You only have 30 hours/week total.

### Computing Your Gradient

The gradient tells you: *"If I add 1 hour of study, which subject gives the biggest GPA boost?"*

```python
def gpa_gradient(x, y, z):
    # Partial derivatives (marginal benefit - marginal cost)
    dG_dx = 0.5/np.sqrt(x) - 0.02*x
    dG_dy = 0.5/np.sqrt(y) - 0.02*y
    dG_dz = 0.5/np.sqrt(z) - 0.02*z
    return np.array([dG_dx, dG_dy, dG_dz])

# Your current schedule
x, y, z = 10, 12, 8  # hours per subject

grad = gpa_gradient(x, y, z)

print(f"Current Schedule: Math={x}h, English={y}h, Science={z}h")
print(f"Gradient: {grad}")
```

**Output**:
```
Current Schedule: Math=10h, English=12h, Science=8h
Gradient: [-0.042, -0.096, 0.017]
```

**Interpretation**:
- **Math (-0.042)**: Negative! Studying MORE math will actually LOWER your GPA (burnout).
- **English (-0.096)**: Very Negative! You are over-studying English.
- **Science (+0.017)**: Positive! You should shift time to Science.

**Action**: Study less English/Math, study more Science!

---

## Example 3: Tuning Your Recommendation System

### The Problem

You are building a Netflix-style recommender. You have 3 knobs to tune:
- $\alpha$ = Recency weight (how much recent views matter)
- $\beta$ = Popularity weight (how much overall hits matter)
- $\gamma$ = Personalization weight (how much user history matters)

**Your Error Function** (Lower is better):
$$ E(\alpha, \beta, \gamma) = (\alpha - 0.6)^2 + (\beta - 0.3)^2 + (\gamma - 0.8)^2 + 0.1\alpha\beta $$

**Goal**: Find the knob settings that minimize error.

### Gradient Descent Optimization

Since we want to MINIMIZE error, we move **opposite** to the gradient.

```python
def error_gradient(alpha, beta, gamma):
    dE_dalpha = 2*(alpha - 0.6) + 0.1*beta
    dE_dbeta = 2*(beta - 0.3) + 0.1*alpha
    dE_dgamma = 2*(gamma - 0.8)
    return np.array([dE_dalpha, dE_dbeta, dE_dgamma])

# Start with random settings
params = np.array([0.2, 0.5, 0.4])
learning_rate = 0.1

print("Optimizing your system...")
for step in range(15):
    grad = error_gradient(*params)
    params = params - learning_rate * grad  # Move OPPOSITE to gradient
    
    if step % 5 == 0:
        print(f"Step {step}: Œ±={params[0]:.2f}, Œ≤={params[1]:.2f}, Œ≥={params[2]:.2f}")

print(f"Optimal Settings: Œ±={params[0]:.2f}, Œ≤={params[1]:.2f}, Œ≥={params[2]:.2f}")
```

**Real Application**: Real recommendation systems optimize thousands of such parameters automatically using this exact method!

---

## Directional Derivatives: Choosing Your Path

### The Question

The gradient tells you the **steepest** way up. But what if you can't go that way? What if you want to go Northeast?

**Directional Derivative** answers: *"How fast will I climb if I walk in THIS specific direction?"*

![Directional Derivative Compass](/images/courses/math-for-ml-calculus/directional-derivative-compass.svg)

### The Formula

To find the rate of change in direction $\mathbf{v}$:

$$
\text{Rate} = \nabla f \cdot \mathbf{v} \quad (\text{Dot Product})
$$

- If direction is same as gradient ‚Üí Max rate (Steepest ascent)
- If direction is perpendicular ‚Üí Zero rate (Walking flat)
- If direction is opposite ‚Üí Negative rate (Steepest descent)

```python
# Gradient at your position
grad = np.array([39, 40])

# You want to move Northeast (45 degrees)
direction = np.array([1, 1]) 
direction = direction / np.linalg.norm(direction) # Normalize length to 1

# How fast will you climb?
rate = np.dot(grad, direction)
print(f"Climbing rate in Northeast direction: {rate:.2f}")
```

**Key Insight**: The dot product measures "alignment". The more your direction aligns with the gradient, the faster you climb!

---

## Hessian Matrix (Second Derivatives)

### What Is It?

Matrix of all second partial derivatives:

$$
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}
$$

### Why It Matters

**Hessian tells you about curvature**:
- Positive definite ‚Üí Local minimum
- Negative definite ‚Üí Local maximum
- Indefinite ‚Üí Saddle point

```python
# Example: f(x,y) = x¬≤ + y¬≤
def hessian_simple():
    return np.array([
        [2, 0],
        [0, 2]
    ])

H = hessian_simple()
eigenvalues = np.linalg.eigvals(H)
print(f"Eigenvalues: {eigenvalues}")
# Both positive ‚Üí minimum!
```

---

## Practice Exercises

### Exercise 1: Profit Optimization

```python
# Profit function with 2 variables
def profit(x, y):
    return 50*x + 40*y - x**2 - y**2 - 0.5*x*y

# TODO:
# 1. Compute the gradient
# 2. Find the point where gradient = 0
# 3. Verify it's a maximum using the Hessian
```

<details>
<summary>Solution</summary>

```python
def profit_gradient(x, y):
    return np.array([50 - 2*x - 0.5*y, 40 - 2*y - 0.5*x])

# Solve ‚àáP = 0
# 50 - 2x - 0.5y = 0
# 40 - 2y - 0.5x = 0

A = np.array([[2, 0.5], [0.5, 2]])
b = np.array([50, 40])
optimal = np.linalg.solve(A, b)

print(f"Optimal: x={optimal[0]:.2f}, y={optimal[1]:.2f}")
print(f"Maximum profit: ${profit(*optimal):.2f}")

# Hessian
H = np.array([[-2, -0.5], [-0.5, -2]])
eigenvalues = np.linalg.eigvals(H)
print(f"Eigenvalues: {eigenvalues}")
# Both negative ‚Üí maximum!
```
</details>

---

## üéØ Practice Exercises & Real-World Applications

<Note>
**Challenge yourself!** These exercises show how gradients guide decisions in business, ML, and everyday life.
</Note>

### Exercise 1: Marketing Budget Allocation üìä

A company has a marketing budget to split between Google Ads and Instagram:

```python
import numpy as np

# Conversion rate depends on both channels (they interact!)
# Conversions(g, i) = 100*sqrt(g) + 80*sqrt(i) + 10*sqrt(g*i)
# where g = Google spend ($000s), i = Instagram spend ($000s)
#
# Total budget: $50,000 (g + i = 50)
# Revenue per conversion: $50

# TODO:
# 1. Write the profit function (revenue - costs)
# 2. Compute the gradient ‚àáProfit
# 3. Find the optimal allocation
# 4. What's the gradient at g=25, i=25? What does it tell you?
```

<Accordion title="üí° Solution">
```python
import numpy as np

def conversions(g, i):
    """Total conversions from both channels"""
    return 100*np.sqrt(g) + 80*np.sqrt(i) + 10*np.sqrt(g*i)

def profit(g, i, revenue_per_conv=50):
    """Profit = Revenue - Costs"""
    return revenue_per_conv * conversions(g, i) - (g + i) * 1000

def gradient(g, i, revenue_per_conv=50):
    """
    ‚àÇP/‚àÇg = 50 * (50/‚àög + 5*‚àö(i/g)) - 1000
    ‚àÇP/‚àÇi = 50 * (40/‚àöi + 5*‚àö(g/i)) - 1000
    """
    dP_dg = revenue_per_conv * (50/np.sqrt(g) + 5*np.sqrt(i/g)) - 1000
    dP_di = revenue_per_conv * (40/np.sqrt(i) + 5*np.sqrt(g/i)) - 1000
    return np.array([dP_dg, dP_di])

print("üìä Marketing Budget Optimization")
print("=" * 55)

# Current equal split
g_curr, i_curr = 25, 25
grad = gradient(g_curr, i_curr)
print(f"\nüìç Current Split: Google=${g_curr}k, Instagram=${i_curr}k")
print(f"   Conversions: {conversions(g_curr, i_curr):.0f}")
print(f"   Profit: ${profit(g_curr, i_curr):,.0f}")
print(f"   Gradient: [{grad[0]:.2f}, {grad[1]:.2f}]")
print(f"\n   üí° Interpretation:")
print(f"   ‚Ä¢ Google marginal value: ${grad[0]:.0f} per $1k extra")
print(f"   ‚Ä¢ Instagram marginal value: ${grad[1]:.0f} per $1k extra")
if grad[0] > grad[1]:
    print(f"   ‚Üí Shift budget TO Google!")
else:
    print(f"   ‚Üí Shift budget TO Instagram!")

# Gradient descent to find optimal (with constraint g + i = 50)
def optimize_constrained():
    g = 25.0
    lr = 0.5
    for _ in range(100):
        grad = gradient(g, 50-g)
        # Move budget based on difference in marginal values
        g = g + lr * (grad[0] - grad[1]) / 2000
        g = np.clip(g, 1, 49)  # Keep valid
    return g, 50-g

g_opt, i_opt = optimize_constrained()
print(f"\nüéØ Optimal Allocation:")
print(f"   Google: ${g_opt:.1f}k ({g_opt/50*100:.0f}%)")
print(f"   Instagram: ${i_opt:.1f}k ({i_opt/50*100:.0f}%)")
print(f"   Profit: ${profit(g_opt, i_opt):,.0f}")
print(f"\nüìà Improvement: +${profit(g_opt, i_opt) - profit(25, 25):,.0f} vs equal split")
```

**Real-World Insight**: This is exactly how performance marketing teams at Google, Meta, and agencies optimize ad spend. The gradient tells you where your next dollar is most valuable!
</Accordion>

---

### Exercise 2: Neural Network Weight Update üß†

Manually compute a gradient update for a tiny neural network:

```python
import numpy as np

# Simple network: 2 inputs ‚Üí 2 weights ‚Üí 1 output
# y = w1*x1 + w2*x2
# Loss = (y - target)¬≤

# Data point: x = [3, 4], target = 10
# Current weights: w = [1, 1]
# Predicted: y = 1*3 + 1*4 = 7
# Loss = (7 - 10)¬≤ = 9

# TODO:
# 1. Compute ‚àÇLoss/‚àÇw1 and ‚àÇLoss/‚àÇw2
# 2. Update weights with learning rate 0.1
# 3. Compute new prediction and loss
# 4. Repeat for 5 steps and watch loss decrease
```

<Accordion title="üí° Solution">
```python
import numpy as np

def predict(w, x):
    return w[0]*x[0] + w[1]*x[1]

def loss(y_pred, y_true):
    return (y_pred - y_true) ** 2

def gradient(w, x, y_true):
    """
    L = (w1*x1 + w2*x2 - target)¬≤
    ‚àÇL/‚àÇw1 = 2(y_pred - target) * x1
    ‚àÇL/‚àÇw2 = 2(y_pred - target) * x2
    """
    y_pred = predict(w, x)
    error = y_pred - y_true
    return np.array([2 * error * x[0], 2 * error * x[1]])

print("üß† Neural Network Gradient Descent")
print("=" * 55)

# Setup
x = np.array([3, 4])
target = 10
w = np.array([1.0, 1.0])
lr = 0.1

print(f"Data: x = {x}, target = {target}")
print(f"Initial weights: w = {w}")
print(f"Learning rate: {lr}")
print("\n" + "-" * 55)
print(f"{'Step':<6} {'Weights':<20} {'Pred':<8} {'Loss':<10} {'Gradient'}")
print("-" * 55)

for step in range(6):
    y_pred = predict(w, x)
    L = loss(y_pred, target)
    grad = gradient(w, x, target)
    
    print(f"{step:<6} [{w[0]:.3f}, {w[1]:.3f}]     {y_pred:<8.2f} {L:<10.4f} [{grad[0]:.2f}, {grad[1]:.2f}]")
    
    # Update weights
    w = w - lr * grad

print("-" * 55)
print(f"\n‚úÖ Final weights: w = [{w[0]:.3f}, {w[1]:.3f}]")
print(f"   Prediction: {predict(w, x):.4f} (target was {target})")
print(f"   Loss reduced from 9.0 to {loss(predict(w, x), target):.6f}")

# Verify: perfect weights would be [1, 1.75] (1*3 + 1.75*4 = 10)
print(f"\nüí° Perfect weights: [1.0, 1.75] ‚Üí {1*3 + 1.75*4}")
```

**Real-World Insight**: This is the fundamental update rule in ALL neural network training! PyTorch, TensorFlow, and JAX all do exactly this - just with millions of weights and clever optimizations.
</Accordion>

---

### Exercise 3: Heat Map Navigation üó∫Ô∏è

You're a robot navigating a temperature field. Find the hottest spot:

```python
import numpy as np

# Temperature field (2D Gaussian peaks)
# T(x, y) = 80*exp(-((x-3)¬≤ + (y-2)¬≤)/10) + 60*exp(-((x+2)¬≤ + (y+1)¬≤)/5)
# Two heat sources: one at (3, 2), another at (-2, -1)

# You start at position (0, 0)
# Use gradient ascent to find the hottest spot

# TODO:
# 1. Compute the gradient of T
# 2. Implement gradient ascent
# 3. Which heat source do you reach?
# 4. Try different starting positions - do you reach different peaks?
```

<Accordion title="üí° Solution">
```python
import numpy as np

def temperature(x, y):
    """Two Gaussian heat sources"""
    peak1 = 80 * np.exp(-((x-3)**2 + (y-2)**2) / 10)  # Peak at (3, 2), max=80
    peak2 = 60 * np.exp(-((x+2)**2 + (y+1)**2) / 5)   # Peak at (-2, -1), max=60
    return peak1 + peak2

def gradient_T(x, y):
    """Gradient of temperature field"""
    # For peak1: 80*exp(-((x-3)¬≤ + (y-2)¬≤)/10)
    # ‚àÇ/‚àÇx = 80 * exp(...) * (-2(x-3)/10) = peak1 * (-(x-3)/5)
    peak1 = 80 * np.exp(-((x-3)**2 + (y-2)**2) / 10)
    peak2 = 60 * np.exp(-((x+2)**2 + (y+1)**2) / 5)
    
    dT_dx = peak1 * (-(x-3) / 5) + peak2 * (-(x+2) / 2.5)
    dT_dy = peak1 * (-(y-2) / 5) + peak2 * (-(y+1) / 2.5)
    
    return np.array([dT_dx, dT_dy])

def gradient_ascent(start_x, start_y, lr=0.5, steps=50):
    """Climb the temperature gradient"""
    x, y = start_x, start_y
    path = [(x, y, temperature(x, y))]
    
    for _ in range(steps):
        grad = gradient_T(x, y)
        x = x + lr * grad[0]
        y = y + lr * grad[1]
        path.append((x, y, temperature(x, y)))
        
        if np.linalg.norm(grad) < 0.01:
            break
    
    return x, y, path

print("üó∫Ô∏è Heat Map Navigation (Gradient Ascent)")
print("=" * 55)
print("Heat sources: Peak1 at (3, 2) = 80¬∞C, Peak2 at (-2, -1) = 60¬∞C")

# Test different starting positions
starts = [(0, 0), (5, 0), (-3, 0), (0, 3), (0, -3)]

print("\nüìç Starting Position ‚Üí Final Position ‚Üí Peak Reached")
print("-" * 55)

for sx, sy in starts:
    fx, fy, path = gradient_ascent(sx, sy)
    final_temp = temperature(fx, fy)
    
    # Determine which peak
    if fx > 0:
        peak = "Peak1 (80¬∞C)"
    else:
        peak = "Peak2 (60¬∞C)"
    
    print(f"   ({sx:3}, {sy:3}) ‚Üí ({fx:.1f}, {fy:.1f}) ‚Üí {peak} at {final_temp:.1f}¬∞C")

# Detailed path from origin
print("\nüö∂ Detailed Path from (0, 0):")
_, _, path = gradient_ascent(0, 0)
print("   Step | Position    | Temperature | Gradient")
print("   -----|-------------|-------------|----------")
for i in [0, 5, 10, 20, len(path)-1]:
    if i < len(path):
        x, y, t = path[i]
        g = gradient_T(x, y)
        print(f"   {i:4} | ({x:4.1f}, {y:4.1f}) | {t:11.2f} | ({g[0]:5.2f}, {g[1]:5.2f})")

print("\nüí° Key Insight:")
print("   Gradient ascent finds LOCAL maxima - you reach")
print("   whichever peak you're closest to initially!")
print("   This is why neural networks can get stuck in local minima!")
```

**Real-World Insight**: This local vs global optimum problem is fundamental in ML. It's why we use random initialization, momentum, and techniques like simulated annealing to escape local optima!
</Accordion>

---

### Exercise 4: Portfolio Optimization üíº

Find the optimal stock allocation to maximize risk-adjusted return:

```python
import numpy as np

# Two stocks: A (high risk/return) and B (low risk/return)
# Expected return: R(a, b) = 0.15*a + 0.08*b (a, b are allocation fractions)
# Variance (risk): V(a, b) = 0.04*a¬≤ + 0.01*b¬≤ + 0.01*a*b
# 
# Sharpe ratio (risk-adjusted return): S = R / sqrt(V)
# Constraint: a + b = 1 (fully invested)

# TODO:
# 1. Express S in terms of a only (since b = 1 - a)
# 2. Find the gradient ‚àÇS/‚àÇa
# 3. Find optimal allocation
# 4. Compare with 50/50 split
```

<Accordion title="üí° Solution">
```python
import numpy as np

def returns(a):
    """Expected return: R = 0.15*a + 0.08*(1-a)"""
    b = 1 - a
    return 0.15 * a + 0.08 * b

def variance(a):
    """Portfolio variance"""
    b = 1 - a
    return 0.04 * a**2 + 0.01 * b**2 + 0.01 * a * b

def sharpe(a):
    """Sharpe ratio = Return / Risk"""
    return returns(a) / np.sqrt(variance(a))

def sharpe_gradient(a, eps=1e-6):
    """Numerical gradient for Sharpe ratio"""
    return (sharpe(a + eps) - sharpe(a - eps)) / (2 * eps)

print("üíº Portfolio Optimization")
print("=" * 55)
print("Stock A: 15% return, 20% volatility (high risk)")
print("Stock B: 8% return, 10% volatility (low risk)")
print("Correlation: 0.5")

# Gradient ascent to find optimal allocation
a = 0.5  # Start at 50/50
lr = 0.5
history = [(a, sharpe(a))]

for _ in range(50):
    grad = sharpe_gradient(a)
    a_new = a + lr * grad
    a = np.clip(a_new, 0, 1)  # Keep valid allocation
    history.append((a, sharpe(a)))
    if abs(grad) < 1e-6:
        break

optimal_a = a

print(f"\nüéØ Optimal Allocation:")
print(f"   Stock A (high risk): {optimal_a*100:.1f}%")
print(f"   Stock B (low risk): {(1-optimal_a)*100:.1f}%")
print(f"   Expected Return: {returns(optimal_a)*100:.2f}%")
print(f"   Portfolio Risk: {np.sqrt(variance(optimal_a))*100:.2f}%")
print(f"   Sharpe Ratio: {sharpe(optimal_a):.4f}")

# Comparison table
print("\nüìä Allocation Comparison:")
print("   Allocation | Return | Risk   | Sharpe")
print("   ----------|--------|--------|--------")
for alloc, label in [(0, "100% B"), (0.5, "50/50"), (optimal_a, "Optimal"), (1, "100% A")]:
    r = returns(alloc)
    v = np.sqrt(variance(alloc))
    s = sharpe(alloc)
    marker = " ‚Üê" if abs(alloc - optimal_a) < 0.01 else ""
    print(f"   {label:9} | {r*100:5.1f}% | {v*100:5.1f}% | {s:.4f}{marker}")

print(f"\nüí° Key Insight:")
print(f"   The gradient told us to shift from 50/50 toward higher Stock A")
print(f"   allocation, but not 100% - diversification reduces risk!")
```

**Real-World Insight**: This is Modern Portfolio Theory (Markowitz, Nobel Prize 1990). Every robo-advisor (Wealthfront, Betterment) uses gradient-based optimization to find efficient portfolios!
</Accordion>

---

## Key Takeaways

‚úÖ **Gradient** = vector of partial derivatives  
‚úÖ **Points uphill** = direction of steepest ascent  
‚úÖ **Optimization** = set gradient = 0  
‚úÖ **Hessian** = tells you if it's min/max/saddle  
‚úÖ **ML uses gradients** = to optimize millions of parameters  

---

## What's Next?

You now understand gradients for multi-variable functions. But how do we handle COMPOSITIONS of functions (like neural networks with many layers)?

That's the **chain rule** - and it's the key to backpropagation!

<Card title="Next: Chain Rule & Backpropagation" icon="arrow-right" href="/courses/math-for-ml-calculus/03-chain-rule">
  Discover how neural networks learn through backpropagation
</Card>
