---
title: "Chain Rule & Backpropagation"
sidebarTitle: "Chain Rule"
description: "How neural networks learn - understanding the chain rule through composition"
icon: "link"
---

# Chain Rule & Backpropagation

## Your Challenge: The Butterfly Effect

In complex systems, a small change *here* can cause a massive result *there*.

Imagine you run a global manufacturing company.
1. **Raw Material Price** goes up by $0.10.
2. **Production Cost** increases by $1.00.
3. **Product Price** increases by $5.00.
4. **Sales Volume** drops by 1,000 units.
5. **Total Revenue** crashes by $50,000.

**Your Question**: *"How did a 10-cent change cause a $50,000 crash?"*

To understand this, you need to trace the impact through every link in the chain.

This is exactly what the **Chain Rule** does. And it's how neural networks "blame" a specific weight in Layer 1 for an error in Layer 50.

### The Domino Effect

![Chain Rule Domino Effect](/images/courses/math-for-ml-calculus/chain-rule-domino.svg)

Think of it as a row of dominoes.
- You push the first domino (Input).
- It hits the second (Hidden Layer).
- Which hits the third (Output).

**The Chain Rule says**:
> "The total impact is the product of all the individual impacts along the chain."

If Domino A hits Domino B with force 2, and Domino B hits Domino C with force 3...
Then Domino A hits Domino C with force **2 × 3 = 6**.

---

## The Intuition: Composition of Functions

### The Problem

You have a function inside another function:
$$ y = f(g(x)) $$

- $x$ = Input (Raw Material Price)
- $u = g(x)$ = Intermediate (Production Cost)
- $y = f(u)$ = Output (Revenue)

**Question**: If I change $x$, how much does $y$ change?

### The Solution

You multiply the rates of change!

$$ \frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} $$

- $\frac{du}{dx}$: How much Cost changes when Material Price changes.
- $\frac{dy}{du}$: How much Revenue changes when Cost changes.

### Let's Code It

```python
# 1. Material Price → Production Cost
def cost(material_price):
    return material_price * 10 + 50  # 10 units per product

# 2. Production Cost → Revenue
def revenue(production_cost):
    return -5 * production_cost + 2000 # Higher cost = Lower revenue

# Composed: Material Price → Revenue
def total_revenue(material_price):
    c = cost(material_price)
    return revenue(c)

# The Derivatives (Impacts)
d_cost_d_material = 10    # Every $1 material increase adds $10 to cost
d_revenue_d_cost = -5     # Every $1 cost increase reduces revenue by $5

# The Chain Rule
d_revenue_d_material = d_revenue_d_cost * d_cost_d_material
# -5 * 10 = -50

print(f"Impact of material price on revenue: {d_revenue_d_material}")
print("Interpretation: A $1 increase in material price kills revenue by $50.")
```

**Key Insight**: You can break a complex system into small, simple links. Multiply them together to get the total effect.

---

## Mathematical Definition

For composed functions $f(g(x))$:

$$
\frac{d}{dx}f(g(x)) = f'(g(x)) \cdot g'(x)
$$

**In words**: 
1. Take derivative of outer function (evaluated at inner function)
2. Multiply by derivative of inner function

### Simple Example

$$
h(x) = (x^2 + 1)^3
$$

**Think of it as**: $h(x) = f(g(x))$ where:
- Inner function: $g(x) = x^2 + 1$
- Outer function: $f(u) = u^3$

**Derivatives**:
- $g'(x) = 2x$
- $f'(u) = 3u^2$

**Chain rule**:

$$
h'(x) = f'(g(x)) \cdot g'(x) = 3(x^2 + 1)^2 \cdot 2x = 6x(x^2 + 1)^2
$$

```python
def h(x):
    return (x**2 + 1)**3

def h_derivative(x):
    # Chain rule
    inner = x**2 + 1
    outer_derivative = 3 * inner**2
    inner_derivative = 2 * x
    return outer_derivative * inner_derivative

x = 2
print(f"h'({x}) = {h_derivative(x)}")  # 150

# Verify numerically
h_val = 0.0001
numerical = (h(x + h_val) - h(x)) / h_val
print(f"Numerical: {numerical:.2f}")  # ≈ 150
```

---

## Example 1: Multi-Stage Business Process

### The Scenario

**Supply Chain**: Raw materials → Manufacturing → Sales → Revenue

```
Material Cost → Production Quantity → Sales Volume → Total Revenue
```

**Functions**:

$$
\begin{align}
\text{Production}(c) &= 1000 - 10c \quad \text{(higher cost → less production)} \\
\text{Sales}(p) &= 0.8p \quad \text{(80% of production sells)} \\
\text{Revenue}(s) &= 50s \quad \text{($50 per unit sold)}
\end{align}
$$

**Question**: If material cost increases by $1, how much does revenue change?

### Solution Using Chain Rule

```python
def production(cost):
    return 1000 - 10 * cost

def sales(production_qty):
    return 0.8 * production_qty

def revenue(sales_qty):
    return 50 * sales_qty

# Composed function
def total_revenue(cost):
    prod = production(cost)
    sale = sales(prod)
    return revenue(sale)

# Derivatives
def d_production_d_cost(cost):
    return -10

def d_sales_d_production(prod):
    return 0.8

def d_revenue_d_sales(sales):
    return 50

# Chain rule: multiply all derivatives
cost = 20
prod = production(cost)
sale = sales(prod)

chain_derivative = (d_production_d_cost(cost) * 
                   d_sales_d_production(prod) * 
                   d_revenue_d_sales(sale))

print(f"At cost=${cost}:")
print(f"  Production: {prod} units")
print(f"  Sales: {sale} units")
print(f"  Revenue: ${total_revenue(cost)}")
print(f"\nIf cost increases by $1:")
print(f"  Revenue changes by: ${chain_derivative}")
# -10 × 0.8 × 50 = -$400
```

**Interpretation**: 
- $1 cost increase → 10 fewer units produced
- 10 fewer units → 8 fewer sales (80% sell rate)
- 8 fewer sales → $400 less revenue

**Real Application**: Amazon uses chain rule to optimize their entire supply chain, from warehouses to delivery!

---

---

## Example 2: Your Learning Chain

### The Scenario

Let's model your own learning process as a chain of functions:
1. **Study Time** ($x$) → **Understanding** ($u$)
2. **Understanding** ($u$) → **Test Score** ($t$)
3. **Test Score** ($t$) → **Final Grade** ($G$)

**Your Goal**: Find out how much 1 extra hour of study improves your Final Grade.

### The Functions

```python
def understanding(study_hours):
    """More study → better understanding (diminishing returns)"""
    return 10 * np.sqrt(study_hours)

def test_score(understanding_level):
    """Understanding → test score"""
    return 5 * understanding_level + 20

def final_grade(test_avg):
    """Test average → final grade (curved)"""
    return 0.9 * test_avg + 5
```

### Applying the Chain Rule

You want to find $\frac{dG}{dx}$ (Change in Grade per Hour).

$$ \frac{dG}{dx} = \frac{dG}{dt} \cdot \frac{dt}{du} \cdot \frac{du}{dx} $$

```python
# 1. Derivative of Understanding w.r.t Hours
def d_understanding_d_hours(h):
    return 5 / np.sqrt(h)  # Derivative of 10√h

# 2. Derivative of Test Score w.r.t Understanding
def d_test_d_understanding(u):
    return 5

# 3. Derivative of Grade w.r.t Test Score
def d_grade_d_test(t):
    return 0.9

# Calculate for 9 hours of study
hours = 9
u = understanding(hours)
t = test_score(u)

# Multiply the links!
grade_improvement = (d_grade_d_test(t) * 
                    d_test_d_understanding(u) * 
                    d_understanding_d_hours(hours))

print(f"At 9 hours/week:")
print(f"1 extra hour adds +{grade_improvement:.2f} points to your Final Grade")
```

**Output**:
```
At 9 hours/week:
1 extra hour adds +7.50 points to your Final Grade
```

**Insight**: The chain rule lets you connect your input (effort) directly to your output (grade), even through multiple steps!

---

## Example 3: Your First Neural Network

### The Computational Graph

This is how deep learning actually works. We represent the network as a graph of nodes.

![Computational Graph for Backpropagation](/images/courses/math-for-ml-calculus/computational-graph-visual.svg)

**The Flow**:
1. **Forward Pass (Blue)**: You calculate the prediction and the error.
2. **Backward Pass (Red)**: You calculate who is to blame for the error.

### The Code (Backpropagation)

Let's implement the graph above for a single neuron:
$$ x \to z \to a \to L $$

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def train_neuron(x, w, b, y_true):
    # --- 1. Forward Pass ---
    z = w * x + b
    a = sigmoid(z)
    loss = (a - y_true)**2
    
    print(f"Prediction: {a:.4f}, Error: {loss:.4f}")
    
    # --- 2. Backward Pass (Chain Rule) ---
    # We want dL/dw (How much is w to blame?)
    
    # Link 1: How Loss changes with Activation
    dL_da = 2 * (a - y_true)
    
    # Link 2: How Activation changes with z
    da_dz = a * (1 - a)
    
    # Link 3: How z changes with weight w
    dz_dw = x
    
    # Total Chain: Multiply them all!
    dL_dw = dL_da * da_dz * dz_dw
    
    return dL_dw

# Test it
x = 2.0      # Input
w = 0.5      # Current Weight
b = 0.1      # Bias
y_true = 1.0 # Target (We want output to be 1)

gradient = train_neuron(x, w, b, y_true)
print(f"\nGradient (dL/dw): {gradient:.4f}")
print("Interpretation: Increasing w will reduce the error!")
```

**This is Backpropagation.** It's just the Chain Rule applied to a graph!

### Multi-Layer Networks

For a deep network with 100 layers, you just have a longer chain:

$$ \frac{dL}{dw_1} = \frac{dL}{da_{100}} \cdot \frac{da_{100}}{dz_{100}} \dots \frac{dz_1}{dw_1} $$

The computer simply multiplies these numbers backward from the end to the start.

---

---

## Practice Exercises

### Exercise 1: Chain Rule Practice

```python
# Given: h(x) = sin(x²)
# Find: h'(x) using chain rule

# TODO:
# 1. Identify inner and outer functions
# 2. Find their derivatives
# 3. Apply chain rule
# 4. Verify numerically at x=2
```

<details>
<summary>Solution</summary>

```python
# h(x) = sin(x²)
# Inner: g(x) = x²,  g'(x) = 2x
# Outer: f(u) = sin(u), f'(u) = cos(u)

# Chain rule: h'(x) = cos(x²) · 2x

def h(x):
    return np.sin(x**2)

def h_derivative(x):
    return np.cos(x**2) * 2*x

x = 2
analytical = h_derivative(x)
numerical = (h(x + 0.0001) - h(x)) / 0.0001

print(f"Analytical: {analytical:.6f}")
print(f"Numerical: {numerical:.6f}")
```
</details>

---

## Key Takeaways

✅ **Chain rule** = multiply derivatives along the chain  
✅ **Backpropagation** = chain rule applied backward  
✅ **Deep learning** = chain rule through many layers  
✅ **Gradients flow backward** = from output to input  
✅ **Every framework uses this** = PyTorch, TensorFlow, JAX  

---

## What's Next?

You now understand how gradients flow through compositions. But how do we USE these gradients to actually train models?

That's **Gradient Descent** - the optimization algorithm that powers all of machine learning!

<Card title="Next: Gradient Descent" icon="arrow-right" href="/courses/math-for-ml-calculus/04-gradient-descent">
  Learn the algorithm that makes machines learn
</Card>
