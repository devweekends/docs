---
title: "Optimization Techniques"
sidebarTitle: "Optimization"
description: "Going beyond Gradient Descent - Momentum, RMSprop, and Adam"
icon: "rocket"
---

# Optimization Techniques

## Your Challenge: The Valley of Deceit

Standard Gradient Descent is like walking downhill blindfolded. It works great on a smooth, simple hill.

But real-world loss landscapes are **treacherous**.
- **Local Minima**: Small dips that look like the bottom but aren't.
- **Saddle Points**: Flat areas where you get stuck.
- **Ravines**: Steep walls where you bounce back and forth.

**Your Goal**: Navigate this treacherous terrain to find the *true* global minimum, fast.

You need better equipment than just "walking downhill". You need **Momentum** and **Adaptive Steps**.

---

## Momentum: The Heavy Ball

### The Intuition

Imagine rolling a ping-pong ball down a bumpy hill. It gets stuck in every little pothole (Local Minimum).

Now imagine rolling a **heavy bowling ball**.
- It gains speed.
- When it hits a small pothole, its **momentum** carries it right through.
- It eventually settles in the deepest valley.

![Momentum Ball Analogy](/images/courses/math-for-ml-calculus/momentum-ball.svg)

### The Math

Instead of just following the gradient, we keep a "velocity" ($v$) that accumulates speed.

$$
\begin{align}
v_{new} &= \beta \cdot v_{old} + (1 - \beta) \cdot \nabla f(x) \\
x_{new} &= x_{old} - \alpha \cdot v_{new}
\end{align}
$$

- $\beta$: Friction (usually 0.9). Retains 90% of speed.
- $v$: Velocity.

### The Code

```python
import numpy as np

# A function with a local minimum at x=-2 and global at x=2
def f(x): return 0.1*x**4 - 3*x**2 + x
def grad(x): return 0.4*x**3 - 6*x + 1

# 1. Standard SGD (Gets stuck)
x = -3.0
lr = 0.1
for _ in range(20):
    x = x - lr * grad(x)
print(f"SGD stuck at x={x:.2f}")  # ~ -2.0 (Local Min)

# 2. Momentum (Escapes!)
x = -3.0
v = 0.0
beta = 0.9
for _ in range(20):
    v = beta * v + (1 - beta) * grad(x)
    x = x - lr * v
print(f"Momentum reached x={x:.2f}") # ~ 2.0 (Global Min)
```

**Key Insight**: Momentum helps you blast through small traps and speed up on flat surfaces!

---

## RMSprop & Adam: Adaptive Shoes

### The Problem with Ravines

Imagine a narrow ravine.
- Steep walls (High gradient in $y$ direction).
- Gentle slope towards the sea (Low gradient in $x$ direction).

If you take big steps, you bounce off the walls ($y$) and never move forward ($x$).
If you take small steps, you move forward ($x$) but it takes forever.

**Solution**: Wear **Adaptive Shoes**.
- If the ground is steep ($y$), take tiny steps.
- If the ground is flat ($x$), take huge steps.

### Adam (Adaptive Moment Estimation)

Adam combines both ideas:
1. **Momentum**: Keep moving forward (Velocity).
2. **RMSprop**: Adapt step size based on terrain steepness (Variance).

It is the "Gold Standard" optimizer in Deep Learning today.

### The Code (Using PyTorch)

You rarely implement Adam from scratch. You use a library.

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 1. Define your model
model = nn.Linear(10, 1)

# 2. Choose your optimizer
# SGD
optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)

# Momentum
optimizer_mom = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# Adam (The Best)
optimizer_adam = optim.Adam(model.parameters(), lr=0.001)

# 3. Training Loop
# optimizer.zero_grad()
# loss.backward()
# optimizer.step()  # This applies the math!
```

---

## Comparison: Who Wins?

| Optimizer | Analogy | Best For |
|-----------|---------|----------|
| **SGD** | Drunk walker | Simple problems |
| **Momentum** | Bowling ball | Noisy gradients, Local minima |
| **Adam** | Smart Robot | Almost everything (Default choice) |

### Visual Comparison

![Optimizer Comparison](/images/courses/math-for-ml-calculus/optimizer-comparison-path.svg)

If we race them on a complex terrain:
1. **SGD**: Stumbles, gets stuck.
2. **Momentum**: Overshoots but eventually settles.
3. **Adam**: Beelines straight for the goal.

---

## Practice Exercise: Escape the Trap

### The Scenario

You are training a model that keeps getting stuck at 80% accuracy.
- Loss isn't going down.
- Gradients are small but not zero.

**Diagnosis**: You are likely in a Saddle Point or Local Minimum.

**Your Task**: Switch from SGD to Adam and observe the difference.

```python
# Pseudo-code for your experiment
model = MyNeuralNet()
criterion = nn.MSELoss()

# Experiment A: SGD
opt_a = optim.SGD(model.parameters(), lr=0.01)
train(model, opt_a) # Result: 80% acc

# Experiment B: Adam
opt_b = optim.Adam(model.parameters(), lr=0.001)
train(model, opt_b) # Result: 95% acc!
```

**Takeaway**: Changing the optimizer is often the easiest way to improve your model!

---

## What's Next?

You have mastered the core math of learning!
1. **Derivatives**: How things change.
2. **Gradients**: The direction of change.
3. **Chain Rule**: How changes propagate.
4. **Gradient Descent**: How to learn.
5. **Optimization**: How to learn *fast*.

Now, you are ready to build something real.

<Card title="Final Project" icon="flag-checkered" href="/courses/math-for-ml-calculus/06-final-project">
  Build a Neural Network from Scratch
</Card>
