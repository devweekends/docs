---
title: "Derivatives & Rates of Change"
sidebarTitle: "Derivatives"
description: "Understanding how things change - the foundation of machine learning"
icon: "chart-line"
---

# Derivatives & Rates of Change

## Your Challenge: The Pricing Problem

You just launched your online store selling wireless headphones. Exciting! But now you face a critical decision:

**What price should you charge?**

You experiment with different prices over several weeks:

- **Week 1 ($30/pair)**: 1,000 customers bought! But... your profit was only $10,000
  - "Great sales, but I'm barely making money after costs ($20/pair)"
  
- **Week 2 ($100/pair)**: Only 200 customers bought. Profit: $16,000
  - "Better profit per sale, but I'm losing too many customers!"
  
- **Week 3 ($50/pair)**: 800 customers. Profit: $24,000
  - "Getting better... but is this the best I can do?"

**Your Question**: *"There must be a sweet spot - a price that maximizes my profit. But how do I find it without testing every single price?"*

### The Slow Way (What You're Doing Now)

You could test 100 different prices, one per week. That would take **2 years** and cost you thousands in lost revenue!

### The Fast Way (What You'll Learn)

There's a better approach: **Derivatives**

Instead of blindly testing prices, derivatives tell you:
- At $30: "Increase price â†’ profit will go UP"
- At $75: "Perfect! Any change makes profit go DOWN"  
- At $100: "Decrease price â†’ profit will go UP"

**Result**: You find the optimal price ($75) in minutes, not years. Your profit jumps to $30,250/month!

---

## What You'll Be Able To Do

By the end of this module, you'll answer questions like:

âœ… **Your Business**: What price maximizes YOUR profit?  
âœ… **Your Learning**: How many hours should YOU study for maximum score?  
âœ… **Your ML Models**: How should YOU adjust weights to reduce errors?  
âœ… **Your Life**: What's YOUR optimal speed to minimize fuel consumption?  

**Your tool**: Derivatives - the mathematical way to find optimal solutions.

<Info>
**Estimated Time**: 3-4 hours  
**Difficulty**: Beginner  
**Prerequisites**: Basic algebra  
**You'll Build**: Your own pricing optimizer, learning rate finder, and simple neural network
</Info>

---

## Your Problem: Finding the Pattern

Let's model your business mathematically and visualize your pricing landscape:

![Your Pricing Landscape](/images/courses/math-for-ml-calculus/pricing-landscape-explained.svg)

**What this shows**:
- The green curve is your profit at different prices
- Red dots are the prices you tested
- The gold star is the optimal price ($75)
- Arrows show which direction the derivative tells you to move

```python
import numpy as np
import matplotlib.pyplot as plt

def profit(price):
    """
    Your profit model:
    - At $30: 1000 customers
    - Lose 10 customers for every $1 price increase
    - Cost per headphone: $20
    """
    customers = 1300 - 10 * price
    profit_per_sale = price - 20  # price minus cost
    return customers * profit_per_sale

# Visualize your pricing landscape
prices = np.linspace(20, 130, 1000)
profits = [profit(p) for p in prices]

plt.figure(figsize=(12, 6))
plt.plot(prices, profits, linewidth=3, color='#10b981', label='Your Profit')

# Mark your experiments
plt.scatter([30, 50, 100], [profit(30), profit(50), profit(100)], 
           s=200, c='red', zorder=5, label='You tested these')

# Mark the optimal
plt.scatter([75], [profit(75)], s=300, c='gold', marker='*', 
           zorder=6, label='Optimal (you\'ll find this!)')

plt.xlabel('Price ($)', fontsize=14)
plt.ylabel('Your Monthly Profit ($)', fontsize=14)
plt.title('Your Pricing Landscape', fontsize=16, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.show()

print("Your experiments:")
print(f"  $30: ${profit(30):,.0f} profit")
print(f"  $50: ${profit(50):,.0f} profit")  
print(f"  $100: ${profit(100):,.0f} profit")
print(f"\nOptimal price: $75 â†’ ${profit(75):,.0f} profit â­")
```

**Your Insight**: "The graph shows a hill! I need to find the peak. But how?"

---

## Enter: The Derivative (Your Solution)

### What You Need to Know

At any price, you need to answer: **"If I increase my price by $1, does my profit go up or down?"**

This is EXACTLY what a derivative tells you!

**Derivative = Rate of Change**

    customers = 1300 - 10 * price
    return (price - 20) * customers

# Your current price
your_price = 50

# "If I increase my price by $1, how much does my profit change?"
small_increase = 1
profit_now = profit(your_price)
profit_after = profit(your_price + small_increase)
change_in_profit = profit_after - profit_now

print(f"At your current price of ${your_price}:")
print(f"  Your profit now: ${profit_now:,.0f}")
print(f"  Your profit at ${your_price + small_increase}: ${profit_after:,.0f}")
print(f"  Change: ${change_in_profit:,.0f}")
print(f"  â†’ Derivative â‰ˆ {change_in_profit}")
print(f"     (your profit changes by ${change_in_profit} per $1 price increase)")

if change_in_profit > 0:
    print(f"\n  âœ… Your profit is INCREASING â†’ You should raise your price!")
elif change_in_profit < 0:
    print(f"\n  âŒ Your profit is DECREASING â†’ You should lower your price!")
else:
    print(f"\n  â­ Your profit is at MAXIMUM â†’ You found the perfect price!")
```

**Output**:
```
At your current price of $50:
  Your profit now: $24,000
  Your profit at $51: $24,490
  Change: $490
  â†’ Derivative â‰ˆ 490
     (your profit changes by $490 per $1 price increase)

  âœ… Your profit is INCREASING â†’ You should raise your price!
```

**Your Reaction**: "Wow! At $50, I should increase my price. Each dollar increase adds $490 to my profit!"

---

## What Is a Derivative? (The Intuitive Explanation)

### Everyday Analogy: Your Car's Speedometer

**Think about driving a car:**

ðŸš— **Position** = where you are (e.g., mile marker 50)  
ðŸ“Š **Speed** = how fast your position is changing (e.g., 60 mph)  
âš¡ **Acceleration** = how fast your speed is changing (e.g., +5 mph/second)

**The speedometer shows your derivative!**

It tells you: "Right now, at this exact moment, you're going 60 mph."

Mathematically:
- Position = $f(t)$ (function of time)
- Speed = $f'(t)$ (derivative of position)
- Acceleration = $f''(t)$ (derivative of derivative)

### Mathematical Definition (Now It Makes Sense!)

**Derivative = Rate of change**

> "If I increase x by a tiny amount, how much does f(x) change?"

**Formula**:

$$
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$

**In plain English**:
1. Move a tiny bit to the right (x â†’ x+h)
2. See how much f(x) changed
3. Divide change in f by change in x
4. Make h smaller and smaller (approaching zero)

### Geometric View: The Tangent Line

![Derivative as Slope](/images/courses/math-for-ml-calculus/derivative-slope.svg)

**The derivative at a point = slope of the tangent line**

**Why tangent line?**
- Secant line: connects two points (average rate of change)
- Tangent line: touches at ONE point (instantaneous rate of change)
- As points get closer, secant â†’ tangent

### Computing a Derivative Numerically

Let's compute the derivative of $f(x) = x^2$ at $x = 3$:

```python
import numpy as np

def f(x):
    """Our function: f(x) = xÂ²"""
    return x**2

# We want the derivative at x=3
x = 3

# Method 1: Numerical approximation
print("=== Numerical Approximation ===")
for h in [0.1, 0.01, 0.001, 0.0001]:
    # Compute slope of secant line
    df = f(x + h) - f(x)  # Change in f
    dx = h                 # Change in x
    derivative_approx = df / dx
    
    print(f"h = {h:7.4f} â†’ f'(3) â‰ˆ {derivative_approx:.6f}")

print("\n=== Exact Answer ===")
# For f(x) = xÂ², the derivative is f'(x) = 2x
exact_derivative = 2 * x
print(f"f'(3) = 2Ã—3 = {exact_derivative}")

print("\n=== Interpretation ===")
print(f"At x=3, if we increase x by 1, f(x) increases by approximately {exact_derivative}")
print(f"At x=3, the function is rising with a slope of {exact_derivative}")
```

**Output**:
```
=== Numerical Approximation ===
h =  0.1000 â†’ f'(3) â‰ˆ 6.100000
h =  0.0100 â†’ f'(3) â‰ˆ 6.010000
h =  0.0010 â†’ f'(3) â‰ˆ 6.001000
h =  0.0001 â†’ f'(3) â‰ˆ 6.000100

=== Exact Answer ===
f'(3) = 2Ã—3 = 6

=== Interpretation ===
At x=3, if we increase x by 1, f(x) increases by approximately 6
At x=3, the function is rising with a slope of 6
```

**Key Insights**:
âœ… As h gets smaller, our approximation gets better  
âœ… The derivative is the **instantaneous** rate of change  
âœ… At x=3, the function $x^2$ is rising steeply (slope = 6)  
âœ… This tells us: small changes in x cause BIG changes in f(x)  

### Why This Matters for Machine Learning

**In ML, we have a loss function** $L(w)$ where $w$ = model weights:

```python
# Simplified neural network
def loss(weight):
    """How wrong our predictions are"""
    predictions = weight * data
    errors = predictions - true_values
    return np.mean(errors**2)

# The derivative tells us:
# "If I increase this weight slightly, does loss go up or down?"

dL_dw = compute_derivative(loss, weight)

if dL_dw > 0:
    # Loss increases when weight increases
    # â†’ Decrease weight to reduce loss!
    weight = weight - learning_rate * dL_dw
else:
    # Loss decreases when weight increases  
    # â†’ Increase weight to reduce loss!
    weight = weight - learning_rate * dL_dw
```

**This is gradient descent** - the algorithm that powers ALL of machine learning!

---

## Example 1: Minimizing Business Costs

### The Problem

You're optimizing ad spending. Your cost function is:

$$
C(x) = x^2 - 10x + 100
$$

Where $x$ is ad spend in thousands of dollars.

**Goal**: Find the spending level that minimizes cost.

### Step 1: Understand the Function

```python
def cost(x):
    return x**2 - 10*x + 100

# Visualize
x_values = np.linspace(0, 10, 100)
costs = [cost(x) for x in x_values]

plt.plot(x_values, costs)
plt.xlabel('Ad Spend ($1000s)')
plt.ylabel('Total Cost ($)')
plt.title('Cost vs. Ad Spend')
plt.grid(True)
plt.show()
```

### Step 2: Compute the Derivative

**Derivative of $C(x) = x^2 - 10x + 100$**:

$$
C'(x) = 2x - 10
$$

```python
def cost_derivative(x):
    return 2*x - 10

# At x=3
x = 3
slope = cost_derivative(x)
print(f"At x={x}, slope = {slope}")  # -4

# Interpretation:
# Negative slope â†’ cost is decreasing
# We should increase x!
```

### Step 3: Find the Minimum

**At the minimum, the derivative = 0** (flat tangent line)

$$
C'(x) = 0 \\
2x - 10 = 0 \\
x = 5
$$

```python
# Optimal ad spend
optimal_x = 5
min_cost = cost(optimal_x)

print(f"Optimal ad spend: ${optimal_x},000")
print(f"Minimum cost: ${min_cost}")

# Verify it's a minimum
print(f"Slope at x=4: {cost_derivative(4)}")  # -2 (decreasing)
print(f"Slope at x=5: {cost_derivative(5)}")  # 0 (flat)
print(f"Slope at x=6: {cost_derivative(6)}")  # 2 (increasing)
```

**Key Insight**: 
- Derivative < 0 â†’ function decreasing â†’ move right
- Derivative = 0 â†’ potential minimum/maximum
- Derivative > 0 â†’ function increasing â†’ move left

**Real Application**: Google Ads uses derivatives to optimize bidding strategies for millions of advertisers!

---

## Example 2: Optimizing Student Learning

### The Problem

A student's test score depends on study hours:

$$
S(h) = -h^2 + 12h + 20
$$

Where $h$ is hours studied per day.

**Question**: How many hours should they study to maximize their score?

### Understanding the Relationship

```python
def score(hours):
    return -hours**2 + 12*hours + 20

# Visualize
hours = np.linspace(0, 15, 100)
scores = [score(h) for h in hours]

plt.plot(hours, scores)
plt.xlabel('Study Hours per Day')
plt.ylabel('Test Score')
plt.title('Study Hours vs. Test Score')
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.grid(True)
plt.show()
```

**Observation**: Too few hours â†’ low score. Too many hours â†’ burnout, score decreases!

### Finding the Optimal Study Time

**Derivative**:

$$
S'(h) = -2h + 12
$$

```python
def score_derivative(h):
    return -2*h + 12

# Find where derivative = 0
# -2h + 12 = 0
# h = 6

optimal_hours = 6
max_score = score(optimal_hours)

print(f"Optimal study time: {optimal_hours} hours/day")
print(f"Maximum score: {max_score}")

# Check the derivative
print(f"\\nAt h=5: slope = {score_derivative(5)}")  # 2 (increasing)
print(f"At h=6: slope = {score_derivative(6)}")  # 0 (maximum!)
print(f"At h=7: slope = {score_derivative(7)}")  # -2 (decreasing)
```

**Interpretation**:
- Before 6 hours: More study â†’ higher score (positive derivative)
- At 6 hours: Perfect balance (zero derivative)
- After 6 hours: More study â†’ lower score due to burnout (negative derivative)

**Real Application**: Khan Academy uses similar models to recommend optimal practice time for students!

---

## Example 3: Tuning Recommendation Systems

### The Problem

Netflix wants to tune a recommendation parameter $\alpha$ to minimize prediction error:

$$
E(\alpha) = (\alpha - 0.8)^2 + 0.1
$$

**Goal**: Find the $\alpha$ that minimizes error.

### Visualizing the Error

```python
def error(alpha):
    return (alpha - 0.8)**2 + 0.1

# Visualize
alphas = np.linspace(0, 2, 100)
errors = [error(a) for a in alphas]

plt.plot(alphas, errors)
plt.xlabel('Parameter Î±')
plt.ylabel('Prediction Error')
plt.title('Recommendation Error vs. Parameter')
plt.grid(True)
plt.show()
```

### Finding Optimal Parameter

**Derivative**:

$$
E'(\alpha) = 2(\alpha - 0.8)
$$

```python
def error_derivative(alpha):
    return 2*(alpha - 0.8)

# Find minimum: E'(Î±) = 0
# 2(Î± - 0.8) = 0
# Î± = 0.8

optimal_alpha = 0.8
min_error = error(optimal_alpha)

print(f"Optimal Î±: {optimal_alpha}")
print(f"Minimum error: {min_error}")

# Gradient descent simulation
alpha = 0.2  # Start with bad guess
learning_rate = 0.1
history = [alpha]

for step in range(10):
    gradient = error_derivative(alpha)
    alpha = alpha - learning_rate * gradient
    history.append(alpha)
    print(f"Step {step+1}: Î±={alpha:.4f}, error={error(alpha):.4f}")

# Visualize convergence
plt.plot(history, marker='o')
plt.xlabel('Step')
plt.ylabel('Î± value')
plt.title('Gradient Descent Convergence')
plt.axhline(y=0.8, color='r', linestyle='--', label='Optimal')
plt.legend()
plt.grid(True)
plt.show()
```

**Key Insight**: This is exactly how machine learning works!
1. Start with random parameters
2. Compute derivative (gradient)
3. Move in opposite direction of gradient
4. Repeat until convergence

**Real Application**: Netflix uses gradient descent to tune thousands of parameters in their recommendation system!

---

## Derivative Rules

Now that you understand WHY derivatives matter, here are the rules:

### Power Rule

$$
\frac{d}{dx}x^n = nx^{n-1}
$$

```python
# Examples
# d/dx (xÂ²) = 2x
# d/dx (xÂ³) = 3xÂ²
# d/dx (xâ»Â¹) = -xâ»Â²

def power_rule_derivative(n):
    """Returns derivative function for x^n"""
    return lambda x: n * x**(n-1)

# Derivative of xÂ²
f_prime = power_rule_derivative(2)
print(f"d/dx(xÂ²) at x=3: {f_prime(3)}")  # 6
```

### Constant Rule

$$
\frac{d}{dx}c = 0
$$

**Why?** Constants don't change!

### Sum Rule

$$
\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)
$$

```python
# Example: f(x) = xÂ² + 3x + 5
# f'(x) = 2x + 3 + 0 = 2x + 3

def f(x):
    return x**2 + 3*x + 5

def f_derivative(x):
    return 2*x + 3

# Verify numerically
x = 4
h = 0.0001
numerical = (f(x+h) - f(x)) / h
analytical = f_derivative(x)

print(f"Numerical: {numerical:.4f}")
print(f"Analytical: {analytical}")
```

### Product Rule

$$
\frac{d}{dx}[f(x)g(x)] = f'(x)g(x) + f(x)g'(x)
$$

```python
# Example: h(x) = xÂ² Â· sin(x)
# h'(x) = 2xÂ·sin(x) + xÂ²Â·cos(x)

import numpy as np

def h(x):
    return x**2 * np.sin(x)

def h_derivative(x):
    return 2*x*np.sin(x) + x**2*np.cos(x)

x = 2
print(f"h'({x}) = {h_derivative(x):.4f}")
```

### Chain Rule (Preview)

$$
\frac{d}{dx}f(g(x)) = f'(g(x)) \cdot g'(x)
$$

We'll cover this in depth in Module 3!

---

## Higher-Order Derivatives

### Second Derivative

The derivative of the derivative!

$$
f''(x) = \frac{d^2}{dx^2}f(x)
$$

**Interpretation**: How fast is the rate of change changing?

```python
# Example: f(x) = xÂ³
# f'(x) = 3xÂ²
# f''(x) = 6x

def f(x):
    return x**3

def f_prime(x):
    return 3*x**2

def f_double_prime(x):
    return 6*x

x = 2
print(f"f({x}) = {f(x)}")
print(f"f'({x}) = {f_prime(x)}")  # Rate of change
print(f"f''({x}) = {f_double_prime(x)}")  # Acceleration
```

**Physical Interpretation**:
- $f(x)$ = position
- $f'(x)$ = velocity (rate of change of position)
- $f''(x)$ = acceleration (rate of change of velocity)

### Concavity

**Second derivative tells you about curvature**:

- $f''(x) > 0$ â†’ Concave up (smiling face) â†’ Local minimum
- $f''(x) < 0$ â†’ Concave down (frowning face) â†’ Local maximum
- $f''(x) = 0$ â†’ Inflection point

```python
# Cost function: C(x) = xÂ² - 10x + 100
# C'(x) = 2x - 10
# C''(x) = 2

# Since C''(x) = 2 > 0 everywhere, function is always concave up
# So x=5 (where C'(x)=0) is definitely a MINIMUM!

def cost(x):
    return x**2 - 10*x + 100

def cost_second_derivative(x):
    return 2

x = 5
print(f"At x={x}:")
print(f"Second derivative: {cost_second_derivative(x)}")
print(f"â†’ Concave up â†’ This is a minimum!")
```

---

## Numerical Derivatives

When you can't compute derivatives analytically:

### Forward Difference

$$
f'(x) \approx \frac{f(x+h) - f(x)}{h}
$$

### Central Difference (More Accurate)

$$
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}
$$

```python
def numerical_derivative(f, x, h=1e-5, method='central'):
    """Compute derivative numerically"""
    if method == 'forward':
        return (f(x + h) - f(x)) / h
    elif method == 'central':
        return (f(x + h) - f(x - h)) / (2 * h)
    else:
        raise ValueError("Method must be 'forward' or 'central'")

# Test on f(x) = xÂ²
def f(x):
    return x**2

x = 3
exact = 2*x  # Analytical derivative

forward = numerical_derivative(f, x, method='forward')
central = numerical_derivative(f, x, method='central')

print(f"Exact: {exact}")
print(f"Forward difference: {forward:.6f}")
print(f"Central difference: {central:.6f}")
```

**When to use**:
- Complex functions without closed-form derivatives
- Debugging analytical derivatives
- Quick prototyping

---

## Practice Exercises

### Exercise 1: Profit Maximization

```python
# A company's profit function is:
# P(x) = -2xÂ² + 40x - 100
# where x is production quantity in thousands

# TODO:
# 1. Find the derivative P'(x)
# 2. Find the production quantity that maximizes profit
# 3. What is the maximum profit?
# 4. Verify it's a maximum using the second derivative
```

<details>
<summary>Solution</summary>

```python
def profit(x):
    return -2*x**2 + 40*x - 100

def profit_derivative(x):
    return -4*x + 40

def profit_second_derivative(x):
    return -4

# Find maximum: P'(x) = 0
# -4x + 40 = 0
# x = 10

optimal_x = 10
max_profit = profit(optimal_x)

print(f"Optimal production: {optimal_x},000 units")
print(f"Maximum profit: ${max_profit},000")

# Verify it's a maximum
print(f"Second derivative: {profit_second_derivative(optimal_x)}")
print(f"â†’ Negative â†’ Concave down â†’ Maximum!")
```
</details>

---

## Key Takeaways

âœ… **Derivative = rate of change** - How output changes with input  
âœ… **Geometric view** - Slope of tangent line  
âœ… **Optimization** - Set derivative = 0 to find min/max  
âœ… **Second derivative** - Tells you if it's min or max  
âœ… **ML connection** - Gradient descent uses derivatives to learn  

---

## What's Next?

You now understand derivatives for single-variable functions. But ML models have MANY variables (thousands or millions!).

How do we handle that? **Gradients** - the multi-variable version of derivatives!

<Card title="Next: Gradients & Multivariable Calculus" icon="arrow-right" href="/courses/math-for-ml-calculus/02-gradients">
  Learn how to optimize functions with many variables
</Card>
