---
title: "Final Project: Build a Neural Network"
sidebarTitle: "Final Project"
description: "Build a complete Neural Network from scratch using only NumPy"
icon: "flag-checkered"
---

# Final Project: The Architect

## Your Graduation Exam

You have learned the theory.
- **Derivatives**: The rate of change.
- **Gradients**: The direction of steepest ascent.
- **Chain Rule**: How to propagate blame.
- **Gradient Descent**: How to learn.

Now, you must prove your mastery. You will not use PyTorch. You will not use TensorFlow.
**You will build a brain using nothing but raw math (NumPy).**

---

## The Blueprint

You are building a neural network to solve a classification problem.

**Architecture**:
- **Input Layer**: 2 neurons ($x_1, x_2$)
- **Hidden Layer**: 3 neurons ($h_1, h_2, h_3$)
- **Output Layer**: 1 neuron ($y$)

![Neural Network Blueprint](/images/courses/math-for-ml-calculus/neural-network-architecture.svg)

---

## Step 1: The Bricks (Initialization)

A neural network is just a collection of matrices (weights) and vectors (biases).

```python
import numpy as np

def init_params(input_size, hidden_size, output_size):
    # Weights: Random small numbers
    # Biases: Zeros
    W1 = np.random.randn(input_size, hidden_size) * 0.01
    b1 = np.zeros((1, hidden_size))
    
    W2 = np.random.randn(hidden_size, output_size) * 0.01
    b2 = np.zeros((1, output_size))
    
    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2}
```

---

## Step 2: The Mortar (Activation)

Neurons need to be non-linear. We'll use **ReLU** for the hidden layer and **Sigmoid** for the output.

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def relu(z):
    return np.maximum(0, z)

# We also need derivatives for the backward pass!
def sigmoid_derivative(a):
    return a * (1 - a)

def relu_derivative(z):
    return (z > 0).astype(float)
```

---

## Step 3: The Construction (Forward Pass)

Data flows from input to output.
$$ Z_1 = X \cdot W_1 + b_1 $$
$$ A_1 = \text{ReLU}(Z_1) $$
$$ Z_2 = A_1 \cdot W_2 + b_2 $$
$$ A_2 = \text{Sigmoid}(Z_2) $$

```python
def forward(X, params):
    # Unpack
    W1, b1 = params["W1"], params["b1"]
    W2, b2 = params["W2"], params["b2"]
    
    # Layer 1
    Z1 = np.dot(X, W1) + b1
    A1 = relu(Z1)
    
    # Layer 2
    Z2 = np.dot(A1, W2) + b2
    A2 = sigmoid(Z2)
    
    cache = {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}
    return A2, cache
```

---

## Step 4: The Inspection (Loss)

How wrong is our model? We'll use **Mean Squared Error** for simplicity.

```python
def compute_loss(Y_pred, Y_true):
    m = Y_true.shape[0]
    loss = (1 / (2*m)) * np.sum((Y_pred - Y_true)**2)
    return loss
```

---

## Step 5: The Renovation (Backward Pass)

This is the hardest part. We use the **Chain Rule** to find gradients.

```python
def backward(X, Y_true, params, cache):
    m = X.shape[0]
    W2 = params["W2"]
    A1, A2 = cache["A1"], cache["A2"]
    Z1 = cache["Z1"]
    
    # Output Layer Gradients
    dZ2 = A2 - Y_true  # Derivative of MSE + Sigmoid simplifies to this!
    dW2 = (1/m) * np.dot(A1.T, dZ2)
    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)
    
    # Hidden Layer Gradients
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * relu_derivative(Z1)
    dW1 = (1/m) * np.dot(X.T, dZ1)
    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)
    
    return {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}
```

---

## Step 6: The Training Loop

Put it all together!

```python
def train(X, Y, epochs=1000, lr=0.1):
    # 1. Initialize
    params = init_params(2, 3, 1)
    
    for i in range(epochs):
        # 2. Forward
        Y_pred, cache = forward(X, params)
        
        # 3. Loss
        if i % 100 == 0:
            loss = compute_loss(Y_pred, Y)
            print(f"Epoch {i}: Loss {loss:.4f}")
            
        # 4. Backward
        grads = backward(X, Y, params, cache)
        
        # 5. Update (Gradient Descent)
        params["W1"] -= lr * grads["dW1"]
        params["b1"] -= lr * grads["db1"]
        params["W2"] -= lr * grads["dW2"]
        params["b2"] -= lr * grads["db2"]
        
    return params

# Test it on XOR data
X = np.array([[0,0], [0,1], [1,0], [1,1]])
Y = np.array([[0], [1], [1], [0]])

print("Training...")
trained_params = train(X, Y, epochs=5000, lr=0.1)

print("\nPredictions:")
preds, _ = forward(X, trained_params)
print(preds)
```

---

## Congratulations!

You have just built a neural network from scratch.
- You didn't use a "black box".
- You built the box yourself.
- You understand every gear and lever inside.

**This is the power of Calculus.** It turns "magic" into **math**.

<Card title="Start Linear Algebra" icon="arrow-right" href="/courses/math-for-ml-linear-algebra/00-introduction">
  Now master the other half of AI: Linear Algebra.
</Card>
