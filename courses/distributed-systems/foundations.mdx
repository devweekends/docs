---
title: "Foundations of Distributed Systems"
sidebarTitle: "Foundations"
description: "Build the mental models that all distributed systems are built upon"
icon: "book"
---

# Track 1: Foundations

Before diving into consensus protocols and complex architectures, you must understand the fundamental challenges that make distributed systems hard.

<Info>
**Track Duration**: 28-38 hours  
**Modules**: 5  
**Key Topics**: Network fundamentals, Time & Ordering, Failure Models, CAP/PACELC
</Info>

---

## Module 1: Why Distributed Systems?

### The Need for Distribution

Every successful system eventually outgrows a single machine:

```text
┌─────────────────────────────────────────────────────────────────────────────┐
│                     WHY DISTRIBUTED SYSTEMS?                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  SINGLE MACHINE LIMITS                    DISTRIBUTION BENEFITS             │
│  ─────────────────────                    ─────────────────────             │
│  ▸ CPU: ~128 cores max                    ▸ Horizontal scaling              │
│  ▸ RAM: ~12 TB max (expensive)            ▸ Fault tolerance                 │
│  ▸ Disk: I/O bottlenecks                  ▸ Geographic distribution         │
│  ▸ Network: Single point of failure       ▸ Cost efficiency                 │
│                                           ▸ Regulatory compliance           │
│                                                                              │
│  EXAMPLE: Google Search                                                      │
│  ─────────────────────                                                      │
│  ▸ 8.5 billion searches/day                                                 │
│  ▸ Index: 100+ petabytes                                                    │
│  ▸ Response time: < 0.5 seconds                                             │
│  ▸ Impossible on a single machine                                           │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Types of Distributed Systems

<CardGroup cols={3}>
  <Card title="Compute Clusters" icon="microchip">
    **Purpose**: Process data across many machines
    
    **Examples**:
    - Hadoop/Spark clusters
    - Kubernetes pods
    - AWS Lambda fleet
  </Card>
  <Card title="Storage Systems" icon="database">
    **Purpose**: Store and retrieve data reliably
    
    **Examples**:
    - Distributed databases (Spanner, CockroachDB)
    - Object stores (S3, GCS)
    - Distributed file systems (HDFS, GFS)
  </Card>
  <Card title="Coordination Systems" icon="sitemap">
    **Purpose**: Coordinate distributed components
    
    **Examples**:
    - Zookeeper, etcd, Consul
    - Message queues (Kafka, RabbitMQ)
    - Service meshes
  </Card>
</CardGroup>

### The Eight Fallacies of Distributed Computing

Every distributed systems engineer must internalize these:

<AccordionGroup>
  <Accordion title="1. The Network is Reliable" icon="wifi">
    **Reality**: Networks fail constantly.
    
    ```
    Common failures:
    ├── Packet loss (1 in 1000 typical, worse under load)
    ├── Network partitions (entire segments become unreachable)
    ├── Cable cuts (more common than you'd think)
    └── Switch/router failures
    
    Real incident: AWS US-East-1 2011
    ├── Network configuration change
    ├── Cascading failures
    └── Multi-hour outage affecting Netflix, Reddit, Quora
    ```
    
    **Defense**: Implement retries, timeouts, circuit breakers, and idempotency.
  </Accordion>
  
  <Accordion title="2. Latency is Zero" icon="stopwatch">
    **Reality**: Every network call has latency.
    
    | Operation | Time |
    |-----------|------|
    | Same datacenter | 0.5ms |
    | Cross-region (US East ↔ West) | 40-65ms |
    | Cross-continent (US ↔ Europe) | 70-120ms |
    | Cross-pacific (US ↔ Asia) | 100-200ms |
    
    **Impact**: A service with 10 sequential remote calls adds 500-2000ms latency.
    
    **Defense**: Cache aggressively, parallelize calls, use async processing.
  </Accordion>
  
  <Accordion title="3. Bandwidth is Infinite" icon="gauge-high">
    **Reality**: Bandwidth is expensive and limited.
    
    ```
    Example: Transferring 1TB over network
    
    10 Gbps link:
    └── Theoretical: 13 minutes
    └── Real-world: 20-30 minutes (overhead, congestion)
    
    AWS Data Transfer Costs:
    └── Intra-region: $0.01/GB
    └── Cross-region: $0.02/GB
    └── To internet: $0.09/GB
    └── 1PB/month internet egress: $90,000
    ```
    
    **Defense**: Compress data, use efficient serialization (protobuf), batch requests.
  </Accordion>
  
  <Accordion title="4. The Network is Secure" icon="shield">
    **Reality**: Assume the network is compromised.
    
    Attack vectors:
    - Man-in-the-middle attacks
    - DNS spoofing
    - BGP hijacking (internet routing attacks)
    - Insider threats
    
    **Defense**: mTLS everywhere, zero-trust architecture, encrypt at rest and in transit.
  </Accordion>
  
  <Accordion title="5. Topology Doesn't Change" icon="diagram-project">
    **Reality**: Network topology changes constantly.
    
    ```
    Dynamic changes:
    ├── Auto-scaling adds/removes instances
    ├── Deployments replace containers
    ├── Failovers redirect traffic
    └── Cloud provider maintenance
    ```
    
    **Defense**: Use service discovery, health checks, load balancer draining.
  </Accordion>
  
  <Accordion title="6. There is One Administrator" icon="user-gear">
    **Reality**: Multiple teams, policies, and even companies.
    
    In a typical microservices architecture:
    - Platform team manages Kubernetes
    - Each service team manages their services
    - Security team manages policies
    - Network team manages infrastructure
    
    **Defense**: Clear ownership, documented interfaces, SLAs between teams.
  </Accordion>
  
  <Accordion title="7. Transport Cost is Zero" icon="dollar-sign">
    **Reality**: Data transfer costs real money.
    
    | Cloud Provider | Egress Cost |
    |----------------|-------------|
    | AWS | $0.09/GB |
    | GCP | $0.12/GB |
    | Azure | $0.087/GB |
    
    **High-traffic example**: 1PB egress = $90,000/month
    
    **Defense**: Keep data close to compute, use CDNs, compress aggressively.
  </Accordion>
  
  <Accordion title="8. The Network is Homogeneous" icon="network-wired">
    **Reality**: Different hardware, protocols, and vendors everywhere.
    
    ```
    Heterogeneity:
    ├── Different server generations (various CPU, RAM)
    ├── Different network equipment (Cisco, Juniper, etc.)
    ├── Different protocols (HTTP/1.1, HTTP/2, gRPC, QUIC)
    ├── Different cloud providers (AWS, GCP, Azure)
    └── Different latency profiles
    ```
    
    **Defense**: Abstract hardware differences, use consistent protocols, test on diverse environments.
  </Accordion>
</AccordionGroup>

---

## Module 2: Network Fundamentals

### TCP Guarantees and Failures

TCP provides order and reliability, but not much else:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          TCP GUARANTEES                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ✓ GUARANTEED                         ✗ NOT GUARANTEED                      │
│  ─────────────                        ─────────────────                      │
│  ▸ Ordered delivery                   ▸ Bounded latency                     │
│  ▸ Reliable delivery (eventually)     ▸ Bounded bandwidth                   │
│  ▸ Error detection                    ▸ Connection will succeed             │
│  ▸ Flow control                       ▸ Message boundaries preserved        │
│  ▸ Duplicate elimination              ▸ Timely delivery                     │
│                                                                              │
│  FAILURE MODES                                                               │
│  ─────────────                                                              │
│  ▸ Connection reset (RST)                                                   │
│  ▸ Connection timeout                                                       │
│  ▸ Half-open connections (one side doesn't know it's dead)                 │
│  ▸ Head-of-line blocking                                                    │
│  ▸ TCP incast (many-to-one overwhelms receiver)                            │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Network Partitions

A **network partition** occurs when nodes can't communicate with each other:

```
BEFORE PARTITION:
                    ┌────────────────┐
                    │   Network      │
                    └────────────────┘
                    ↑    ↑    ↑    ↑
                    │    │    │    │
               ┌────┼────┼────┼────┼────┐
               │    │    │    │    │    │
              [A]  [B]  [C]  [D]  [E]  [F]
              All nodes can communicate


DURING PARTITION:
     Partition 1              ║              Partition 2
  ┌─────────────┐             ║           ┌─────────────┐
  │   Network   │             ║           │   Network   │
  └─────────────┘             ║           └─────────────┘
   ↑     ↑    ↑               ║            ↑     ↑    ↑
   │     │    │           PARTITION        │     │    │
  [A]   [B]  [C]              ║           [D]   [E]  [F]
  
  A, B, C can talk             D, E, F can talk
  to each other                to each other
  but NOT to D, E, F           but NOT to A, B, C
```

**Key insight**: During a partition, you must choose between:
- **Availability**: Both partitions continue serving requests (might diverge)
- **Consistency**: Reject requests from the minority partition

### Message Delivery Semantics

<Tabs>
  <Tab title="At-Most-Once">
    **Fire and forget** - no retries
    
    ```python
    def send_message(message):
        network.send(message)  # No retry on failure
        # Message delivered 0 or 1 times
    ```
    
    **Use case**: Metrics, logs (losing some is acceptable)
    
    **Risk**: Message loss
  </Tab>
  
  <Tab title="At-Least-Once">
    **Retry until acknowledged**
    
    ```python
    def send_message(message):
        while True:
            network.send(message)
            if ack_received():
                break
            # Retry on timeout
        # Message delivered 1 or more times
    ```
    
    **Use case**: Most systems (with idempotency)
    
    **Risk**: Duplicate processing
  </Tab>
  
  <Tab title="Exactly-Once">
    **Holy grail** - each message processed exactly once
    
    ```python
    def send_message(message):
        message_id = generate_unique_id()
        while True:
            network.send(message, id=message_id)
            if ack_received():
                break
        # Receiver deduplicates by message_id
    ```
    
    **Use case**: Financial transactions, critical operations
    
    **Reality**: Implemented as at-least-once + deduplication
  </Tab>
</Tabs>

### Failure Detection

How do you know if a node is dead?

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      FAILURE DETECTION APPROACHES                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  HEARTBEAT-BASED                                                            │
│  ───────────────                                                            │
│  Node A ──[heartbeat]──> Node B                                             │
│           every 1 sec                                                        │
│                                                                              │
│  If no heartbeat for 5 seconds → Mark as failed                             │
│                                                                              │
│  Problems:                                                                   │
│  ├── Too aggressive: False positives (just slow, not dead)                  │
│  └── Too conservative: Slow detection of actual failures                    │
│                                                                              │
│  PHI ACCRUAL FAILURE DETECTOR (Cassandra uses this)                         │
│  ─────────────────────────────                                              │
│  Instead of binary (alive/dead), output a suspicion level:                  │
│                                                                              │
│  φ = -log₁₀(probability node is still alive)                                │
│                                                                              │
│  φ = 1 → 90% confident node is alive                                        │
│  φ = 2 → 99% confident node is alive                                        │
**This is one of the most important topics in distributed systems.**

### Why Physical Clocks Fail

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    PHYSICAL CLOCK PROBLEMS                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  CLOCK SKEW (different machines show different times)                        │
│  ────────────                                                               │
│  Machine A: 10:00:00.000                                                    │
│  Machine B: 10:00:00.150 (150ms ahead)                                      │
│  Machine C: 09:59:59.850 (150ms behind)                                     │
│                                                                              │
│  NTP SYNCHRONIZATION                                                         │
│  ───────────────────                                                        │
│  NTP accuracy: 1-10ms on LAN, 10-100ms over internet                        │
│  During sync: Clock can jump forward or backward!                           │
│                                                                              │
│  CLOCK DRIFT                                                                │
│  ────────────                                                               │
│  Quartz crystals drift: ~50ppm (50 microseconds per second)                 │
│  After 1 hour: 180ms drift possible                                         │
│                                                                              │
│  LEAP SECONDS                                                               │
│  ────────────                                                               │
│  Added to compensate for Earth's rotation slowing                           │
│  23:59:59 → 23:59:60 → 00:00:00                                            │
│  Famously caused issues at Reddit, LinkedIn, Mozilla (2012)                 │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Lamport Timestamps

Leslie Lamport's logical clock solution (1978):

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      LAMPORT TIMESTAMPS                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  RULES:                                                                      │
│  1. Each process maintains a counter (starts at 0)                          │
│  2. Before each event, increment the counter                                │
│  3. When sending: attach current counter to message                         │
│  4. When receiving: counter = max(local, received) + 1                      │
│                                                                              │
│  EXAMPLE:                                                                    │
│                                                                              │
│  Process A        Process B        Process C                                │
│  ─────────        ─────────        ─────────                                │
│     1 ────────────────────> 2                                               │
│     │                       │                                               │
│     │           5 <────────────────── 3                                     │
│     │           │                     │                                     │
│     4           6 ────────────────────> 7                                   │
│     │           │                     │                                     │
│     │           │           8 <───────┘                                     │
│                                                                              │
│  PROPERTY: If A → B (A happened before B), then L(A) < L(B)                 │
│  LIMITATION: L(A) < L(B) does NOT mean A → B (no causality)                 │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Implementation**:

```python
class LamportClock:
    def __init__(self):
        self.counter = 0
    
    def increment(self):
        """Call before any local event"""
        self.counter += 1
        return self.counter
    
    def send(self):
        """Call when sending a message"""
        self.counter += 1
        return self.counter
    
    def receive(self, received_timestamp):
        """Call when receiving a message"""
        self.counter = max(self.counter, received_timestamp) + 1
        return self.counter
```

### Vector Clocks

Vector clocks track **causality** - they tell you if two events are related:

![Vector Clocks](/images/courses/vector-clocks.svg)

**Implementation**:

```python
class VectorClock:
    def __init__(self, node_id, num_nodes):
        self.node_id = node_id
        self.clock = [0] * num_nodes
    
    def increment(self):
        self.clock[self.node_id] += 1
        return self.clock.copy()
    
    def send(self):
        self.clock[self.node_id] += 1
        return self.clock.copy()
    
    def receive(self, received_clock):
        for i in range(len(self.clock)):
            self.clock[i] = max(self.clock[i], received_clock[i])
        self.clock[self.node_id] += 1
        return self.clock.copy()
    
    def happens_before(self, other_clock):
        """Returns True if this clock happened before other_clock"""
        return (all(s <= o for s, o in zip(self.clock, other_clock)) and
                any(s < o for s, o in zip(self.clock, other_clock)))
    
    def concurrent(self, other_clock):
        """Returns True if events are concurrent"""
        return (not self.happens_before(other_clock) and 
                not VectorClock.compare(other_clock, self.clock))
```

### Hybrid Logical Clocks (HLC)

Combines physical and logical clocks:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    HYBRID LOGICAL CLOCKS                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  HLC = (physical_time, logical_counter)                                     │
│                                                                              │
│  PROPERTIES:                                                                │
│  1. Always greater than physical time (useful for debugging)                │
│  2. Preserves causality like logical clocks                                 │
│  3. Bounded skew from physical time                                         │
│                                                                              │
│  USED BY:                                                                   │
│  ├── CockroachDB                                                            │
│  ├── MongoDB                                                                │
│  └── Many distributed databases                                             │
│                                                                              │
│  ALGORITHM:                                                                 │
│  ─────────                                                                  │
│  On local event or send:                                                    │
│    l' = max(l, physical_time)                                               │
│    if l' == l: c' = c + 1                                                   │
│    else: c' = 0                                                             │
│    l = l', c = c'                                                           │
│                                                                              │
│  On receive(m.l, m.c):                                                      │
│    l' = max(l, m.l, physical_time)                                          │
│    if l' == l == m.l: c' = max(c, m.c) + 1                                  │
│    else if l' == l: c' = c + 1                                              │
│    else if l' == m.l: c' = m.c + 1                                          │
│    else: c' = 0                                                             │
│    l = l', c = c'                                                           │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### TrueTime (Google Spanner)

Google's hardware-based approach:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          TRUETIME                                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  HARDWARE:                                                                  │
│  ├── GPS receivers (provide absolute time)                                  │
│  └── Atomic clocks (maintain time during GPS outages)                      │
│                                                                              │
│  API:                                                                       │
│  TT.now() returns [earliest, latest] interval                               │
│  TT.after(t) → true if t has definitely passed                             │
│  TT.before(t) → true if t has definitely not arrived                       │
│                                                                              │
│  UNCERTAINTY:                                                               │
│  ├── Average: 4ms                                                           │
│  └── Worst case: 10ms                                                       │
│                                                                              │
│  HOW SPANNER USES IT:                                                       │
│  ────────────────────                                                       │
│  Transaction commits at timestamp t                                         │
│  Wait until TT.after(t) before making visible                               │
│  This guarantees external consistency!                                      │
│                                                                              │
│  COST: Special hardware + GPS antennas + careful engineering                │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Module 4: Failure Models

Understanding what can go wrong is crucial for building resilient systems.

### Types of Failures

<Tabs>
  <Tab title="Fail-Stop">
    **Node crashes and stays down**
    
    Characteristics:
    - Clean failure, detectable
    - Node doesn't recover with corrupted state
    - Easiest to handle
    
    Example: Hardware failure, power loss
    
    ```
    [Node A] ──── X ────────────────────
                 │
              Crash
              (other nodes eventually detect)
    ```
  </Tab>
  
  <Tab title="Fail-Recover">
    **Node crashes but can restart**
    
    Characteristics:
    - Node may recover with persistent state
    - Must handle partial writes
    - Need recovery protocols
    
    Example: Software crash, restart
    
    ```
    [Node A] ──── X ────── [Restart] ──── [Running]
                 │             │
              Crash       Recovery
                         (load state from disk)
    ```
  </Tab>
  
  <Tab title="Byzantine">
    **Node behaves arbitrarily (possibly maliciously)**
    
    Characteristics:
    - Can send wrong/conflicting information
    - Can lie about its state
    - Hardest to handle
    - Requires 3f+1 nodes to tolerate f Byzantine failures
    
    Example: Hacked node, buggy code, malicious actor
    
    ```
    [Node A] says "value = 5" to Node B
    [Node A] says "value = 7" to Node C
    (Same node, different lies)
    ```
  </Tab>
</Tabs>

### Partial Failures

The defining challenge of distributed systems:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        PARTIAL FAILURES                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  SCENARIO: Node A calls Node B                                              │
│                                                                              │
│  A ──[request]──> B                                                         │
│                   │                                                         │
│         (silence)                                                           │
│                                                                              │
│  What happened?                                                             │
│  ├── Request lost in network?                                               │
│  ├── B crashed before processing?                                           │
│  ├── B processed but crashed before responding?                             │
│  ├── B responded but response lost?                                         │
│  └── B is just slow?                                                        │
│                                                                              │
│  A DOESN'T KNOW!                                                            │
│                                                                              │
│  IMPLICATIONS:                                                              │
│  ─────────────                                                              │
│  1. Cannot distinguish slow from dead                                       │
│  2. Operations might have executed (or not)                                 │
│  3. Must design for uncertainty                                             │
│  4. Idempotency becomes essential                                           │
│                                                                              │
│  PATTERN: Retry with idempotency keys                                       │
│  ─────────────────────────────────                                          │
│  request_id = "abc123"                                                      │
│  if already_processed(request_id):                                          │
│      return cached_response                                                 │
│  else:                                                                      │
│      process_and_store(request_id, response)                                │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Gray Failures

Subtle failures that are hard to detect:

```
Examples of gray failures:
├── CPU running at 50% speed due to thermal throttling
├── One of 10 disks has 100x higher latency
├── Memory errors causing random crashes
├── Network dropping 1% of packets
├── Software bug triggered only by specific input
└── Gradual memory leak over hours

Detection challenges:
├── Metrics look "mostly okay"
├── Intermittent symptoms
├── Hard to reproduce
└── May only affect some requests
```

---

## Module 5: CAP and PACELC Theorems

### CAP Theorem

<Warning>
CAP is often misunderstood. It only applies **during a network partition**.
</Warning>

![CAP Theorem](/images/courses/cap-theorem.svg)

### CP vs AP Deep Dive

<CardGroup cols={2}>
  <Card title="CP Systems" icon="lock">
    **During partition, choose Consistency**
    
    Behavior:
    - Minority partition stops accepting writes
    - May reject reads too (for linearizability)
    - Majority partition continues
    
    Good for:
    - Financial systems
    - Inventory management
    - Any system where stale data is dangerous
    
    Example: Bank account balance
  </Card>
  <Card title="AP Systems" icon="globe">
    **During partition, choose Availability**
    
    Behavior:
    - Both partitions continue serving
    - May return stale/conflicting data
    - Resolve conflicts after partition heals
    
    Good for:
    - Social media feeds
    - Shopping carts
    - DNS
    
    Example: Twitter timeline
  </Card>
</CardGroup>

### PACELC: The Better Framework

CAP only describes behavior during partitions. PACELC adds normal operation:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           PACELC                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  IF Partition                                                               │
│     Choose between Availability and Consistency                             │
│  ELSE (normal operation)                                                    │
│     Choose between Latency and Consistency                                  │
│                                                                              │
│  EXAMPLES:                                                                  │
│  ─────────                                                                  │
│  DynamoDB/Cassandra: PA/EL                                                  │
│  ├── Partition: Available                                                   │
│  └── Normal: Low latency (eventual consistency)                             │
│                                                                              │
│  Spanner: PC/EC                                                             │
│  ├── Partition: Consistent (blocks)                                         │
│  └── Normal: Consistent (higher latency, but distributed transactions)     │
│                                                                              │
│  MongoDB: PA/EC                                                             │
│  ├── Partition: Available (eventual reads)                                  │
│  └── Normal: Consistent (waits for primary)                                 │
│                                                                              │
│  PNUTS (Yahoo): PC/EL                                                       │
│  ├── Partition: Consistent (blocks)                                         │
│  └── Normal: Low latency (local reads)                                      │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Consistency Spectrum

Consistency is not binary - it's a spectrum:

```
STRONGER ─────────────────────────────────────────────────── WEAKER
   │                                                           │
   ▼                                                           ▼
┌──────────────┬─────────────┬────────────┬──────────────┬────────────┐
│ Linearizable │ Sequential  │  Causal    │   Session    │  Eventual  │
│              │ Consistent  │            │  Consistent  │            │
└──────────────┴─────────────┴────────────┴──────────────┴────────────┘
   │                │              │             │              │
   │                │              │             │              │
   ▼                ▼              ▼             ▼              ▼
All ops appear   All ops see    Causally     Same session   Eventually
in real-time    same order     related ops   sees own       all see
order           (not real-     in order      writes         same value
                time)
                
Cost: Highest ◄─────────────────────────────────────────► Cost: Lowest
Latency: High ◄─────────────────────────────────────────► Latency: Low
```

---

## Key Interview Questions

<AccordionGroup>
  <Accordion title="Q: Can you achieve consensus in an asynchronous system?">
    **Answer**: No, provably impossible (FLP theorem). But we can achieve it practically by:
    - Using timeouts (introduces synchrony assumption)
    - Randomization
    - Failure detectors
    
    Most practical systems (Raft, Paxos) assume partial synchrony.
  </Accordion>
  
  <Accordion title="Q: How do you detect if a node is dead vs slow?">
    **Answer**: You can't with certainty. Approaches:
    - **Timeouts**: Simple but prone to false positives
    - **Heartbeats**: Regular health checks
    - **Phi accrual detector**: Probabilistic suspicion level
    - **Lease-based**: Node must renew lease to be considered alive
    
    In practice, use adaptive timeouts based on historical latency.
  </Accordion>
  
  <Accordion title="Q: Explain vector clocks vs Lamport timestamps">
    **Answer**:
    - **Lamport**: Single counter, preserves "happens-before" one direction only
    - **Vector clocks**: One counter per node, can detect concurrent events
    
    Use Lamport when you just need ordering.
    Use vector clocks when you need to detect conflicts.
  </Accordion>
  
  <Accordion title="Q: Is your system CP or AP?">
    **Answer**: Frame it as a trade-off discussion:
    - First, acknowledge CAP only applies during partitions
    - Discuss what consistency level you actually need
    - Explain PACELC for normal operation
    - Give examples: "For our payment system, we're CP because incorrect balance is unacceptable. For user preferences, we're AP because eventual consistency is fine."
  </Accordion>
</AccordionGroup>

---

## Next Steps

<Card title="Continue to Track 2: Consensus Protocols" icon="arrow-right" href="/courses/distributed-systems/consensus">
  Learn Paxos, Raft, and other consensus algorithms that power distributed systems
</Card>
