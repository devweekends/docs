---
title: "Distributed Systems Mastery"
sidebarTitle: "Course Overview"
description: "Crack FAANG-level interviews with deep distributed systems expertise — Consensus, Raft, Paxos, and production patterns"
icon: "network-wired"
---

# Distributed Systems Mastery

A comprehensive, interview-focused curriculum designed for engineers targeting **Staff/Principal roles at top tech companies** (Google, Meta, Amazon, Netflix, Stripe, etc.). This course covers everything from fundamentals to cutting-edge distributed systems concepts.

<Info>
**Course Duration**: 14-18 weeks (self-paced)  
**Target Outcome**: Staff+ Engineer at FAANG / Top-tier distributed systems expertise  
**Prerequisites**: Strong programming, basic networking, database fundamentals  
**Language**: Concepts with implementations in Go/Java/Python
</Info>

---

## Why This Course?

<CardGroup cols={2}>
  <Card title="FAANG Interview Ready" icon="building">
    Covers exact topics asked at Google, Amazon, Meta, and other top companies
  </Card>
  <Card title="Deep Theoretical Foundation" icon="book">
    Understand Raft, Paxos, ZAB, and other consensus protocols inside-out
  </Card>
  <Card title="Production Battle-Tested" icon="shield-halved">
    Patterns from systems handling millions of QPS at scale
  </Card>
  <Card title="Hands-On Projects" icon="laptop-code">
    Build your own distributed KV store, consensus implementation, and more
  </Card>
</CardGroup>

<Warning>
**Interview Reality**: At Staff+ level, you're expected to design systems that handle **billions of requests**, survive **data center failures**, and maintain **consistency guarantees**. This course prepares you for exactly that.
</Warning>

---

## What Companies Ask

| Company | Common Topics |
|---------|--------------|
| **Google** | Consensus protocols, Spanner, Bigtable internals, Paxos, distributed transactions |
| **Amazon** | DynamoDB internals, eventual consistency, vector clocks, Dynamo paper |
| **Meta** | TAO, ZippyDB, consensus at scale, social graph distribution |
| **Netflix** | EVCache, Cassandra, chaos engineering, resilience patterns |
| **Stripe** | Distributed transactions, exactly-once delivery, idempotency |
| **Uber** | Ringpop, consistent hashing, real-time systems, Cadence workflows |

---

## Course Structure

The curriculum is organized into **6 tracks** progressing from fundamentals to Staff+ expertise:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                   DISTRIBUTED SYSTEMS MASTERY                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  TRACK 1: FOUNDATIONS           TRACK 2: CONSENSUS                          │
│  ─────────────────────          ──────────────────                          │
│  □ Why Distributed?             □ The Consensus Problem                     │
│  □ Network Fundamentals         □ Paxos (Single & Multi)                    │
│  □ Time & Ordering              □ Raft (In-Depth)                           │
│  □ Failure Models               □ Viewstamped Replication                   │
│  □ CAP/PACELC Theorems          □ ZAB (Zookeeper)                           │
│                                                                              │
│  TRACK 3: REPLICATION           TRACK 4: TRANSACTIONS                       │
│  ─────────────────────          ──────────────────────                      │
│  □ Single-Leader                □ ACID in Distributed World                 │
│  □ Multi-Leader                 □ 2PC and 3PC                               │
│  □ Leaderless                   □ Saga Pattern                              │
│  □ Conflict Resolution          □ TCC Pattern                               │
│  □ CRDTs                        □ Distributed Locking                       │
│                                                                              │
│  TRACK 5: DATA SYSTEMS          TRACK 6: PRODUCTION                         │
│  ─────────────────────          ────────────────────                        │
│  □ Partitioning Strategies      □ Observability at Scale                    │
│  □ Consistent Hashing           □ Chaos Engineering                         │
│  □ Distributed Databases        □ SRE Practices                             │
│  □ Distributed Storage          □ Incident Management                       │
│  □ Stream Processing            □ Capacity Planning                         │
│                                                                              │
│  CAPSTONE PROJECTS                                                          │
│  ─────────────────                                                          │
│  □ Build Distributed KV Store                                               │
│  □ Implement Raft Consensus                                                 │
│  □ Design Interview Practice                                                │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Track 1: Foundations

Build the mental models that all distributed systems are built upon.

<AccordionGroup>
  <Accordion title="Module 1: Why Distributed Systems?" icon="circle-question">
    **Duration**: 4-6 hours
    
    Understanding the fundamental reasons and challenges.
    
    - Single machine limitations (CPU, memory, disk, network)
    - Horizontal vs Vertical scaling trade-offs
    - The Eight Fallacies of Distributed Computing (deep dive)
    - Types: Compute clusters, Storage systems, Coordination systems
    - Real examples: Google's evolution from single machines to global infrastructure
    
    **Interview Focus**: Why not just use a bigger machine? Cost-benefit analysis
  </Accordion>
  
  <Accordion title="Module 2: Network Fundamentals" icon="network-wired">
    **Duration**: 6-8 hours
    
    Networking knowledge every distributed systems engineer needs.
    
    - TCP guarantees and failure modes
    - Network partitions: What they are and how to detect them
    - Message passing: At-most-once, at-least-once, exactly-once
    - RPC frameworks: gRPC, Thrift, Protocol Buffers
    - Failure detection: Heartbeats, Phi accrual detector
    
    **Interview Focus**: How do you detect if a node is dead vs slow?
  </Accordion>
  
  <Accordion title="Module 3: Time and Ordering" icon="clock">
    **Duration**: 8-10 hours
    
    **Critical Topic**: Time is the foundation of distributed systems reasoning.
    
    - Why physical clocks fail (NTP drift, leap seconds)
    - Logical clocks (Lamport timestamps)
    - Vector clocks (causality tracking)
    - Hybrid logical clocks (HLC)
    - TrueTime (Google's GPS + atomic clock approach)
    - Happens-before relationship
    
    **Interview Focus**: How does Google Spanner achieve external consistency?
  </Accordion>
  
  <Accordion title="Module 4: Failure Models" icon="triangle-exclamation">
    **Duration**: 4-6 hours
    
    Understanding what can go wrong is crucial for designing resilient systems.
    
    - Fail-stop vs Fail-recover
    - Byzantine failures
    - Network failures: Partition, delay, reordering
    - Partial failures: The hardest problem
    - Gray failures (subtle, hard-to-detect issues)
    
    **Interview Focus**: Design for partial failures in a payment system
  </Accordion>
  
  <Accordion title="Module 5: CAP and PACELC Theorems" icon="scale-balanced">
    **Duration**: 6-8 hours
    
    The fundamental trade-offs in distributed systems.
    
    - CAP Theorem: Proof and implications
    - CP vs AP: Real-world examples
    - PACELC: The more practical framework
    - Beyond CAP: Harvest and Yield
    - Consistency spectrum: Linearizable → Eventual
    
    **Interview Focus**: Is your system CP or AP? What are the trade-offs?
  </Accordion>
</AccordionGroup>

---

## Track 2: Consensus Protocols

The heart of distributed systems. Master these for Staff+ interviews.

<AccordionGroup>
  <Accordion title="Module 6: The Consensus Problem" icon="handshake">
    **Duration**: 4-6 hours
    
    Why consensus is hard and why it matters.
    
    - FLP Impossibility result (and its implications)
    - Safety vs Liveness guarantees
    - Consensus use cases: Leader election, configuration, transactions
    - Relationship to State Machine Replication
    
    **Interview Focus**: Can you achieve consensus in an asynchronous system?
  </Accordion>
  
  <Accordion title="Module 7: Paxos Protocol" icon="scroll">
    **Duration**: 10-12 hours
    
    **The Original**: Understanding Paxos is fundamental.
    
    - Basic Paxos: Prepare/Promise, Accept/Accepted
    - Why Paxos works (safety proofs)
    - Multi-Paxos optimizations
    - Paxos Made Simple (Lamport's paper walkthrough)
    - Fast Paxos and Flexible Paxos
    - EPaxos (Leaderless variant)
    
    **Interview Focus**: Walk through a Paxos round with failures
  </Accordion>
  
  <Accordion title="Module 8: Raft Consensus (Deep Dive)" icon="star">
    **Duration**: 12-14 hours
    
    **Most Asked**: Raft is the go-to consensus protocol in interviews.
    
    - Leader election mechanism
    - Log replication and commit rules
    - Safety properties and proofs
    - Membership changes (joint consensus)
    - Log compaction and snapshots
    - Raft vs Paxos comparison
    - etcd/Consul implementation details
    
    **Hands-On**: Implement Raft from scratch
    
    **Interview Focus**: What happens when the leader fails during commit?
  </Accordion>
  
  <Accordion title="Module 9: Viewstamped Replication" icon="eye">
    **Duration**: 4-6 hours
    
    An alternative view of consensus.
    
    - VR protocol overview
    - View changes and recovery
    - Comparison with Raft
    - When to use VR vs Raft
  </Accordion>
  
  <Accordion title="Module 10: ZAB (Zookeeper Atomic Broadcast)" icon="z">
    **Duration**: 6-8 hours
    
    How Zookeeper maintains distributed coordination.
    
    - ZAB protocol phases
    - Leader activation and synchronization
    - Recovery and failover
    - Zookeeper guarantees (FIFO, linearizable writes)
    - Zookeeper use cases: Locking, configuration, leader election
    
    **Interview Focus**: Design a distributed lock using Zookeeper
  </Accordion>
</AccordionGroup>

---

## Track 3: Replication Strategies

How data is copied and kept consistent across nodes.

<AccordionGroup>
  <Accordion title="Module 11: Single-Leader Replication" icon="user">
    **Duration**: 6-8 hours
    
    The simplest and most common replication strategy.
    
    - Synchronous vs Asynchronous replication
    - Semi-synchronous replication
    - Replication lag and its problems
    - Read-your-writes, Monotonic reads, Consistent prefix
    - Failover handling and split-brain prevention
    - MySQL/PostgreSQL replication internals
    
    **Interview Focus**: How do you handle replication lag in a user-facing feature?
  </Accordion>
  
  <Accordion title="Module 12: Multi-Leader Replication" icon="users">
    **Duration**: 8-10 hours
    
    When single-leader isn't enough.
    
    - Use cases: Multi-datacenter, offline clients
    - Conflict detection and resolution
    - Last-write-wins (LWW) and its problems
    - Custom conflict resolution logic
    - CockroachDB and TiDB approach
    
    **Interview Focus**: Design multi-region writes for a collaborative editor
  </Accordion>
  
  <Accordion title="Module 13: Leaderless Replication" icon="circle-nodes">
    **Duration**: 8-10 hours
    
    The Dynamo-style approach used by Cassandra, Riak, etc.
    
    - Read/write quorums (R + W > N)
    - Sloppy quorums and hinted handoff
    - Anti-entropy: Read repair, Merkle trees
    - Dynamo paper deep dive
    - Cassandra consistency levels
    
    **Interview Focus**: When would you choose leaderless over leader-based?
  </Accordion>
  
  <Accordion title="Module 14: Conflict Resolution" icon="code-merge">
    **Duration**: 6-8 hours
    
    When conflicts happen, how do you resolve them?
    
    - Application-level resolution
    - Version vectors
    - LWW strategies and pitfalls
    - Merge functions
    - Operational transformation (Google Docs)
    
    **Interview Focus**: Design conflict resolution for a shopping cart
  </Accordion>
  
  <Accordion title="Module 15: CRDTs" icon="puzzle-piece">
    **Duration**: 8-10 hours
    
    **Advanced Topic**: Conflict-free Replicated Data Types.
    
    - Operation-based vs State-based CRDTs
    - G-Counter, PN-Counter
    - G-Set, 2P-Set, OR-Set
    - LWW-Register, MV-Register
    - CRDT-based databases (Riak, Redis CRDT)
    - Performance and memory implications
    
    **Interview Focus**: Design a collaborative text editor using CRDTs
  </Accordion>
</AccordionGroup>

---

## Track 4: Distributed Transactions

Maintaining data integrity across multiple nodes.

<AccordionGroup>
  <Accordion title="Module 16: ACID in Distributed Systems" icon="flask">
    **Duration**: 6-8 hours
    
    How ACID properties translate to distributed environments.
    
    - Local vs Distributed transactions
    - Isolation levels: Read uncommitted → Serializable
    - Snapshot isolation and write skew
    - Serializable Snapshot Isolation (SSI)
    
    **Interview Focus**: What isolation level would you choose for a banking system?
  </Accordion>
  
  <Accordion title="Module 17: Two-Phase Commit (2PC)" icon="code-commit">
    **Duration**: 8-10 hours
    
    The classic distributed transaction protocol.
    
    - Prepare and Commit phases
    - Coordinator failures and blocking
    - Participant failures and recovery
    - 2PC in practice: XA transactions
    - Why 2PC is often avoided (performance, availability)
    
    **Interview Focus**: What happens if the coordinator crashes after prepare?
  </Accordion>
  
  <Accordion title="Module 18: Three-Phase Commit (3PC)" icon="code-compare">
    **Duration**: 4-6 hours
    
    Attempting to solve 2PC's blocking problem.
    
    - Pre-commit phase addition
    - Non-blocking under certain failures
    - Why 3PC isn't commonly used
    - Network partition problems
  </Accordion>
  
  <Accordion title="Module 19: Saga Pattern" icon="route">
    **Duration**: 8-10 hours
    
    **Production Favorite**: Long-running transactions without locks.
    
    - Choreography vs Orchestration
    - Compensating transactions
    - Semantic locks and countermeasures
    - Saga execution coordinator
    - Saga pattern in microservices
    - Temporal.io and Cadence workflows
    
    **Hands-On**: Implement an order saga with compensation
    
    **Interview Focus**: Design a travel booking saga with compensation logic
  </Accordion>
  
  <Accordion title="Module 20: TCC Pattern" icon="check-double">
    **Duration**: 4-6 hours
    
    Try-Confirm-Cancel for distributed transactions.
    
    - Two-phase approach at application level
    - Resource reservation
    - Timeout handling
    - When to use TCC vs Saga
    
    **Interview Focus**: TCC vs 2PC vs Saga - when to use each?
  </Accordion>
  
  <Accordion title="Module 21: Distributed Locking" icon="lock">
    **Duration**: 6-8 hours
    
    Coordinating access to shared resources.
    
    - Single-node locks in distributed systems (Redis SETNX)
    - Redlock algorithm and its critique
    - Fencing tokens for safety
    - Zookeeper-based locks
    - Lease-based locking
    
    **Interview Focus**: Design a distributed rate limiter with locks
  </Accordion>
</AccordionGroup>

---

## Track 5: Data Systems at Scale

Partitioning, storage, and processing at massive scale.

<AccordionGroup>
  <Accordion title="Module 22: Partitioning Strategies" icon="table-cells">
    **Duration**: 8-10 hours
    
    How to split data across nodes effectively.
    
    - Key-range partitioning
    - Hash partitioning
    - Hybrid approaches
    - Secondary indexes: Local vs Global
    - Rebalancing strategies
    - Hot spots and skew handling
    
    **Interview Focus**: How would you partition a social network's posts?
  </Accordion>
  
  <Accordion title="Module 23: Consistent Hashing" icon="circle">
    **Duration**: 6-8 hours
    
    The foundational algorithm for distributed systems.
    
    - Basic consistent hashing
    - Virtual nodes for load balancing
    - Bounded-load consistent hashing
    - Jump consistent hashing
    - Rendezvous hashing (HRW)
    
    **Hands-On**: Implement consistent hashing with virtual nodes
    
    **Interview Focus**: Design a distributed cache with consistent hashing
  </Accordion>
  
  <Accordion title="Module 24: Distributed Databases Deep Dive" icon="database">
    **Duration**: 12-14 hours
    
    How production databases work internally.
    
    - **Spanner**: TrueTime, external consistency, Paxos groups
    - **CockroachDB**: Raft, serializable isolation, SQL distribution
    - **TiDB**: Raft + Percolator, hybrid OLTP/OLAP
    - **Cassandra**: Gossip, consistent hashing, tunable consistency
    - **DynamoDB**: Leaderless, GSI, adaptive capacity
    - **MongoDB**: Raft-based replication, sharding
    
    **Interview Focus**: How does Spanner achieve global consistency?
  </Accordion>
  
  <Accordion title="Module 25: Distributed Storage Systems" icon="hard-drive">
    **Duration**: 8-10 hours
    
    Block and object storage at scale.
    
    - GFS/HDFS architecture
    - Object storage (S3 architecture)
    - Erasure coding for durability
    - Ceph architecture
    - Tiered storage strategies
    
    **Interview Focus**: Design a petabyte-scale storage system
  </Accordion>
  
  <Accordion title="Module 26: Stream Processing" icon="water">
    **Duration**: 10-12 hours
    
    Real-time data processing at scale.
    
    - Event sourcing and event-driven architecture
    - Kafka internals: Partitions, consumer groups, exactly-once
    - Stream processing: Flink, Kafka Streams
    - Windowing: Tumbling, Sliding, Session
    - Watermarks and late data handling
    - Exactly-once semantics in streaming
    
    **Interview Focus**: Design a real-time analytics pipeline
  </Accordion>
</AccordionGroup>

---

## Track 6: Production Excellence

Operating distributed systems at scale.

<AccordionGroup>
  <Accordion title="Module 27: Observability at Scale" icon="eye">
    **Duration**: 6-8 hours
    
    You can't fix what you can't see.
    
    - Distributed tracing (Jaeger, Zipkin, OpenTelemetry)
    - Metrics aggregation at scale
    - Log aggregation and analysis
    - Correlation across services
    - SLIs, SLOs, and error budgets
    
    **Interview Focus**: How do you debug a latency spike across 100 services?
  </Accordion>
  
  <Accordion title="Module 28: Chaos Engineering" icon="explosion">
    **Duration**: 6-8 hours
    
    Netflix-style reliability through controlled chaos.
    
    - Chaos Monkey and the Simian Army
    - Designing chaos experiments
    - Blast radius control
    - Failure injection frameworks (Litmus, Chaos Mesh)
    - Game days and runbooks
    
    **Interview Focus**: How would you test your system's resilience?
  </Accordion>
  
  <Accordion title="Module 29: SRE Practices" icon="helmet-safety">
    **Duration**: 6-8 hours
    
    Keeping systems running at scale.
    
    - Toil reduction and automation
    - On-call best practices
    - Postmortem culture (blameless)
    - Error budgets and release velocity
    - Progressive rollouts
    
    **Interview Focus**: Describe your approach to a 50% latency increase
  </Accordion>
  
  <Accordion title="Module 30: Incident Management" icon="siren">
    **Duration**: 4-6 hours
    
    When things go wrong at scale.
    
    - Incident response playbooks
    - Communication during outages
    - Escalation procedures
    - Root cause analysis
    - Learning from failures
  </Accordion>
  
  <Accordion title="Module 31: Capacity Planning" icon="chart-line">
    **Duration**: 6-8 hours
    
    Ensuring your system can handle growth.
    
    - Load testing strategies
    - Capacity modeling
    - Performance regression detection
    - Autoscaling strategies
    - Cost optimization at scale
    
    **Interview Focus**: How do you prepare for a 10x traffic spike?
  </Accordion>
</AccordionGroup>

---

## Capstone Projects

Apply everything you've learned.

<CardGroup cols={2}>
  <Card title="Project 1: Distributed KV Store" icon="box">
    Build a key-value store with:
    - Raft-based replication
    - Consistent hashing for partitioning
    - Read/write quorums
    - Snapshot and recovery
  </Card>
  <Card title="Project 2: Implement Raft" icon="star">
    A complete Raft implementation:
    - Leader election
    - Log replication
    - Membership changes
    - Persistence and recovery
  </Card>
  <Card title="Project 3: Distributed Lock Service" icon="lock">
    Build a coordination service:
    - Ephemeral nodes
    - Watch mechanism
    - Sequential ordering
    - Lock implementation
  </Card>
  <Card title="Project 4: Mock Interviews" icon="comments">
    Practice system design:
    - Design Uber's dispatch system
    - Design Stripe's payment processing
    - Design Netflix's CDN
    - Design Twitter's timeline
  </Card>
</CardGroup>

---

## Key Papers to Read

Essential reading for deep understanding:

| Paper | Why It Matters |
|-------|---------------|
| **Dynamo** (Amazon) | Leaderless replication, vector clocks, eventual consistency |
| **Spanner** (Google) | TrueTime, globally consistent transactions |
| **Raft** (Stanford) | Understandable consensus |
| **Paxos Made Simple** (Lamport) | The original consensus paper |
| **MapReduce** (Google) | Distributed computation paradigm |
| **Kafka** (LinkedIn) | Distributed log architecture |
| **Time, Clocks** (Lamport) | Logical time foundations |
| **CALM Theorem** | Consistency without coordination |
| **FLP Impossibility** | Limits of distributed consensus |
| **Harvest/Yield** | Practical CAP trade-offs |

---

## Interview Preparation Strategy

<Steps>
  <Step title="Master the Theory (Weeks 1-6)">
    Complete Tracks 1-3. Focus on:
    - CAP/PACELC intuition
    - Raft protocol (draw from memory)
    - Replication trade-offs
  </Step>
  <Step title="Go Deep on Transactions (Weeks 7-9)">
    Complete Track 4. Be ready to:
    - Design a saga for any use case
    - Explain 2PC failure modes
    - Discuss distributed locking trade-offs
  </Step>
  <Step title="Study Real Systems (Weeks 10-12)">
    Complete Track 5. Know:
    - How Spanner achieves global consistency
    - How Kafka provides exactly-once
    - When to use Cassandra vs PostgreSQL
  </Step>
  <Step title="Mock Interviews (Weeks 13-16)">
    - Practice 3-4 system design problems per week
    - Focus on distributed aspects
    - Record yourself and review
  </Step>
</Steps>

---

## Who This Course Is For

<Tabs>
  <Tab title="Target Audience">
    - **Senior Engineers** (4+ years) aiming for Staff/Principal
    - **Backend Engineers** wanting deep distributed systems knowledge
    - **Infrastructure Engineers** building platforms
    - **Anyone targeting FAANG/top-tier companies**
  </Tab>
  <Tab title="Prerequisites">
    - Strong programming in at least one language
    - Basic networking (TCP/IP, HTTP)
    - Database fundamentals (SQL, transactions)
    - Some system design exposure
  </Tab>
  <Tab title="Time Commitment">
    - **Full-time study**: 8-10 weeks
    - **Part-time (10-15 hrs/week)**: 14-18 weeks
    - **Each module**: 4-14 hours
    - **Projects**: 20-40 hours each
  </Tab>
</Tabs>

---

## Ready to Begin?

<Card title="Start with Track 1" icon="rocket" href="/courses/distributed-systems/foundations">
  Begin with the foundations module to build your mental model of distributed systems
</Card>
