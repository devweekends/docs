---
title: "Distributed Systems Mastery"
sidebarTitle: "Course Overview"
description: "Crack FAANG-level interviews with deep distributed systems expertise — Consensus, Raft, Paxos, and production patterns"
icon: "network-wired"
---

# Distributed Systems Mastery

A comprehensive, interview-focused curriculum designed for engineers targeting **Staff/Principal roles at top tech companies** (Google, Meta, Amazon, Netflix, Stripe, etc.). This course covers everything from fundamentals to cutting-edge distributed systems concepts.

<Info>
**Course Duration**: 18-24 weeks (self-paced)  
**Target Outcome**: Staff+ Engineer at FAANG / Top-tier distributed systems expertise  
**Prerequisites**: Strong programming, basic networking, database fundamentals  
**Language**: Concepts with implementations in Go/Java/Python  
**New Content**: 5 additional tracks with 45+ modules, real-world case studies, and Staff+ interview problems
</Info>

---

## Why This Course?

<CardGroup cols={2}>
  <Card title="FAANG Interview Ready" icon="building">
    Covers exact topics asked at Google, Amazon, Meta, and other top companies
  </Card>
  <Card title="Deep Theoretical Foundation" icon="book">
    Understand Raft, Paxos, ZAB, and other consensus protocols inside-out
  </Card>
  <Card title="Production Battle-Tested" icon="shield-halved">
    Patterns from systems handling millions of QPS at scale
  </Card>
  <Card title="Hands-On Projects" icon="laptop-code">
    Build your own distributed KV store, consensus implementation, and more
  </Card>
</CardGroup>

<Warning>
**Interview Reality**: At Staff+ level, you're expected to design systems that handle **billions of requests**, survive **data center failures**, and maintain **consistency guarantees**. This course prepares you for exactly that.
</Warning>

---

## What Companies Ask

| Company | Common Topics |
|---------|--------------|
| **Google** | Consensus protocols, Spanner, Bigtable internals, Paxos, distributed transactions |
| **Amazon** | DynamoDB internals, eventual consistency, vector clocks, Dynamo paper |
| **Meta** | TAO, ZippyDB, consensus at scale, social graph distribution |
| **Netflix** | EVCache, Cassandra, chaos engineering, resilience patterns |
| **Stripe** | Distributed transactions, exactly-once delivery, idempotency |
| **Uber** | Ringpop, consistent hashing, real-time systems, Cadence workflows |

---

## Course Structure

The curriculum is organized into **9 tracks** progressing from fundamentals to Staff+ expertise:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                   DISTRIBUTED SYSTEMS MASTERY                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  TRACK 1: CORE FOUNDATIONS        TRACK 2: CONSENSUS & COORDINATION         │
│  ─────────────────────────        ──────────────────────────────            │
│  □ Why Distributed?               □ The Consensus Problem                   │
│  □ Network Fundamentals           □ Paxos (Single & Multi)                  │
│  □ Time & Ordering                □ Raft (In-Depth)                         │
│  □ Consistency Models (NEW)       □ Byzantine Fault Tolerance (NEW)         │
│  □ Distributed Snapshots (NEW)    □ Gossip Protocols (NEW)                  │
│  □ CAP/PACELC Theorems            □ Formal Verification (TLA+) (NEW)        │
│                                   □ Leader Election & Locks                 │
│                                                                              │
│  TRACK 3: REPLICATION             TRACK 4: TRANSACTIONS                     │
│  ─────────────────────            ──────────────────────                    │
│  □ Single-Leader                  □ ACID in Distributed World               │
│  □ Multi-Leader                   □ 2PC and 3PC                             │
│  □ Leaderless                     □ Saga Pattern                            │
│  □ Conflict Resolution            □ TCC Pattern                             │
│  □ CRDTs                          □ Distributed Locking                     │
│                                                                              │
│  TRACK 5: DATA SYSTEMS            TRACK 6: MESSAGING & EVENTS               │
│  ─────────────────────            ──────────────────────────                │
│  □ Partitioning Strategies (NEW)  □ Kafka Deep Dive (NEW)                   │
│  □ Consistent Hashing             □ Event Sourcing & CQRS                   │
│  □ Distributed Databases          □ Message Queue Patterns                  │
│  □ Distributed Storage            □ Exactly-Once Semantics                  │
│  □ Stream Processing              □ Dead Letter Queues                      │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════│
│  ADVANCED PRODUCTION TRACKS                                                  │
│  ═══════════════════════════════════════════════════════════════════════════│
│                                                                              │
│  TRACK 7: CLOCK SYNCHRONIZATION   TRACK 8: FAULT TOLERANCE                  │
│  ───────────────────────────────  ────────────────────────                  │
│  □ TrueTime Deep Dive             □ Circuit Breaker Patterns                │
│  □ GPS & Atomic Clocks            □ Bulkhead Isolation                      │
│  □ Hybrid Logical Clocks          □ Retry Strategies                        │
│  □ Clock Synchronization          □ Timeout Patterns                        │
│  □ Spanner's Architecture         □ Graceful Degradation                    │
│                                                                              │
│  TRACK 9: DISTRIBUTED CACHING     TRACK 10: PRODUCTION & PRACTICE           │
│  ─────────────────────────────    ─────────────────────────────             │
│  □ Cache Strategies               □ Observability at Scale                  │
│  □ Cache Invalidation             □ Chaos Engineering                       │
│  □ Redis/Memcached Patterns       □ Real-World Case Studies                 │
│  □ CDN Caching                    □ Interview Practice Problems             │
│  □ Cache Stampede Prevention      □ Staff+ Level Problem Sets               │
│                                                                              │
│  CAPSTONE PROJECTS (Module 51)                                              │
│  ────────────────────────────                                              │
│  □ Build Distributed KV Store                                               │
│  □ Implement Raft Consensus                                                 │
│  □ Design Interview Practice                                                │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Track 1: Foundations

Build the mental models that all distributed systems are built upon.

<AccordionGroup>
  <Accordion title="Module 1: Why Distributed Systems?" icon="circle-question">
    **Duration**: 4-6 hours
    
    Understanding the fundamental reasons and challenges.
    
    - Single machine limitations (CPU, memory, disk, network)
    - Horizontal vs Vertical scaling trade-offs
    - The Eight Fallacies of Distributed Computing (deep dive)
    - Types: Compute clusters, Storage systems, Coordination systems
    - Real examples: Google's evolution from single machines to global infrastructure
    
    **Interview Focus**: Why not just use a bigger machine? Cost-benefit analysis
  </Accordion>
  
  <Accordion title="Module 2: Network Fundamentals" icon="network-wired">
    **Duration**: 6-8 hours
    
    Networking knowledge every distributed systems engineer needs.
    
    - TCP guarantees and failure modes
    - Network partitions: What they are and how to detect them
    - Message passing: At-most-once, at-least-once, exactly-once
    - RPC frameworks: gRPC, Thrift, Protocol Buffers
    - Failure detection: Heartbeats, Phi accrual detector
    
    **Interview Focus**: How do you detect if a node is dead vs slow?
  </Accordion>
  
  <Accordion title="Module 3: Time and Ordering" icon="clock">
    **Duration**: 8-10 hours
    
    **Critical Topic**: Time is the foundation of distributed systems reasoning.
    
    - Why physical clocks fail (NTP drift, leap seconds)
    - Logical clocks (Lamport timestamps)
    - Vector clocks (causality tracking)
    - Hybrid logical clocks (HLC)
    - TrueTime (Google's GPS + atomic clock approach)
    - Happens-before relationship
    
    **Interview Focus**: How does Google Spanner achieve external consistency?
  </Accordion>
  
  <Accordion title="Module 4: Failure Models" icon="triangle-exclamation">
    **Duration**: 4-6 hours
    
    Understanding what can go wrong is crucial for designing resilient systems.
    
    - Fail-stop vs Fail-recover
    - Byzantine failures
    - Network failures: Partition, delay, reordering
    - Partial failures: The hardest problem
    - Gray failures (subtle, hard-to-detect issues)
    
    **Interview Focus**: Design for partial failures in a payment system
  </Accordion>
  
  <Accordion title="Module 5: CAP and PACELC Theorems" icon="scale-balanced">
    **Duration**: 6-8 hours
    
    The fundamental trade-offs in distributed systems.
    
    - CAP Theorem: Proof and implications
    - CP vs AP: Real-world examples
    - PACELC: The more practical framework
    - Beyond CAP: Harvest and Yield
    - Consistency spectrum: Linearizable → Eventual
    
    **Interview Focus**: Is your system CP or AP? What are the trade-offs?
  </Accordion>

  <Accordion title="Module 47: Distributed Snapshots" icon="camera">
    **Duration**: 6-8 hours
    
    Capturing a consistent global state in a distributed system.
    
    - The Global State Problem: Why we can't just "freeze" time
    - Chandy-Lamport Algorithm: Markers and state recording
    - Consistent vs Inconsistent cuts
    - Practical uses: Distributed debugging, Checkpointing, Termination detection
    
    **Interview Focus**: How do you take a backup of a distributed database without stopping writes?
  </Accordion>

  <Accordion title="Module 48: Gossip Protocols" icon="comments">
    **Duration**: 6-8 hours
    
    Decentralized communication and membership.
    
    - Epidemic algorithms: Rumor mongering and anti-entropy
    - SWIM: Scalable Weakly-consistent Infection-style Process Group Membership
    - Phi Accrual Failure Detection: Suspicion-based detectors
    - Use cases: Cassandra membership, Redis Cluster, HashiCorp Serf/Consul
    
    **Interview Focus**: How does a 1000-node cluster detect a single node failure without a central master?
  </Accordion>
</AccordionGroup>

---

## Track 2: Consensus Protocols

The heart of distributed systems. Master these for Staff+ interviews.

<AccordionGroup>
  <Accordion title="Module 6: The Consensus Problem" icon="handshake">
    **Duration**: 4-6 hours
    
    Why consensus is hard and why it matters.
    
    - FLP Impossibility result (and its implications)
    - Safety vs Liveness guarantees
    - Consensus use cases: Leader election, configuration, transactions
    - Relationship to State Machine Replication
    
    **Interview Focus**: Can you achieve consensus in an asynchronous system?
  </Accordion>
  
  <Accordion title="Module 7: Paxos Protocol" icon="scroll">
    **Duration**: 10-12 hours
    
    **The Original**: Understanding Paxos is fundamental.
    
    - Basic Paxos: Prepare/Promise, Accept/Accepted
    - Why Paxos works (safety proofs)
    - Multi-Paxos optimizations
    - Paxos Made Simple (Lamport's paper walkthrough)
    - Fast Paxos and Flexible Paxos
    - EPaxos (Leaderless variant)
    
    **Interview Focus**: Walk through a Paxos round with failures
  </Accordion>
  
  <Accordion title="Module 8: Raft Consensus (Deep Dive)" icon="star">
    **Duration**: 12-14 hours
    
    **Most Asked**: Raft is the go-to consensus protocol in interviews.
    
    - Leader election mechanism
    - Log replication and commit rules
    - Safety properties and proofs
    - Membership changes (joint consensus)
    - Log compaction and snapshots
    - Raft vs Paxos comparison
    - etcd/Consul implementation details
    
    **Hands-On**: Implement Raft from scratch
    
    **Interview Focus**: What happens when the leader fails during commit?
  </Accordion>
  
  <Accordion title="Module 9: Viewstamped Replication" icon="eye">
    **Duration**: 4-6 hours
    
    An alternative view of consensus.
    
    - VR protocol overview
    - View changes and recovery
    - Comparison with Raft
    - When to use VR vs Raft
  </Accordion>
  
  <Accordion title="Module 10: ZAB (Zookeeper Atomic Broadcast)" icon="z">
    **Duration**: 6-8 hours
    
    How Zookeeper maintains distributed coordination.
    
    - ZAB protocol phases
    - Leader activation and synchronization
    - Recovery and failover
    - Zookeeper guarantees (FIFO, linearizable writes)
    - Zookeeper use cases: Locking, configuration, leader election
    
    **Interview Focus**: Design a distributed lock using Zookeeper
  </Accordion>

  <Accordion title="Module 49: Byzantine Fault Tolerance (BFT)" icon="shield-virus">
    **Duration**: 8-10 hours
    
    Handling malicious nodes and arbitrary failures.
    
    - The Byzantine Generals Problem: Understanding the consensus bound (3f+1)
    - PBFT: Practical Byzantine Fault Tolerance internals
    - Modern BFT: Tendermint, HotStuff (used in Libra/Diem)
    - Proof of Work vs BFT: Performance and safety trade-offs
    
    **Interview Focus**: How do you achieve consensus when some nodes might be actively lying or malicious?
  </Accordion>

  <Accordion title="Module 50: Formal Verification (TLA+)" icon="shield-check">
    **Duration**: 8-10 hours
    
    Proving correctness before writing code.
    
    - Why testing isn't enough: State space explosion
    - TLA+ and PlusCal basics
    - Modeling safety (invariants) vs liveness (progress)
    - The TLC Model Checker and error traces
    - Case studies: How AWS and MongoDB use TLA+
    
    **Interview Focus**: How do you prove that your custom consensus protocol is correct?
  </Accordion>
</AccordionGroup>

---

## Track 3: Replication Strategies

How data is copied and kept consistent across nodes.

<AccordionGroup>
  <Accordion title="Module 11: Single-Leader Replication" icon="user">
    **Duration**: 6-8 hours
    
    The simplest and most common replication strategy.
    
    - Synchronous vs Asynchronous replication
    - Semi-synchronous replication
    - Replication lag and its problems
    - Read-your-writes, Monotonic reads, Consistent prefix
    - Failover handling and split-brain prevention
    - MySQL/PostgreSQL replication internals
    
    **Interview Focus**: How do you handle replication lag in a user-facing feature?
  </Accordion>
  
  <Accordion title="Module 12: Multi-Leader Replication" icon="users">
    **Duration**: 8-10 hours
    
    When single-leader isn't enough.
    
    - Use cases: Multi-datacenter, offline clients
    - Conflict detection and resolution
    - Last-write-wins (LWW) and its problems
    - Custom conflict resolution logic
    - CockroachDB and TiDB approach
    
    **Interview Focus**: Design multi-region writes for a collaborative editor
  </Accordion>
  
  <Accordion title="Module 13: Leaderless Replication" icon="circle-nodes">
    **Duration**: 8-10 hours
    
    The Dynamo-style approach used by Cassandra, Riak, etc.
    
    - Read/write quorums (R + W > N)
    - Sloppy quorums and hinted handoff
    - Anti-entropy: Read repair, Merkle trees
    - Dynamo paper deep dive
    - Cassandra consistency levels
    
    **Interview Focus**: When would you choose leaderless over leader-based?
  </Accordion>
  
  <Accordion title="Module 14: Conflict Resolution" icon="code-merge">
    **Duration**: 6-8 hours
    
    When conflicts happen, how do you resolve them?
    
    - Application-level resolution
    - Version vectors
    - LWW strategies and pitfalls
    - Merge functions
    - Operational transformation (Google Docs)
    
    **Interview Focus**: Design conflict resolution for a shopping cart
  </Accordion>
  
  <Accordion title="Module 15: CRDTs" icon="puzzle-piece">
    **Duration**: 8-10 hours
    
    **Advanced Topic**: Conflict-free Replicated Data Types.
    
    - Operation-based vs State-based CRDTs
    - G-Counter, PN-Counter
    - G-Set, 2P-Set, OR-Set
    - LWW-Register, MV-Register
    - CRDT-based databases (Riak, Redis CRDT)
    - Performance and memory implications
    
    **Interview Focus**: Design a collaborative text editor using CRDTs
  </Accordion>
</AccordionGroup>

---

## Track 4: Distributed Transactions

Maintaining data integrity across multiple nodes.

<AccordionGroup>
  <Accordion title="Module 16: ACID in Distributed Systems" icon="flask">
    **Duration**: 6-8 hours
    
    How ACID properties translate to distributed environments.
    
    - Local vs Distributed transactions
    - Isolation levels: Read uncommitted → Serializable
    - Snapshot isolation and write skew
    - Serializable Snapshot Isolation (SSI)
    
    **Interview Focus**: What isolation level would you choose for a banking system?
  </Accordion>
  
  <Accordion title="Module 17: Two-Phase Commit (2PC)" icon="code-commit">
    **Duration**: 8-10 hours
    
    The classic distributed transaction protocol.
    
    - Prepare and Commit phases
    - Coordinator failures and blocking
    - Participant failures and recovery
    - 2PC in practice: XA transactions
    - Why 2PC is often avoided (performance, availability)
    
    **Interview Focus**: What happens if the coordinator crashes after prepare?
  </Accordion>
  
  <Accordion title="Module 18: Three-Phase Commit (3PC)" icon="code-compare">
    **Duration**: 4-6 hours
    
    Attempting to solve 2PC's blocking problem.
    
    - Pre-commit phase addition
    - Non-blocking under certain failures
    - Why 3PC isn't commonly used
    - Network partition problems
  </Accordion>
  
  <Accordion title="Module 19: Saga Pattern" icon="route">
    **Duration**: 8-10 hours
    
    **Production Favorite**: Long-running transactions without locks.
    
    - Choreography vs Orchestration
    - Compensating transactions
    - Semantic locks and countermeasures
    - Saga execution coordinator
    - Saga pattern in microservices
    - Temporal.io and Cadence workflows
    
    **Hands-On**: Implement an order saga with compensation
    
    **Interview Focus**: Design a travel booking saga with compensation logic
  </Accordion>
  
  <Accordion title="Module 20: TCC Pattern" icon="check-double">
    **Duration**: 4-6 hours
    
    Try-Confirm-Cancel for distributed transactions.
    
    - Two-phase approach at application level
    - Resource reservation
    - Timeout handling
    - When to use TCC vs Saga
    
    **Interview Focus**: TCC vs 2PC vs Saga - when to use each?
  </Accordion>
  
  <Accordion title="Module 21: Distributed Locking" icon="lock">
    **Duration**: 6-8 hours
    
    Coordinating access to shared resources.
    
    - Single-node locks in distributed systems (Redis SETNX)
    - Redlock algorithm and its critique
    - Fencing tokens for safety
    - Zookeeper-based locks
    - Lease-based locking
    
    **Interview Focus**: Design a distributed rate limiter with locks
  </Accordion>
</AccordionGroup>

---

## Track 5: Data Systems at Scale

Partitioning, storage, and processing at massive scale.

<AccordionGroup>
  <Accordion title="Module 22: Partitioning Strategies" icon="table-cells">
    **Duration**: 8-10 hours
    
    How to split data across nodes effectively.
    
    - Key-range partitioning
    - Hash partitioning
    - Hybrid approaches
    - Secondary indexes: Local vs Global
    - Rebalancing strategies
    - Hot spots and skew handling
    
    **Interview Focus**: How would you partition a social network's posts?
  </Accordion>
  
  <Accordion title="Module 23: Consistent Hashing" icon="circle">
    **Duration**: 6-8 hours
    
    The foundational algorithm for distributed systems.
    
    - Basic consistent hashing
    - Virtual nodes for load balancing
    - Bounded-load consistent hashing
    - Jump consistent hashing
    - Rendezvous hashing (HRW)
    
    **Hands-On**: Implement consistent hashing with virtual nodes
    
    **Interview Focus**: Design a distributed cache with consistent hashing
  </Accordion>
  
  <Accordion title="Module 24: Distributed Databases Deep Dive" icon="database">
    **Duration**: 12-14 hours
    
    How production databases work internally.
    
    - **Spanner**: TrueTime, external consistency, Paxos groups
    - **CockroachDB**: Raft, serializable isolation, SQL distribution
    - **TiDB**: Raft + Percolator, hybrid OLTP/OLAP
    - **Cassandra**: Gossip, consistent hashing, tunable consistency
    - **DynamoDB**: Leaderless, GSI, adaptive capacity
    - **MongoDB**: Raft-based replication, sharding
    
    **Interview Focus**: How does Spanner achieve global consistency?
  </Accordion>
  
  <Accordion title="Module 25: Distributed Storage Systems" icon="hard-drive">
    **Duration**: 8-10 hours
    
    Block and object storage at scale.
    
    - GFS/HDFS architecture
    - Object storage (S3 architecture)
    - Erasure coding for durability
    - Ceph architecture
    - Tiered storage strategies
    
    **Interview Focus**: Design a petabyte-scale storage system
  </Accordion>
  
  <Accordion title="Module 26: Stream Processing" icon="water">
    **Duration**: 10-12 hours
    
    Real-time data processing at scale.
    
    - Event sourcing and event-driven architecture
    - Kafka internals: Partitions, consumer groups, exactly-once
    - Stream processing: Flink, Kafka Streams
    - Windowing: Tumbling, Sliding, Session
    - Watermarks and late data handling
    - Exactly-once semantics in streaming
    
    **Interview Focus**: Design a real-time analytics pipeline
  </Accordion>
</AccordionGroup>

---

## Track 6: Production Excellence

Operating distributed systems at scale.

<AccordionGroup>
  <Accordion title="Module 27: Observability at Scale" icon="eye">
    **Duration**: 6-8 hours
    
    You can't fix what you can't see.
    
    - Distributed tracing (Jaeger, Zipkin, OpenTelemetry)
    - Metrics aggregation at scale
    - Log aggregation and analysis
    - Correlation across services
    - SLIs, SLOs, and error budgets
    
    **Interview Focus**: How do you debug a latency spike across 100 services?
  </Accordion>
  
  <Accordion title="Module 28: Chaos Engineering" icon="explosion">
    **Duration**: 6-8 hours
    
    Netflix-style reliability through controlled chaos.
    
    - Chaos Monkey and the Simian Army
    - Designing chaos experiments
    - Blast radius control
    - Failure injection frameworks (Litmus, Chaos Mesh)
    - Game days and runbooks
    
    **Interview Focus**: How would you test your system's resilience?
  </Accordion>
  
  <Accordion title="Module 29: SRE Practices" icon="helmet-safety">
    **Duration**: 6-8 hours
    
    Keeping systems running at scale.
    
    - Toil reduction and automation
    - On-call best practices
    - Postmortem culture (blameless)
    - Error budgets and release velocity
    - Progressive rollouts
    
    **Interview Focus**: Describe your approach to a 50% latency increase
  </Accordion>

  <Accordion title="Module 30: Advanced Resiliency Patterns" icon="shield-check">
    **Duration**: 6-8 hours
    
    Designing systems that are inherently resistant to failure.
    
    - Static Stability and over-provisioning
    - Cell-based Architectures and blast radius control
    - Dependency Isolation (The Bulkhead Pattern)
    - Avoiding control-plane dependencies in recovery paths
    
    **Interview Focus**: How do you design a system that survives an AZ failure without autoscaling?
  </Accordion>
  
  <Accordion title="Module 31: Incident Management" icon="siren">
    **Duration**: 4-6 hours
    
    When things go wrong at scale.
    
    - Incident response playbooks
    - Communication during outages
    - Escalation procedures
    - Root cause analysis
    - Learning from failures
  </Accordion>
  
  <Accordion title="Module 32: Capacity Planning" icon="chart-line">
    **Duration**: 6-8 hours
    
    Ensuring your system can handle growth.
    
    - Load testing strategies
    - Capacity modeling
    - Performance regression detection
    - Autoscaling strategies
    - Cost optimization at scale
    
    **Interview Focus**: How do you prepare for a 10x traffic spike?
  </Accordion>
</AccordionGroup>

---

## Track 7: Clock Synchronization (Advanced)

Time is the foundation of distributed systems. Master clock synchronization for Staff+ expertise.

<AccordionGroup>
  <Accordion title="Module 33: Clock Synchronization Protocols" icon="network-wired">
    **Duration**: 6-8 hours
    
    How clocks stay synchronized across networks.
    
    - NTP architecture and stratum levels
    - PTP/IEEE 1588 for microsecond precision
    - Clock drift detection and correction
    - Network asymmetry compensation
    - Monitoring clock health in production
    
    **Interview Focus**: How do you detect and handle clock skew in your system?
  </Accordion>

  <Accordion title="Module 34: Logical and Vector Clocks" icon="arrows-split-up-and-left">
    **Duration**: 8-10 hours
    
    Capturing causality without physical time.
    
    - Lamport timestamps and happened-before relationship
    - Vector clocks for precise conflict detection
    - Comparison rules and concurrency proofs
    - Implementation in DynamoDB and Riak
    
    **Interview Focus**: When would you use vector clocks vs logical clocks?
  </Accordion>

  <Accordion title="Module 35: Hybrid Logical Clocks" icon="arrows-rotate">
    **Duration**: 8-10 hours
    
    Combining physical and logical time for practical systems.
    
    - HLC design and implementation
    - Timestamp encoding strategies
    - CockroachDB's MVCC with HLC
    - Causality tracking with bounded skew
    - HLC vs Vector Clocks trade-offs
    
    **Hands-On**: Implement HLC for a distributed database
    
    **Interview Focus**: Why choose HLC over pure logical clocks?
  </Accordion>

  <Accordion title="Module 36: TrueTime and Atomic Clocks" icon="satellite">
    **Duration**: 10-12 hours
    
    **Critical Topic**: Google Spanner's revolutionary approach to time.
    
    - GPS time transfer and accuracy bounds
    - Atomic clock drift characteristics (Rubidium vs Cesium)
    - TrueTime API: `TT.now()`, `TT.after()`, `TT.before()`
    - Uncertainty intervals and commit-wait protocol
    - How Spanner uses TrueTime for external consistency
    
    **Interview Focus**: Walk through how Spanner commits a transaction using TrueTime
  </Accordion>
</AccordionGroup>

---

## Track 8: Fault Tolerance Patterns

Building resilient systems that survive failures.

<AccordionGroup>
  <Accordion title="Module 37: Circuit Breaker Pattern" icon="circle-stop">
    **Duration**: 8-10 hours
    
    **Production Essential**: Prevent cascade failures.
    
    - State machine: Closed → Open → Half-Open
    - Failure threshold configuration
    - Timeout and retry integration
    - Hystrix and Resilience4j implementations
    - Monitoring circuit breaker health
    
    **Hands-On**: Implement a circuit breaker with state transitions
    
    **Interview Focus**: Design circuit breakers for a payment gateway
  </Accordion>
  
  <Accordion title="Module 38: Bulkhead Isolation" icon="box">
    **Duration**: 6-8 hours
    
    Contain failures to prevent system-wide outages.
    
    - Thread pool isolation patterns
    - Semaphore-based bulkheads
    - Connection pool partitioning
    - Process-level isolation
    - Kubernetes resource limits as bulkheads
    
    **Interview Focus**: How do you prevent one slow service from affecting others?
  </Accordion>
  
  <Accordion title="Module 39: Retry Strategies" icon="rotate">
    **Duration**: 6-8 hours
    
    When and how to retry failed operations.
    
    - Exponential backoff with jitter
    - Retry budgets and thundering herd prevention
    - Idempotency keys for safe retries
    - Deadline propagation across services
    - Distinguishing transient vs permanent failures
    
    **Interview Focus**: Design a retry strategy for a distributed task queue
  </Accordion>
  
  <Accordion title="Module 40: Graceful Degradation" icon="stairs">
    **Duration**: 8-10 hours
    
    Maintaining partial functionality during failures.
    
    - Feature flags for degradation
    - Fallback strategies and stale data serving
    - Load shedding and admission control
    - Quality-of-service tiering
    - Netflix's fallback hierarchies
    
    **Interview Focus**: How would you degrade an e-commerce site during database issues?
  </Accordion>
  
  <Accordion title="Module 41: Timeout Patterns" icon="hourglass">
    **Duration**: 4-6 hours
    
    The most important but often misunderstood pattern.
    
    - Connection vs read vs write timeouts
    - Timeout cascades and deadline propagation
    - Context cancellation across service boundaries
    - Calculating appropriate timeout values
    - Timeout vs circuit breaker interaction
    
    **Interview Focus**: How do you set timeouts for a microservices call chain?
  </Accordion>
</AccordionGroup>

---

## Track 9: Distributed Caching

Caching patterns for high-performance distributed systems.

<AccordionGroup>
  <Accordion title="Module 42: Cache Strategies" icon="layer-group">
    **Duration**: 8-10 hours
    
    Choosing the right caching pattern for your use case.
    
    - Cache-aside (lazy loading) pattern
    - Read-through and write-through caching
    - Write-behind (write-back) caching
    - Refresh-ahead pattern
    - Cache eviction policies (LRU, LFU, TTL)
    
    **Interview Focus**: When would you choose write-behind over write-through?
  </Accordion>
  
  <Accordion title="Module 43: Cache Invalidation" icon="trash-can">
    **Duration**: 8-10 hours
    
    **The Hard Problem**: Keeping caches consistent.
    
    - TTL-based invalidation strategies
    - Event-driven invalidation with Kafka/pub-sub
    - Tag-based cache invalidation
    - Cascading invalidation patterns
    - Cache versioning strategies
    
    **Interview Focus**: Design cache invalidation for a product catalog
  </Accordion>
  
  <Accordion title="Module 44: Redis & Memcached Architecture" icon="database">
    **Duration**: 10-12 hours
    
    Production-grade distributed cache implementations.
    
    - Redis Cluster architecture and slot migration
    - Redis Sentinel for high availability
    - Memcached consistent hashing
    - Memory management and eviction
    - Replication lag and read consistency
    - Redis vs Memcached decision framework
    
    **Interview Focus**: Design a distributed session store using Redis
  </Accordion>
  
  <Accordion title="Module 45: CDN Caching" icon="globe">
    **Duration**: 6-8 hours
    
    Caching at the edge for global performance.
    
    - CDN architecture and PoP design
    - Cache-Control header strategies
    - Origin shield and tiered caching
    - Cache purging at scale
    - Dynamic content caching patterns
    
    **Interview Focus**: Design CDN caching for a video streaming platform
  </Accordion>
  
  <Accordion title="Module 46: Cache Stampede Prevention" icon="shield">
    **Duration**: 6-8 hours
    
    Preventing thundering herd on cache misses.
    
    - Locking and mutex patterns
    - Probabilistic early expiration
    - Request coalescing
    - Background refresh strategies
    - Circuit breaker integration
    
    **Interview Focus**: How do you handle cache stampede during Black Friday?
  </Accordion>
</AccordionGroup>

---

## Special Track: Real-World Case Studies

Learn from production systems at scale.

<AccordionGroup>
  <Accordion title="Google Spanner Architecture" icon="google">
    **Duration**: 8-10 hours
    
    The first globally distributed, strongly consistent database.
    
    - TrueTime and external consistency
    - Paxos groups and data placement
    - Lock-free read-only transactions
    - Schema changes without downtime
    - Real failure stories and lessons learned
    
    **Interview Focus**: How does Spanner achieve 5 nines availability?
  </Accordion>
  
  <Accordion title="Amazon Dynamo & DynamoDB" icon="amazon">
    **Duration**: 8-10 hours
    
    The paper that launched NoSQL.
    
    - Consistent hashing with virtual nodes
    - Vector clocks and conflict resolution
    - Sloppy quorums and hinted handoff
    - Evolution from Dynamo to DynamoDB
    - Global Tables and cross-region replication
    
    **Interview Focus**: Design a shopping cart using Dynamo-style storage
  </Accordion>
  
  <Accordion title="Netflix Resilience Architecture" icon="film">
    **Duration**: 6-8 hours
    
    Chaos engineering pioneers.
    
    - Chaos Monkey and Simian Army
    - EVCache and caching at scale
    - Zuul gateway and load shedding
    - Failure injection testing
    - Multi-region active-active deployment
    
    **Interview Focus**: Design a chaos engineering strategy for your system
  </Accordion>
  
  <Accordion title="Uber's Real-Time Systems" icon="car">
    **Duration**: 6-8 hours
    
    Building reliable systems for millions of rides.
    
    - Ringpop for membership and routing
    - Cadence/Temporal workflow orchestration
    - Geospatial indexing at scale
    - Real-time dispatch and matching
    - Multi-region failover strategies
    
    **Interview Focus**: Design a ride-matching system like Uber
  </Accordion>
</AccordionGroup>

---

## Staff+ Interview Practice Problems

Curated problems for senior-level interviews.

<CardGroup cols={2}>
  <Card title="Global Rate Limiter" icon="gauge-high" href="/courses/distributed-systems/interview-problems">
    Design a rate limiter that works across multiple data centers with sub-millisecond overhead
  </Card>
  <Card title="Distributed Transaction Coordinator" icon="arrows-turn-to-dots" href="/courses/distributed-systems/interview-problems">
    Build a transaction coordinator supporting 2PC, Saga, and TCC patterns
  </Card>
  <Card title="Real-Time Leaderboard" icon="ranking-star" href="/courses/distributed-systems/interview-problems">
    Design a leaderboard supporting millions of concurrent players with real-time updates
  </Card>
  <Card title="Multi-Region Database" icon="earth-americas" href="/courses/distributed-systems/interview-problems">
    Design a database with strong consistency guarantees across continents
  </Card>
</CardGroup>

<Info>
**Interview Tip**: Each problem includes detailed solutions, trade-off analysis, and follow-up questions commonly asked at Google, Meta, Amazon, and other top companies.
</Info>

---

## Capstone Projects

Apply everything you've learned.

<CardGroup cols={2}>
  <Card title="Project 1: Distributed KV Store" icon="box">
    Build a key-value store with:
    - Raft-based replication
    - Consistent hashing for partitioning
    - Read/write quorums
    - Snapshot and recovery
  </Card>
  <Card title="Project 2: Implement Raft" icon="star">
    A complete Raft implementation:
    - Leader election
    - Log replication
    - Membership changes
    - Persistence and recovery
  </Card>
  <Card title="Project 3: Distributed Lock Service" icon="lock">
    Build a coordination service:
    - Ephemeral nodes
    - Watch mechanism
    - Sequential ordering
    - Lock implementation
  </Card>
  <Card title="Project 4: Mock Interviews" icon="comments">
    Practice system design:
    - Design Uber's dispatch system
    - Design Stripe's payment processing
    - Design Netflix's CDN
    - Design Twitter's timeline
  </Card>
</CardGroup>

---

## Key Papers to Read

Essential reading for deep understanding:

| Paper | Why It Matters |
|-------|---------------|
| **Dynamo** (Amazon) | Leaderless replication, vector clocks, eventual consistency |
| **Spanner** (Google) | TrueTime, globally consistent transactions |
| **Raft** (Stanford) | Understandable consensus |
| **Paxos Made Simple** (Lamport) | The original consensus paper |
| **MapReduce** (Google) | Distributed computation paradigm |
| **Kafka** (LinkedIn) | Distributed log architecture |
| **Time, Clocks** (Lamport) | Logical time foundations |
| **CALM Theorem** | Consistency without coordination |
| **FLP Impossibility** | Limits of distributed consensus |
| **Harvest/Yield** | Practical CAP trade-offs |

---

## Interview Preparation Strategy

<Steps>
  <Step title="Master the Theory (Weeks 1-6)">
    Complete Tracks 1-3. Focus on:
    - CAP/PACELC intuition
    - Raft protocol (draw from memory)
    - Replication trade-offs
  </Step>
  <Step title="Go Deep on Transactions (Weeks 7-9)">
    Complete Track 4. Be ready to:
    - Design a saga for any use case
    - Explain 2PC failure modes
    - Discuss distributed locking trade-offs
  </Step>
  <Step title="Study Real Systems (Weeks 10-12)">
    Complete Track 5. Know:
    - How Spanner achieves global consistency
    - How Kafka provides exactly-once
    - When to use Cassandra vs PostgreSQL
  </Step>
  <Step title="Master Advanced Patterns (Weeks 13-15)">
    Complete Tracks 7-9 (New!). Focus on:
    - TrueTime and clock synchronization
    - Circuit breakers and fault tolerance
    - Distributed caching patterns
  </Step>
  <Step title="Study Case Studies (Week 16)">
    Review real-world architectures:
    - Google Spanner, Amazon Dynamo
    - Netflix resilience, Uber real-time
    - Learn from actual failure post-mortems
  </Step>
  <Step title="Mock Interviews (Weeks 17-20)">
    - Practice 3-4 system design problems per week
    - Use the Staff+ Interview Problems module
    - Focus on distributed aspects
    - Record yourself and review
  </Step>
</Steps>

---

## Who This Course Is For

<Tabs>
  <Tab title="Target Audience">
    - **Senior Engineers** (4+ years) aiming for Staff/Principal
    - **Backend Engineers** wanting deep distributed systems knowledge
    - **Infrastructure Engineers** building platforms
    - **Anyone targeting FAANG/top-tier companies**
  </Tab>
  <Tab title="Prerequisites">
    - Strong programming in at least one language
    - Basic networking (TCP/IP, HTTP)
    - Database fundamentals (SQL, transactions)
    - Some system design exposure
  </Tab>
  <Tab title="Time Commitment">
    - **Full-time study**: 10-14 weeks
    - **Part-time (10-15 hrs/week)**: 18-24 weeks
    - **Each module**: 4-14 hours
    - **Projects**: 20-40 hours each
    - **New tracks (7-9)**: Additional 6-8 weeks
  </Tab>
</Tabs>

---

## Ready to Begin?

<CardGroup cols={2}>
  <Card title="Start with Foundations" icon="rocket" href="/courses/distributed-systems/foundations">
    Begin with Track 1 to build your mental model of distributed systems
  </Card>
  <Card title="Jump to Case Studies" icon="building" href="/courses/distributed-systems/case-studies">
    Learn from Google, Amazon, Netflix, and Uber's production systems
  </Card>
  <Card title="Practice Interview Problems" icon="code" href="/courses/distributed-systems/interview-problems">
    Staff+ level problems with detailed solutions and trade-off analysis
  </Card>
  <Card title="Master Fault Tolerance" icon="shield" href="/courses/distributed-systems/fault-tolerance">
    Circuit breakers, retries, and graceful degradation patterns
  </Card>
</CardGroup>
