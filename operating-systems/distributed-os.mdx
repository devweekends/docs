---
title: "Distributed Operating Systems"
sidebarTitle: "Distributed OS"
description: "From-scratch introduction to distributed operating systems: RPC, naming, DSM, DFS, clocks, and consistency, with interview-focused depth"
icon: "network-wired"
---

# Distributed Operating Systems: The Architecture of Scale

A Distributed Operating System (DOS) is the evolution of the OS into the realm of the network. It aims to manage a collection of independent, networked, and communicating nodes as a single, coherent computing resource. While pure DOS (like Amoeba or Plan 9) are rare today, the principles they pioneered underpin every cloud platform and large-scale web application.

This module provides a senior-level exploration of distributed coordination, consensus, and the mathematical limits of networked systems.

<Info>
**Interview Frequency**: Extremely High (System Design crossover)  
**Key Topics**: Paxos/Raft, CAP Theorem, FLP Impossibility, Vector Clocks, Consistent Hashing  
**Time to Master**: 20-25 hours
</Info>

---

## 1. The Mathematical Boundaries

In a distributed system, we lose the luxury of a "Global Truth" (no shared clock, no shared memory).

### 1.1. The FLP Impossibility
The **Fisher, Lynch, Paterson (1985)** theorem is the most important result in distributed computing.
- **The Result**: In an asynchronous network where even **one** process can fail by stopping, it is impossible to reach **Consensus** (agreement) with 100% certainty.
- **Why?**: You cannot distinguish between a process that has crashed and a process that is just very slow.
- **The Workaround**: Real systems use **Timeouts** or **Randomization** to bypass FLP.

### 1.2. The CAP Theorem Revisited
Every distributed data store must choose two out of three:
1.  **Consistency**: All nodes see the same data at the same time.
2.  **Availability**: Every request gets a response (success or failure).
3.  **Partition Tolerance**: The system works despite network drops.
**The Reality**: You *must* choose Partition Tolerance ($P$). So the real choice is between $CP$ (PostgreSQL, Zookeeper) and $AP$ (Cassandra, DynamoDB).

---

## 2. Time, Ordering, and Causality

Without a shared clock, we cannot say "Event A happened at 10:00:01 and B at 10:00:02." We must use **Logical Clocks**.

### 2.1. Lamport Clocks ($L$)
A simple integer that increments on every local event and is attached to every message.
- **Rule**: $L(receive) = \max(L_{local}, L_{incoming}) + 1$.
- **Limitation**: If $L(A) < L(B)$, we don't know if $A \to B$. They might be independent.

### 2.2. Vector Clocks ($V$)
To capture **Concurrence**, each node keeps a vector of clocks for *all* nodes.
- **Conflict Detection**: If $V_A$ has a higher value for Node 1, but $V_B$ has a higher value for Node 2, the events are **Concurrent** (a write conflict).

### 2.3. Google Spanner and TrueTime
Spanner uses specialized hardware (GPS + Atomic Clocks) to provide a "Confidence Interval" $[\epsilon_{low}, \epsilon_{high}]$ for time.
- **Commit Wait**: The system waits until the current time is guaranteed to be past the commit time of the previous transaction.
- **Result**: Global linearizability (Strict Consistency) at scale.

---

## 3. Consensus: The Heart of Coordination

Consensus is the process of getting $N$ nodes to agree on a single value (e.g., "Who is the leader?").

### 3.1. Paxos (The Academic Foundation)
Designed by Leslie Lamport. It is notoriously difficult to understand but mathematically robust.
- **Phases**: Prepare (Ask for permission), Propose (Send value), Accept (Value committed).
- **Quorums**: You only need agreement from a majority ($N/2 + 1$) to proceed.

### 3.2. Raft (The Practical Standard)
Designed to be "understandable."
1.  **Leader Election**: One node is elected leader.
2.  **Log Replication**: Clients send writes to the leader; the leader replicates them to followers.
3.  **Safety**: A follower only votes for a candidate whose log is at least as up-to-date as its own.

---

## 4. Naming and Discovery: Consistent Hashing

How do you find which of 10,000 servers holds the key `"user:123"`?

### 4.1. The Hash Ring
1.  Hash all server IDs into a $2^{160}$ (SHA-1) space (a circle).
2.  Hash the key into the same space.
3.  The key belongs to the **first server encountered clockwise**.
- **The Benefit**: When a server joins or leaves, you only move $1/N$ of the keys. Standard modulo hashing ($Hash(key) \% N$) would require moving *every* key.

---

## 5. Distributed Transactions: 2PC vs. Sagas

### 5.1. Two-Phase Commit (2PC)
1.  **Prepare**: Coordinator asks all participants "Can you commit?"
2.  **Commit**: If all say "Yes," the coordinator sends the "Commit" command.
- **The Flaw**: It is a **Blocking Protocol**. If the coordinator dies after Step 1, all participants are stuck holding locks forever.

### 5.2. Sagas (The Modern Microservices Way)
Instead of one big transaction, break it into small, independent transactions.
- **Compensating Actions**: If Step 3 fails, the system executes "Undo" actions for Step 1 and 2 (e.g., "Refund money" instead of "Rollback database").

---

## 6. Resource Scheduling: Borg and Mesos

A Distributed OS must schedule tasks across the cluster.

### 6.1. Two-Level Scheduling (Mesos)
- **Level 1**: Mesos offers raw resources (CPU/RAM) to frameworks (Spark, Kubernetes).
- **Level 2**: The framework decides which specific tasks to run on those resources.

### 6.2. Dominant Resource Fairness (DRF)
In a cluster with CPU and RAM, how do you divide fairly?
- **Logic**: A user's "share" is defined by their **Dominant Resource** (the one they use the most of). DRF ensures everyone's dominant share is equalized.

---

## 7. Senior Interview Deep Dive

<AccordionGroup>
  <Accordion title="Q1: Why do we use Quorums ($N/2+1$) instead of requiring all nodes to agree?">
    If we required all nodes to agree, the system would stop working if even **one** node crashed (Zero Availability). 
    
    By using a Majority Quorum, the system can tolerate up to $f$ failures in a system of $2f+1$ nodes. This provides a balance between Consistency (two majorities must overlap by at least one node) and Availability.
  </Accordion>

  <Accordion title="Q2: Explain the 'Split Brain' problem and how to prevent it.">
    Split Brain occurs when a network partition divides a cluster into two halves, and both halves think the other half is dead. Both might then elect a leader and start accepting conflicting writes.
    
    **Prevention**: Use **Quorums**. Only the half that has a majority of the nodes is allowed to elect a leader and proceed. The smaller half must stop accepting writes.
  </Accordion>

  <Accordion title="Q3: What is 'Gossip' and why is it used for failure detection?">
    Gossip (Epidemic) protocols involve nodes periodically sending their state to a random subset of other nodes. 
    
    **Advantage**: It is extremely scalable ($O(\log N)$ convergence) and robust. There is no single point of failure (like a master monitor). If a node stops gossiping, its neighbors will eventually mark it as dead and spread the word.
  </Accordion>
</AccordionGroup>

---

## 8. Practice: Building a Distributed Lock

**Challenge**: Use **etcd** or **Redis** (with Redlock) to implement a distributed lock that handles process crashes and network partitions.

```python
# Conceptual pseudocode using etcd (CP system)
def acquire_lock(lock_name, lease_ttl=10):
    lease = etcd.lease(ttl=lease_ttl)
    # Attempt to put key IF it doesn't exist
    success = etcd.put(lock_name, "active", lease=lease, if_not_exists=True)
    if success:
        # Start a keep-alive thread to heart-beat the lease
        lease.keep_alive()
        return True
    return False
```

---

Next: [Containers & Virtualization](/operating-systems/containers-virtualization) â†’
