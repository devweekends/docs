---
title: "I/O Systems"
sidebarTitle: "I/O Systems"
description: "Device drivers, DMA, I/O scheduling, and modern interfaces like io_uring"
icon: "hard-drive"
---

# I/O Systems

The **I/O subsystem** bridges the gap between software and hardware devices. Understanding I/O is crucial for performance optimization and is frequently tested in systems interviews.

<Info>
**Interview Frequency**: Medium-High  
**Key Topics**: DMA, interrupt handling, I/O scheduling, io_uring  
**Time to Master**: 8-10 hours
</Info>

---

## I/O Hardware Basics

### Device Controllers

```
┌─────────────────────────────────────────────────────────────────┐
│                    I/O HARDWARE ARCHITECTURE                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  CPU                                                            │
│   │                                                             │
│   ▼                                                             │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                     System Bus                          │   │
│  └───────────┬─────────────────┬─────────────────┬─────────┘   │
│              │                 │                 │              │
│              ▼                 ▼                 ▼              │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐       │
│  │ Memory        │  │ Disk          │  │ Network       │       │
│  │ Controller    │  │ Controller    │  │ Controller    │       │
│  │               │  │               │  │               │       │
│  │ ┌──────────┐ │  │ ┌──────────┐ │  │ ┌──────────┐ │       │
│  │ │ Status   │ │  │ │ Status   │ │  │ │ Status   │ │       │
│  │ │ Command  │ │  │ │ Command  │ │  │ │ Command  │ │       │
│  │ │ Data     │ │  │ │ Data     │ │  │ │ Data     │ │       │
│  │ └──────────┘ │  │ └──────────┘ │  │ └──────────┘ │       │
│  └───────┬──────┘  └───────┬──────┘  └───────┬──────┘       │
│          │                 │                 │              │
│          ▼                 ▼                 ▼              │
│       [RAM]           [Disk Drive]       [NIC]              │
│                                                              │
└─────────────────────────────────────────────────────────────────┘
```

### Device Registers

Every device controller has registers accessible by the CPU:

| Register | Purpose |
|----------|---------|
| **Status** | Device state (busy, error, ready) |
| **Command** | What operation to perform |
| **Data In** | Data from device to CPU |
| **Data Out** | Data from CPU to device |

### I/O Addressing

<Tabs>
  <Tab title="Port-Mapped I/O">
    Special CPU instructions for I/O:
    ```asm
    ; x86 port I/O
    IN  AL, 0x60    ; Read from keyboard port
    OUT 0x60, AL    ; Write to keyboard port
    ```
    - Separate I/O address space
    - Requires special instructions
    - Common on x86
  </Tab>
  <Tab title="Memory-Mapped I/O">
    Device registers mapped to memory addresses:
    ```c
    // Device at physical address 0xFFFF0000
    volatile uint32_t *device_reg = (uint32_t *)0xFFFF0000;
    *device_reg = COMMAND;  // Write command
    status = *device_reg;   // Read status
    ```
    - Same instructions as memory access
    - Can use caching (must be careful)
    - Common on ARM, modern x86
  </Tab>
</Tabs>

---

## I/O Methods

### Programmed I/O (Polling)

CPU actively polls device status:

```c
void write_data_polling(char *buffer, int size) {
    for (int i = 0; i < size; i++) {
        // Wait until device is ready
        while ((inb(STATUS_PORT) & READY_BIT) == 0) {
            // Busy wait (wastes CPU)
        }
        
        // Send one byte
        outb(DATA_PORT, buffer[i]);
    }
}
```

**Pros**: Simple, no interrupt overhead  
**Cons**: Wastes CPU cycles, doesn't scale

### Interrupt-Driven I/O

Device signals CPU when ready:

```c
// Set up interrupt handler
void disk_interrupt_handler(void) {
    // Read completed data
    char data = inb(DATA_PORT);
    
    // Process data
    buffer[buffer_index++] = data;
    
    // Acknowledge interrupt
    outb(COMMAND_PORT, ACK);
    
    // Wake up waiting process
    wakeup(waiting_process);
}

// Initiate read
void start_read(void) {
    outb(COMMAND_PORT, READ_CMD);
    // Process sleeps, CPU does other work
    sleep(current_process);
}
```

**Pros**: CPU free during I/O  
**Cons**: Interrupt overhead for each transfer

### Direct Memory Access (DMA)

Device transfers directly to/from memory:

```
┌─────────────────────────────────────────────────────────────────┐
│                    DMA TRANSFER                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. CPU sets up DMA:                                            │
│     - Source/destination address                                │
│     - Transfer count                                            │
│     - Direction (read/write)                                    │
│                                                                  │
│  ┌───────┐                                                      │
│  │  CPU  │──────────────────┐                                  │
│  └───┬───┘                  │                                  │
│      │ "set up"             │ CPU continues                    │
│      ▼                      │ other work                       │
│  ┌───────────┐              │                                  │
│  │ DMA       │              │                                  │
│  │Controller │              │                                  │
│  └───────────┘              │                                  │
│      ↑↓ bus master          │                                  │
│  ┌───────────────────────────────┐                            │
│  │         System Bus            │                            │
│  └───────────────────────────────┘                            │
│      ↑                      ↑                                  │
│      │                      │                                  │
│  ┌───────┐              ┌───────┐                             │
│  │Device │◄────────────►│Memory │                             │
│  └───────┘  direct      └───────┘                             │
│             transfer                                           │
│                                                                 │
│  2. When complete: DMA controller sends interrupt              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Comparison

| Method | CPU Usage | Throughput | Use Case |
|--------|-----------|------------|----------|
| Polling | 100% during I/O | Low | Simple devices |
| Interrupt | Context switch per transfer | Medium | Keyboard, serial |
| DMA | Minimal | High | Disk, network |

---

## I/O Scheduling

### Disk I/O Scheduling

Traditional HDDs have mechanical seek time, making scheduling important:

```
┌─────────────────────────────────────────────────────────────────┐
│                    I/O SCHEDULING ALGORITHMS                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Request queue: [98, 183, 37, 122, 14, 124, 65, 67]            │
│  Current head position: 53                                      │
│                                                                  │
│  FCFS (First Come First Served):                               │
│  53 → 98 → 183 → 37 → 122 → 14 → 124 → 65 → 67                │
│  Total movement: 640 tracks                                     │
│                                                                  │
│  SSTF (Shortest Seek Time First):                              │
│  53 → 65 → 67 → 37 → 14 → 98 → 122 → 124 → 183               │
│  Total movement: 236 tracks                                     │
│  Problem: Starvation of far requests                           │
│                                                                  │
│  SCAN (Elevator):                                               │
│  53 → 37 → 14 → [0] → 65 → 67 → 98 → 122 → 124 → 183         │
│  Total movement: 236 tracks                                     │
│  Move in one direction until end, then reverse                 │
│                                                                  │
│  C-SCAN (Circular SCAN):                                        │
│  53 → 65 → 67 → 98 → 122 → 124 → 183 → [199] → [0] → 14 → 37 │
│  Always scan in same direction, jump back                      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Linux I/O Schedulers

```bash
# Check current scheduler
cat /sys/block/sda/queue/scheduler

# Available schedulers (kernel 5.x+)
# [mq-deadline] kyber bfq none
```

| Scheduler | Description | Best For |
|-----------|-------------|----------|
| **none** | No scheduling, FIFO | NVMe, VMs |
| **mq-deadline** | Deadline-based, prevents starvation | HDDs, latency-sensitive |
| **bfq** | Budget Fair Queuing | Desktop, interactive |
| **kyber** | Token-based, low overhead | Fast SSDs |

---

## Buffering and Caching

### Buffer Types

```
┌─────────────────────────────────────────────────────────────────┐
│                    I/O BUFFERING                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  No Buffering:                                                  │
│  ┌───────┐    ┌──────────┐                                     │
│  │Process│◄───│  Device  │  Direct transfer, process blocks   │
│  └───────┘    └──────────┘                                     │
│                                                                  │
│  Single Buffering:                                              │
│  ┌───────┐    ┌────────┐    ┌──────────┐                       │
│  │Process│◄───│ Buffer │◄───│  Device  │                       │
│  └───────┘    └────────┘    └──────────┘                       │
│  Process reads from buffer while device fills next              │
│                                                                  │
│  Double Buffering:                                              │
│  ┌───────┐    ┌────────┐    ┌──────────┐                       │
│  │Process│◄───│Buffer A│◄───│  Device  │                       │
│  │       │    └────────┘    │          │                       │
│  │       │◄───┌────────┐◄───│          │                       │
│  └───────┘    │Buffer B│    └──────────┘                       │
│               └────────┘                                        │
│  Process uses A while device fills B, then swap                │
│                                                                  │
│  Circular Buffering:                                            │
│  Multiple buffers in ring for high-throughput streaming        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Page Cache

The Linux page cache buffers disk I/O:

```c
// Read operation flow
ssize_t read_with_cache(int fd, void *buf, size_t count) {
    // 1. Check page cache for requested pages
    // 2. If cache hit: copy from cache to user buffer
    // 3. If cache miss: 
    //    a. Allocate page frame
    //    b. Read from disk into page frame
    //    c. Add to page cache
    //    d. Copy to user buffer
    // 4. Return bytes read
}

// Bypass cache with O_DIRECT
int fd = open("data", O_RDONLY | O_DIRECT);
// Reads go directly to/from device
// Used by databases for their own caching
```

---

## Modern I/O: io_uring

Linux's revolutionary async I/O interface (kernel 5.1+):

### Traditional async I/O Problems

```c
// Old approach: one syscall per operation
for (int i = 0; i < 1000; i++) {
    read(fds[i], bufs[i], sizes[i]);  // 1000 syscalls!
}

// aio_* functions: complex, limited
// epoll: only notification, still need syscall to read
```

### io_uring Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    IO_URING ARCHITECTURE                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│     User Space                    Kernel Space                  │
│                                                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              Shared Memory (mmap'd)                     │   │
│  │                                                          │   │
│  │  ┌──────────────────────────────────────────────────┐   │   │
│  │  │          Submission Queue (SQ)                   │   │   │
│  │  │   ┌─────┬─────┬─────┬─────┬─────┐               │   │   │
│  │  │   │ SQE │ SQE │ SQE │     │     │ ← Add entries │   │   │
│  │  │   └─────┴─────┴─────┴─────┴─────┘               │   │   │
│  │  │   head ──────────────────────────► tail          │   │   │
│  │  └──────────────────────────────────────────────────┘   │   │
│  │                          │                              │   │
│  │                          │ io_uring_enter()             │   │
│  │                          │ (submit + wait, optional)   │   │
│  │                          ▼                              │   │
│  │  ┌──────────────────────────────────────────────────┐   │   │
│  │  │          Completion Queue (CQ)                   │   │   │
│  │  │   ┌─────┬─────┬─────┬─────┬─────┐               │   │   │
│  │  │   │ CQE │ CQE │     │     │     │ ← Poll these  │   │   │
│  │  │   └─────┴─────┴─────┴─────┴─────┘               │   │   │
│  │  │   head ────────────────────► tail                │   │   │
│  │  └──────────────────────────────────────────────────┘   │   │
│  │                                                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                  │
│  Zero syscalls for submission in SQPOLL mode!                  │
│  Batch multiple operations per syscall                          │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### io_uring Example

```c
#include <liburing.h>

void async_reads(const char *files[], int count) {
    struct io_uring ring;
    io_uring_queue_init(256, &ring, 0);
    
    // Submit all reads
    for (int i = 0; i < count; i++) {
        int fd = open(files[i], O_RDONLY);
        struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);
        
        io_uring_prep_read(sqe, fd, buffers[i], BUF_SIZE, 0);
        sqe->user_data = i;  // Track which request
    }
    
    // Single syscall submits all
    io_uring_submit(&ring);
    
    // Reap completions
    for (int i = 0; i < count; i++) {
        struct io_uring_cqe *cqe;
        io_uring_wait_cqe(&ring, &cqe);
        
        int result = cqe->res;
        int index = cqe->user_data;
        
        // Process result...
        
        io_uring_cqe_seen(&ring, cqe);
    }
    
    io_uring_queue_exit(&ring);
}
```

### io_uring Benefits

| Feature | Benefit |
|---------|---------|
| **Batching** | Submit 1000s of ops with one syscall |
| **Shared memory** | Zero-copy submission/completion |
| **SQPOLL mode** | Kernel polls SQ, zero syscalls |
| **Link chains** | Dependencies between operations |
| **Fixed files/buffers** | Register once, use forever |

---

## Device Drivers

### Driver Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    DRIVER ARCHITECTURE                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  User Space    │   system calls (read, write, ioctl)           │
│  ──────────────│───────────────────────────────────────────────│
│                ▼                                                │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    VFS Layer                            │   │
│  │        (Virtual File System abstraction)                │   │
│  └───────────────────────────┬─────────────────────────────┘   │
│                              ▼                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              Device Driver (Kernel Module)              │   │
│  │                                                          │   │
│  │   struct file_operations {                              │   │
│  │       .open    = my_open,                               │   │
│  │       .read    = my_read,                               │   │
│  │       .write   = my_write,                              │   │
│  │       .ioctl   = my_ioctl,                              │   │
│  │       .release = my_close,                              │   │
│  │   };                                                     │   │
│  │                                                          │   │
│  │   - Interrupt handlers                                   │   │
│  │   - DMA setup                                            │   │
│  │   - Device initialization                                │   │
│  └───────────────────────────┬─────────────────────────────┘   │
│                              ▼                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                   Hardware Device                       │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Character vs Block Devices

| Aspect | Character Device | Block Device |
|--------|-----------------|--------------|
| Access | Byte stream | Fixed-size blocks |
| Buffering | No kernel buffering | Uses buffer cache |
| Random access | Not supported | Supported |
| Examples | Keyboard, serial port | Disk, SSD |
| Major/Minor | Yes | Yes |

```bash
# Check device types
ls -la /dev/
crw-rw----  1 root tty  4,  0  # c = character (tty0)
brw-rw----  1 root disk 8,  0  # b = block (sda)
```

---

## Interview Deep Dive Questions

<AccordionGroup>
  <Accordion title="Q1: Explain DMA and its advantages over PIO">
    **Answer:**
    
    **PIO (Programmed I/O)**:
    ```
    for each byte:
      CPU: check device ready
      CPU: read byte from device register
      CPU: write byte to memory
    ```
    - CPU executes every transfer instruction
    - CPU utilization: 100% during transfer
    - Good for: Small transfers, simple devices
    
    **DMA (Direct Memory Access)**:
    ```
    CPU: configure DMA controller
        - source address (device)
        - destination address (memory)
        - byte count
        - direction
    CPU: start transfer
    CPU: ...do other work...
    DMA: transfers data directly (bus master)
    DMA: interrupt CPU when complete
    ```
    
    **Advantages of DMA**:
    1. **CPU efficiency**: CPU free during transfer
    2. **Higher throughput**: DMA controller optimized for transfers
    3. **Lower latency**: No per-byte CPU overhead
    
    **DMA considerations**:
    - Needs contiguous physical memory (or scatter-gather DMA)
    - Cache coherency: CPU cache may have stale data
    - Setup overhead: Small transfers may be slower than PIO
  </Accordion>
  
  <Accordion title="Q2: Why is io_uring faster than traditional async I/O?">
    **Answer:**
    
    **Traditional read() syscall**:
    ```
    1. User calls read()
    2. Trap to kernel (mode switch)
    3. Kernel validates parameters
    4. Kernel initiates I/O
    5. Kernel copies data to user buffer
    6. Return to user (mode switch)
    
    Cost: 2 mode switches per operation
    ```
    
    **io_uring**:
    ```
    1. User writes SQE to shared memory (no syscall)
    2. User updates tail pointer (no syscall)
    3. [If needed] io_uring_enter() to notify kernel
    4. Kernel processes submission queue
    5. Kernel writes CQE to shared memory
    6. User reads CQE (no syscall)
    
    Cost: 0-1 syscall per BATCH of operations
    ```
    
    **Speed advantages**:
    
    1. **Batching**: 1 syscall for 1000 operations
    2. **SQPOLL mode**: Kernel thread polls SQ, zero user syscalls
    3. **Zero-copy**: Shared memory rings, no copy in/out
    4. **Registered buffers**: Pre-registered, skip validation
    5. **Registered files**: File descriptors pre-validated
    
    **Benchmarks**: 10-100x better IOPS for small I/O workloads
  </Accordion>
  
  <Accordion title="Q3: How does an interrupt work from hardware to user process?">
    **Answer:**
    
    ```
    1. HARDWARE: Device asserts interrupt line
       └─► Interrupt controller (APIC) receives signal
    
    2. INTERRUPT CONTROLLER:
       └─► Determines interrupt vector (IRQ number)
       └─► Signals CPU
    
    3. CPU:
       └─► Finishes current instruction
       └─► Saves context (registers, flags) to stack
       └─► Disables interrupts (for this level)
       └─► Looks up handler in IDT (Interrupt Descriptor Table)
       └─► Jumps to handler address
    
    4. KERNEL INTERRUPT HANDLER (Top Half):
       └─► Acknowledge interrupt to device
       └─► Quick processing (clear device state)
       └─► Schedule bottom half for heavy work
       └─► Return from interrupt (iret)
    
    5. KERNEL BOTTOM HALF (softirq/tasklet/workqueue):
       └─► Process data
       └─► Copy to user buffer
       └─► Wake up waiting process
    
    6. SCHEDULER:
       └─► User process becomes runnable
       └─► Eventually scheduled
    
    7. USER PROCESS:
       └─► read() returns with data
    ```
    
    **Top half vs Bottom half**:
    - Top half: Runs with interrupts disabled, must be fast
    - Bottom half: Can sleep, do heavy processing
    
    **Modern optimizations**:
    - NAPI (networking): Disable interrupts, switch to polling under load
    - Threaded IRQs: Handler runs in kernel thread context
  </Accordion>
  
  <Accordion title="Q4: Design a high-performance logging system">
    **Answer:**
    
    **Requirements**:
    - Write-heavy (100K+ logs/sec)
    - Durability (survive crashes)
    - Low latency (don't block application)
    
    **Design**:
    
    ```
    Application Threads
           │
           ▼
    ┌─────────────────────────────────────┐
    │     Lock-Free Ring Buffer           │
    │  (one per thread, thread-local)     │
    │  [log][log][log][...][log]          │
    └────────────────┬────────────────────┘
                     │
                     ▼
    ┌─────────────────────────────────────┐
    │        Aggregator Thread            │
    │  - Collects from all ring buffers   │
    │  - Batches logs                      │
    │  - Compresses (optional)            │
    └────────────────┬────────────────────┘
                     │
                     ▼
    ┌─────────────────────────────────────┐
    │        Async Writer Thread          │
    │  - Uses io_uring for batched writes │
    │  - O_APPEND for atomicity           │
    │  - fdatasync() periodically         │
    └─────────────────────────────────────┘
    ```
    
    **Key optimizations**:
    
    1. **Thread-local buffers**: No contention between threads
    2. **Ring buffers**: Fixed memory, no allocation
    3. **Batching**: Combine small logs into larger writes
    4. **io_uring**: Async, batched I/O submission
    5. **O_APPEND**: Kernel handles concurrent appends atomically
    6. **Periodic fsync**: Balance durability vs performance
    
    **Durability levels**:
    ```c
    // Level 1: OS buffer only (fastest, least durable)
    write(fd, log, len);
    
    // Level 2: fdatasync (data only, not metadata)
    write(fd, log, len);
    fdatasync(fd);  // Every N writes
    
    // Level 3: fsync (data + metadata)
    write(fd, log, len);
    fsync(fd);  // Slowest but safest
    ```
  </Accordion>
  
  <Accordion title="Q5: Explain O_DIRECT and when to use it">
    **Answer:**
    
    **Normal I/O path**:
    ```
    Application ←→ Page Cache ←→ Disk
    
    read():
    1. Check page cache
    2. If miss, read from disk to cache
    3. Copy from cache to user buffer
    
    write():
    1. Copy to page cache
    2. Mark page dirty
    3. Background writeback to disk
    ```
    
    **O_DIRECT I/O path**:
    ```
    Application ←→ Disk (bypass page cache)
    
    read()/write():
    1. Transfer directly between user buffer and disk
    2. No caching, no copy
    ```
    
    **Requirements for O_DIRECT**:
    - Buffer must be aligned (typically 512 bytes or 4KB)
    - Offset must be aligned
    - Size must be aligned
    
    **When to use O_DIRECT**:
    
    ✅ **Good use cases**:
    - Databases (manage own buffer pool)
    - Video streaming (no reread, waste cache)
    - Large sequential scans (would pollute cache)
    - When you need predictable latency
    
    ❌ **Bad use cases**:
    - General file access
    - Small random reads (cache helps)
    - When kernel caching is beneficial
    
    **Example** (PostgreSQL):
    ```c
    // PG uses shared_buffers (its own cache)
    // Can enable O_DIRECT to avoid double-caching
    fd = open(filename, O_RDWR | O_DIRECT);
    
    // Must use aligned buffer
    posix_memalign(&buf, 4096, 8192);
    pread(fd, buf, 8192, 0);
    ```
  </Accordion>
</AccordionGroup>

---

## Practice Exercises

<Steps>
  <Step title="I/O Scheduler Comparison">
    Use `fio` to benchmark random vs sequential I/O with different schedulers.
  </Step>
  <Step title="io_uring Program">
    Write a file copy program using io_uring. Compare performance with cp.
  </Step>
  <Step title="DMA Simulation">
    Implement a simulation of DMA controller behavior with interrupt generation.
  </Step>
  <Step title="Simple Driver">
    Write a character device driver that logs all read/write operations.
  </Step>
</Steps>

---

## Key Takeaways

<CardGroup cols={2}>
  <Card title="DMA Frees the CPU" icon="microchip">
    Device transfers data directly. CPU only sets up and handles completion.
  </Card>
  <Card title="io_uring is Revolutionary" icon="bolt">
    Batched, zero-copy async I/O. Essential for high-performance systems.
  </Card>
  <Card title="Buffering Matters" icon="layer-group">
    Double buffering enables overlapping I/O and processing.
  </Card>
  <Card title="Scheduler Choice Matters" icon="clock">
    Use none/kyber for NVMe, mq-deadline for HDDs.
  </Card>
</CardGroup>

---

Next: [Synchronization](/operating-systems/synchronization) →
