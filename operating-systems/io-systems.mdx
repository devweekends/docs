---
title: "I/O Systems"
sidebarTitle: "I/O Systems"
description: "Device drivers, DMA, I/O scheduling, and modern interfaces like io_uring"
icon: "hard-drive"
---

# I/O Systems

The **I/O subsystem** bridges the gap between software and hardware devices. Understanding I/O is crucial for performance optimization and is frequently tested in systems interviews.

<Info>
**Interview Frequency**: Medium-High  
**Key Topics**: DMA, interrupt handling, I/O scheduling, io_uring  
**Time to Master**: 8-10 hours
</Info>

---

## I/O Hardware Basics

### Device Controllers

```
┌─────────────────────────────────────────────────────────────────┐
│                    I/O HARDWARE ARCHITECTURE                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  CPU                                                            │
│   │                                                             │
│   ▼                                                             │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                     System Bus                          │   │
│  └───────────┬─────────────────┬─────────────────┬─────────┘   │
│              │                 │                 │              │
│              ▼                 ▼                 ▼              │
│  ┌───────────────┐  ┌───────────────┐  ┌───────────────┐       │
│  │ Memory        │  │ Disk          │  │ Network       │       │
│  │ Controller    │  │ Controller    │  │ Controller    │       │
│  │               │  │               │  │               │       │
│  │ ┌──────────┐ │  │ ┌──────────┐ │  │ ┌──────────┐ │       │
│  │ │ Status   │ │  │ │ Status   │ │  │ │ Status   │ │       │
│  │ │ Command  │ │  │ │ Command  │ │  │ │ Command  │ │       │
│  │ │ Data     │ │  │ │ Data     │ │  │ │ Data     │ │       │
│  │ └──────────┘ │  │ └──────────┘ │  │ └──────────┘ │       │
│  └───────┬──────┘  └───────┬──────┘  └───────┬──────┘       │
│          │                 │                 │              │
│          ▼                 ▼                 ▼              │
│       [RAM]           [Disk Drive]       [NIC]              │
│                                                              │
└─────────────────────────────────────────────────────────────────┘
```

### Device Registers

Every device controller has registers accessible by the CPU:

| Register | Purpose |
|----------|---------|
| **Status** | Device state (busy, error, ready) |
| **Command** | What operation to perform |
| **Data In** | Data from device to CPU |
| **Data Out** | Data from CPU to device |

### I/O Addressing

<Tabs>
  <Tab title="Port-Mapped I/O">
    Special CPU instructions for I/O:
    ```asm
    ; x86 port I/O
    IN  AL, 0x60    ; Read from keyboard port
    OUT 0x60, AL    ; Write to keyboard port
    ```
    - Separate I/O address space
    - Requires special instructions
    - Common on x86
  </Tab>
  <Tab title="Memory-Mapped I/O">
    Device registers mapped to memory addresses:
    ```c
    // Device at physical address 0xFFFF0000
    volatile uint32_t *device_reg = (uint32_t *)0xFFFF0000;
    *device_reg = COMMAND;  // Write command
    status = *device_reg;   // Read status
    ```
    - Same instructions as memory access
    - Can use caching (must be careful)
    - Common on ARM, modern x86
  </Tab>
</Tabs>

---

## I/O Methods

### Programmed I/O (Polling)

CPU actively polls device status:

```c
void write_data_polling(char *buffer, int size) {
    for (int i = 0; i < size; i++) {
        // Wait until device is ready
        while ((inb(STATUS_PORT) & READY_BIT) == 0) {
            // Busy wait (wastes CPU)
        }
        
        // Send one byte
        outb(DATA_PORT, buffer[i]);
    }
}
```

**Pros**: Simple, no interrupt overhead  
**Cons**: Wastes CPU cycles, doesn't scale

### Interrupt-Driven I/O

Device signals CPU when ready:

```c
// Set up interrupt handler
void disk_interrupt_handler(void) {
    // Read completed data
    char data = inb(DATA_PORT);
    
    // Process data
    buffer[buffer_index++] = data;
    
    // Acknowledge interrupt
    outb(COMMAND_PORT, ACK);
    
    // Wake up waiting process
    wakeup(waiting_process);
}

// Initiate read
void start_read(void) {
    outb(COMMAND_PORT, READ_CMD);
    // Process sleeps, CPU does other work
    sleep(current_process);
}
```

**Pros**: CPU free during I/O  
**Cons**: Interrupt overhead for each transfer

### Direct Memory Access (DMA)

DMA allows devices to transfer data directly to/from memory without CPU involvement for each byte. This is crucial for high-performance I/O operations.

```
┌─────────────────────────────────────────────────────────────────┐
│                    DMA ARCHITECTURE                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │                      CPU Complex                         │   │
│  │  ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐                │   │
│  │  │Core 0│  │Core 1│  │Core 2│  │Core 3│                │   │
│  │  └──┬───┘  └──┬───┘  └──┬───┘  └──┬───┘                │   │
│  │     └──────────┴─────────┴─────────┘                    │   │
│  │                    │                                     │   │
│  │              ┌─────┴─────┐                               │   │
│  │              │  L3 Cache │                               │   │
│  │              └─────┬─────┘                               │   │
│  └────────────────────┼──────────────────────────────────────┘   │
│                       │                                          │
│       ┌───────────────┴────────────────────────┐                 │
│       │         Memory Bus / Interconnect      │                 │
│       │         (PCIe, QPI, Infinity Fabric)   │                 │
│       └───┬─────────┬──────────┬──────────┬───┘                 │
│           │         │          │          │                      │
│    ┌──────▼───┐ ┌───▼────┐ ┌──▼────┐  ┌──▼────────┐            │
│    │   RAM    │ │  DMA   │ │ IOMMU │  │  Devices  │            │
│    │ (16 GB)  │ │ Ctrlr  │ │       │  │  (PCIe)   │            │
│    └──────────┘ └───┬────┘ └───────┘  └──┬────────┘            │
│                     │                     │                      │
│                     └─────────────────────┘                      │
│                     Direct path (bus master)                     │
│                                                                  │
│  Key Components:                                                │
│  ━━━━━━━━━━━━━━━                                                │
│  • DMA Controller: Dedicated hardware for memory transfers      │
│  • Bus Mastering: DMA can control system bus                    │
│  • IOMMU: Provides virtual addressing for DMA (security)        │
│  • Devices: Network cards, SSDs, GPUs use DMA heavily           │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### How DMA Works: Step-by-Step

```
┌─────────────────────────────────────────────────────────────────┐
│                    DMA TRANSFER SEQUENCE                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Scenario: Read 4KB from NVMe SSD to memory                     │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Phase 1: CPU Setup (happens once)                              │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  1. OS allocates DMA buffer:                                    │
│     ┌────────────────────────────────────┐                      │
│     │ Physical address: 0x10000000       │                      │
│     │ Size: 4096 bytes                   │                      │
│     │ Must be physically contiguous!     │                      │
│     └────────────────────────────────────┘                      │
│                                                                  │
│  2. CPU programs DMA controller registers:                      │
│     ┌────────────────────────────────────────────┐              │
│     │ DMA_SOURCE_ADDR   = 0xABCD1234 (device)   │              │
│     │ DMA_DEST_ADDR     = 0x10000000 (RAM)      │              │
│     │ DMA_TRANSFER_SIZE = 4096 bytes            │              │
│     │ DMA_DIRECTION     = DEVICE_TO_MEMORY      │              │
│     │ DMA_CONTROL       = START | INT_ON_DONE   │              │
│     └────────────────────────────────────────────┘              │
│                                                                  │
│  3. CPU continues with other work:                              │
│     • Can run other threads                                     │
│     • Can handle other I/O                                      │
│     • Zero CPU cycles spent on data transfer!                   │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Phase 2: DMA Transfer (CPU not involved!)                      │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  Timeline (nanosecond scale):                                   │
│  ┌────────────────────────────────────────────────────┐         │
│  │ T=0ns:   DMA controller requests bus                │         │
│  │          ├─► Asserts BUS_REQ signal                 │         │
│  │          └─► Waits for BUS_GRANT                    │         │
│  │                                                     │         │
│  │ T=50ns:  CPU grants bus (if not in use)            │         │
│  │          └─► DMA becomes "bus master"               │         │
│  │                                                     │         │
│  │ T=100ns: DMA reads from device                      │         │
│  │          ├─► Sends read command to NVMe             │         │
│  │          ├─► Device fetches data from flash         │         │
│  │          └─► Takes ~10-20 microseconds              │         │
│  │                                                     │         │
│  │ T=10µs:  Data arrives in device buffer              │         │
│  │          └─► Ready for transfer                     │         │
│  │                                                     │         │
│  │ T=10µs+: DMA burst transfer                         │         │
│  │          ├─► Reads 64 bytes from device             │         │
│  │          ├─► Writes 64 bytes to memory              │         │
│  │          ├─► Repeats 64 times (4KB total)           │         │
│  │          └─► Uses burst mode for efficiency         │         │
│  │                                                     │         │
│  │ T=12µs:  Transfer complete                          │         │
│  │          ├─► DMA releases bus                       │         │
│  │          └─► Raises interrupt line                  │         │
│  └────────────────────────────────────────────────────┘         │
│                                                                  │
│  Bus Cycles Used:                                               │
│  • 4096 bytes / 64 bytes per burst = 64 bus transactions        │
│  • Each transaction: ~5-10 CPU cycles (suppressed)              │
│  • Total: 320-640 cycles the CPU didn't have to execute!        │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Phase 3: Completion Interrupt                                  │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  1. DMA controller signals interrupt                            │
│  2. CPU saves current context                                   │
│  3. Interrupt handler runs:                                     │
│     ┌────────────────────────────────────┐                      │
│     │ void dma_complete_handler(void) {  │                      │
│     │   check_dma_status();              │                      │
│     │   mark_buffer_ready();             │                      │
│     │   wake_up_waiting_process();       │                      │
│     │ }                                  │                      │
│     └────────────────────────────────────┘                      │
│  4. Process that requested I/O wakes up                         │
│  5. Data ready to use!                                          │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### DMA Modes and Techniques

```
┌─────────────────────────────────────────────────────────────────┐
│                    DMA TRANSFER MODES                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. Single Transfer Mode (Cycle Stealing)                       │
│  ═══════════════════════════════════════════                    │
│                                                                  │
│  DMA transfers one byte/word, then releases bus                 │
│                                                                  │
│  Timeline:                                                      │
│  ┌────────────────────────────────────────────────┐             │
│  │ CPU │ DMA│ CPU │ DMA│ CPU │ DMA│ CPU │ DMA│    │             │
│  │  ▓  │ ▓  │  ▓  │ ▓  │  ▓  │ ▓  │  ▓  │ ▓  │... │             │
│  └────────────────────────────────────────────────┘             │
│                                                                  │
│  Pros: CPU can still use bus (interleaved)                      │
│  Cons: High overhead, many bus arbitrations                     │
│  Use: Slow devices (serial ports, old hardware)                 │
│                                                                  │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │
│                                                                  │
│  2. Burst Mode (Block Transfer)                                 │
│  ═══════════════════════════                                    │
│                                                                  │
│  DMA takes bus for entire transfer, then releases               │
│                                                                  │
│  Timeline:                                                      │
│  ┌────────────────────────────────────────────────┐             │
│  │ CPU │        DMA BURST           │    CPU      │             │
│  │  ▓  │ ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ │     ▓▓▓     │             │
│  └────────────────────────────────────────────────┘             │
│                                                                  │
│  Pros: Maximum throughput, minimal overhead                     │
│  Cons: CPU blocked during burst                                 │
│  Use: Fast devices (NVMe, modern network cards)                 │
│                                                                  │
│  Modern systems use 64-256 byte bursts!                         │
│                                                                  │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │
│                                                                  │
│  3. Demand Mode (Transparent DMA)                               │
│  ═══════════════════════════════════                            │
│                                                                  │
│  DMA only uses bus when CPU doesn't need it                     │
│                                                                  │
│  Timeline:                                                      │
│  ┌────────────────────────────────────────────────┐             │
│  │ CPU │DMA│ CPU │DMA│DMA│CPU│DMA│DMA│CPU│DMA│    │             │
│  │  ▓  │ ▓ │  ▓  │ ▓ │ ▓ │ ▓ │ ▓ │ ▓ │ ▓ │ ▓ │... │             │
│  └────────────────────────────────────────────────┘             │
│                                                                  │
│  Pros: Zero impact on CPU (uses idle cycles)                    │
│  Cons: Unpredictable timing                                     │
│  Use: Background transfers, low priority I/O                    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### Scatter-Gather DMA

One of the most powerful DMA features:

```
┌─────────────────────────────────────────────────────────────────┐
│                    SCATTER-GATHER DMA                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Problem: Virtual memory pages aren't physically contiguous     │
│                                                                  │
│  Example: Read 16KB file (4 pages) into process memory          │
│                                                                  │
│  Virtual Address Space (contiguous):                            │
│  ┌──────────────────────────────────────────────────┐           │
│  │ 0x2000 ─┐  0x3000 ─┐  0x4000 ─┐  0x5000 ─┐      │           │
│  │  Page 0 │   Page 1 │   Page 2 │   Page 3 │      │           │
│  │  (4KB)  │   (4KB)  │   (4KB)  │   (4KB)  │      │           │
│  └─────┬───────────┬───────────┬───────────┬────────┘           │
│        │           │           │           │                    │
│        │  Page Table Translation             │                    │
│        ▼           ▼           ▼           ▼                    │
│  ┌──────────────────────────────────────────────────┐           │
│  │ Physical Memory (NOT contiguous!):               │           │
│  ├──────────────────────────────────────────────────┤           │
│  │ 0x10000: [Page 0 data]     ◄── Physical page 1  │           │
│  │ 0x11000: [other data]                            │           │
│  │ 0x12000: [other data]                            │           │
│  │ 0x13000: [Page 2 data]     ◄── Physical page 3  │           │
│  │ 0x14000: [other data]                            │           │
│  │ 0x15000: [Page 1 data]     ◄── Physical page 5  │           │
│  │ 0x16000: [other data]                            │           │
│  │ 0x17000: [Page 3 data]     ◄── Physical page 7  │           │
│  └──────────────────────────────────────────────────┘           │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Traditional DMA: Would need 4 separate transfers! ✗            │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Scatter-Gather DMA: One operation! ✓                           │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  DMA Descriptor List (in memory):                               │
│  ┌────────────────────────────────────────────────────┐         │
│  │ Descriptor 0:                                      │         │
│  │   address  = 0x10000                               │         │
│  │   length   = 4096                                  │         │
│  │   next     = &descriptor_1                         │         │
│  ├────────────────────────────────────────────────────┤         │
│  │ Descriptor 1:                                      │         │
│  │   address  = 0x15000                               │         │
│  │   length   = 4096                                  │         │
│  │   next     = &descriptor_2                         │         │
│  ├────────────────────────────────────────────────────┤         │
│  │ Descriptor 2:                                      │         │
│  │   address  = 0x13000                               │         │
│  │   length   = 4096                                  │         │
│  │   next     = &descriptor_3                         │         │
│  ├────────────────────────────────────────────────────┤         │
│  │ Descriptor 3:                                      │         │
│  │   address  = 0x17000                               │         │
│  │   length   = 4096                                  │         │
│  │   next     = NULL (end of list)                    │         │
│  └────────────────────────────────────────────────────┘         │
│                                                                  │
│  CPU sets up: Point DMA controller to descriptor list           │
│  DMA engine: Follows linked list automatically!                 │
│  Result: All 4 pages filled in ONE logical operation            │
│                                                                  │
│  Benefits:                                                      │
│  ✓ No need for physically contiguous memory                     │
│  ✓ Works with virtual memory                                    │
│  ✓ Single interrupt at end                                      │
│  ✓ Minimal CPU involvement                                      │
│                                                                  │
│  Linux kernel struct:                                           │
│  ┌────────────────────────────────────┐                         │
│  │ struct scatterlist {               │                         │
│  │   unsigned long page_link;         │                         │
│  │   unsigned int  offset;            │                         │
│  │   unsigned int  length;            │                         │
│  │   dma_addr_t    dma_address;       │                         │
│  │ };                                 │                         │
│  └────────────────────────────────────┘                         │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### DMA Challenges and Solutions

```
┌─────────────────────────────────────────────────────────────────┐
│                    DMA CHALLENGES                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Challenge 1: Cache Coherency                                   │
│  ═══════════════════════════                                    │
│                                                                  │
│  Problem:                                                       │
│  ┌────────────────────────────────────────────────┐             │
│  │ 1. CPU writes data to address 0x10000          │             │
│  │    → Data in CPU cache, not yet in RAM!        │             │
│  │                                                │             │
│  │ 2. DMA reads from 0x10000                      │             │
│  │    → Reads stale data from RAM! ✗              │             │
│  │                                                │             │
│  │ 3. DMA writes new data to 0x10000              │             │
│  │    → RAM updated, but CPU cache still has old! │             │
│  │                                                │             │
│  │ 4. CPU reads from 0x10000                      │             │
│  │    → Gets old cached data! ✗                   │             │
│  └────────────────────────────────────────────────┘             │
│                                                                  │
│  Solution 1: Software Cache Management                          │
│  ┌────────────────────────────────────────────────┐             │
│  │ // Before DMA write (device → memory)          │             │
│  │ dma_map_single(dev, addr, size, DMA_FROM_DEV); │             │
│  │   ├─► Invalidates CPU cache lines              │             │
│  │   └─► Ensures CPU will read from RAM           │             │
│  │                                                │             │
│  │ // Before DMA read (memory → device)           │             │
│  │ dma_map_single(dev, addr, size, DMA_TO_DEV);   │             │
│  │   ├─► Flushes CPU cache to RAM                 │             │
│  │   └─► Ensures DMA sees latest data             │             │
│  │                                                │             │
│  │ // After DMA completes                         │             │
│  │ dma_unmap_single(dev, addr, size, direction);  │             │
│  │   └─► Re-synchronizes cache                    │             │
│  └────────────────────────────────────────────────┘             │
│                                                                  │
│  Solution 2: Hardware Cache Coherency                           │
│  Modern systems (x86, ARM with ACE):                            │
│  • DMA controller snoops CPU cache                              │
│  • Automatic cache invalidation/flush                           │
│  • No software intervention needed!                             │
│  • Cost: More complex hardware, slightly slower DMA             │
│                                                                  │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │
│                                                                  │
│  Challenge 2: Address Translation (IOMMU)                       │
│  ═══════════════════════════════════════                        │
│                                                                  │
│  Problem: Devices use physical addresses, but:                  │
│  • Virtual memory is fragmented                                 │
│  • Security: Device shouldn't access all memory                 │
│  • Virtualization: Guest physical ≠ host physical              │
│                                                                  │
│  Solution: IOMMU (I/O Memory Management Unit)                   │
│  ┌────────────────────────────────────────────────┐             │
│  │                    ┌─────────┐                 │             │
│  │                    │   CPU   │                 │             │
│  │                    └────┬────┘                 │             │
│  │                         │                      │             │
│  │                    ┌────▼────┐                 │             │
│  │                    │   MMU   │ ◄── Virtual     │             │
│  │                    └────┬────┘     addressing  │             │
│  │                         │                      │             │
│  │                    ┌────▼────┐                 │             │
│  │                    │   RAM   │                 │             │
│  │                    └─────────┘                 │             │
│  │                         ▲                      │             │
│  │                         │                      │             │
│  │  ┌─────────┐       ┌───┴────┐                 │             │
│  │  │ Device  │───────│ IOMMU  │ ◄── Device      │             │
│  │  │ (NIC)   │       └────────┘     virtual     │             │
│  │  └─────────┘                      addressing! │             │
│  │       │                                        │             │
│  │       └─► Device uses "virtual" addresses      │             │
│  │           IOMMU translates to physical         │             │
│  │           Just like MMU for CPU!               │             │
│  └────────────────────────────────────────────────┘             │
│                                                                  │
│  Benefits:                                                      │
│  ✓ Security: Device can only access mapped pages               │
│  ✓ Scatter-gather for free (IOMMU handles translation)         │
│  ✓ VM passthrough (guest OS controls device directly)          │
│  ✓ Protection from buggy/malicious devices                      │
│                                                                  │
│  Example (Intel VT-d / AMD-Vi):                                 │
│  ┌────────────────────────────────────────────────┐             │
│  │ Device writes to "DMA address" 0x2000          │             │
│  │        ↓                                       │             │
│  │ IOMMU looks up in device page table            │             │
│  │        ↓                                       │             │
│  │ Translates to physical 0x15000                 │             │
│  │        ↓                                       │             │
│  │ Access granted, write proceeds                 │             │
│  │                                                │             │
│  │ Device tries to write to unmapped 0x9000       │             │
│  │        ↓                                       │             │
│  │ IOMMU: Page fault! Access denied               │             │
│  │        ↓                                       │             │
│  │ Interrupt to OS: "Device attempted illegal DMA"│             │
│  └────────────────────────────────────────────────┘             │
│                                                                  │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │
│                                                                  │
│  Challenge 3: Bus Contention                                    │
│  ═══════════════════════════                                    │
│                                                                  │
│  Problem: Multiple devices competing for bus bandwidth          │
│                                                                  │
│  ┌────────────────────────────────────────────────┐             │
│  │     Device 1 (NIC)  ──┐                        │             │
│  │     Device 2 (SSD)  ──┤                        │             │
│  │     Device 3 (GPU)  ──┼──► System Bus ───► RAM│             │
│  │     CPU             ──┘                        │             │
│  │                                                │             │
│  │  All want bus at same time!                    │             │
│  └────────────────────────────────────────────────┘             │
│                                                                  │
│  Solution: Bus Arbitration                                      │
│  • Priority levels (GPU > SSD > NIC)                            │
│  • Round-robin for fairness                                     │
│  • QoS guarantees for real-time devices                         │
│                                                                  │
│  Modern Solution: Multiple Paths                                │
│  • PCIe lanes: Each device gets dedicated bandwidth            │
│  • NVMe: Up to x4 lanes (4 GB/s!)                               │
│  • GPU: x16 lanes (16 GB/s!)                                    │
│  • No contention until memory controller                        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### Real-World DMA Performance

```
┌─────────────────────────────────────────────────────────────────┐
│                    DMA PERFORMANCE METRICS                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Benchmark: Transfer 1 MB from NVMe SSD to memory               │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Without DMA (Programmed I/O):                                  │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  for (i = 0; i < 1048576; i++) {                                │
│      buffer[i] = inb(device_port);  // Read one byte            │
│  }                                                               │
│                                                                  │
│  • 1,048,576 CPU instructions                                   │
│  • 1,048,576 I/O port reads                                     │
│  • Each read: ~1000 CPU cycles (I/O is slow!)                   │
│  • Total: ~1 billion cycles @ 3 GHz = 333 milliseconds          │
│  • CPU usage: 100% (blocked entire time)                        │
│  • Throughput: 3 MB/s                                           │
│  • CPU available for other work: 0%                             │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  With DMA:                                                      │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  // Setup (one time)                                            │
│  dma_setup(device, buffer, 1048576);  // ~100 instructions      │
│  // ... CPU does other work ...                                 │
│  // Interrupt fires when complete                               │
│                                                                  │
│  • CPU involvement: ~300 cycles (setup + interrupt)             │
│  • Transfer time: ~0.3 milliseconds (3 GB/s)                    │
│  • CPU usage during transfer: 0%                                │
│  • Throughput: 3,000 MB/s (1000x faster!)                       │
│  • CPU available for other work: 99.9%                          │
│                                                                  │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │
│                                                                  │
│  Real Device Throughput (with DMA):                             │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                             │
│                                                                  │
│  Device              Bandwidth      CPU Usage   Notes           │
│  ───────────────────────────────────────────────────────────    │
│  NVMe SSD (PCIe 4.0) 7,000 MB/s    <1%         Sequential read  │
│  10 Gbit Ethernet    1,250 MB/s    5-10%       With checksum    │
│  USB 3.2 Gen 2       1,250 MB/s    <1%         Gen2x2          │
│  SATA SSD            600 MB/s      <1%         Limited by SATA  │
│  GPU memory copy     16,000 MB/s   <1%         PCIe x16        │
│                                                                  │
│  Without DMA, these speeds would be impossible!                 │
│  CPU would be 100% busy just copying data.                      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Comprehensive Interrupt Deep Dive

Interrupts are fundamental to modern OS design. Let's understand them completely:

```
┌─────────────────────────────────────────────────────────────────┐
│                INTERRUPT SYSTEM ARCHITECTURE                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  What is an Interrupt?                                          │
│  ═══════════════════════                                        │
│                                                                  │
│  Definition: Hardware signal that stops CPU's current execution │
│  and forces it to handle an event immediately.                  │
│                                                                  │
│  Think of it like: You're working on a task, someone taps your  │
│  shoulder urgently. You pause, handle their request, then       │
│  resume your original work.                                     │
│                                                                  │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │
│                                                                  │
│  Interrupt Types:                                               │
│  ═══════════════                                                │
│                                                                  │
│  1. Hardware Interrupts (IRQ - Interrupt Request)               │
│     • Generated by hardware devices                             │
│     • Asynchronous (can happen any time)                        │
│     • Examples:                                                 │
│       - Timer tick (every 1-10 ms)                              │
│       - Keyboard press                                          │
│       - Disk I/O complete                                       │
│       - Network packet arrival                                  │
│       - USB device connected                                    │
│                                                                  │
│  2. Software Interrupts (System Calls)                          │
│     • Generated by software via special instructions            │
│     • Synchronous (predictable)                                 │
│     • Examples:                                                 │
│       - x86: INT 0x80 (legacy), SYSCALL (modern)                │
│       - ARM: SVC (Supervisor Call)                              │
│       - Used for: open(), read(), write(), etc.                 │
│                                                                  │
│  3. Exceptions (CPU-generated)                                  │
│     • CPU detects error condition                               │
│     • Synchronous                                               │
│     • Examples:                                                 │
│       - Page fault (memory access error)                        │
│       - Division by zero                                        │
│       - Invalid instruction                                     │
│       - Segmentation fault                                      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### Interrupt Hardware: IRQ Lines and Controllers

```
┌─────────────────────────────────────────────────────────────────┐
│                INTERRUPT CONTROLLER (APIC/PIC)                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Legacy: PIC (Programmable Interrupt Controller)                │
│  Modern: APIC (Advanced PIC) / IO-APIC                          │
│                                                                  │
│  Physical Connection:                                           │
│  ┌────────────────────────────────────────────────────┐         │
│  │                                                    │         │
│  │  ┌──────────┐                                     │         │
│  │  │  Timer   │───IRQ 0                             │         │
│  │  └──────────┘                                     │         │
│  │                                                    │         │
│  │  ┌──────────┐                                     │         │
│  │  │ Keyboard │───IRQ 1                             │         │
│  │  └──────────┘                                     │         │
│  │                            ┌────────────────┐     │         │
│  │  ┌──────────┐              │   IO-APIC      │     │         │
│  │  │  Serial  │───IRQ 3 ────►│  (Interrupt    │     │         │
│  │  └──────────┘              │   Router)      │     │         │
│  │                            │                │     │         │
│  │  ┌──────────┐              │  • Prioritizes │     │         │
│  │  │   Disk   │───IRQ 14────►│  • Masks IRQs  │────►│  CPU   │
│  │  └──────────┘              │  • Routes to   │     │  INTR  │
│  │                            │    specific CPU│     │  Pin   │
│  │  ┌──────────┐              │                │     │         │
│  │  │ Network  │───IRQ 11────►│  • Message     │     │         │
│  │  └──────────┘              │    Signaled    │     │         │
│  │                            │    Interrupts  │     │         │
│  │  ┌──────────┐              │    (MSI/MSI-X) │     │         │
│  │  │   USB    │───IRQ 16────►│                │     │         │
│  │  └──────────┘              └────────────────┘     │         │
│  │                                                    │         │
│  │  ┌──────────┐                                     │         │
│  │  │   GPU    │───MSI-X (no IRQ line!)              │         │
│  │  └──────────┘    (writes directly to memory)      │         │
│  │                                                    │         │
│  └────────────────────────────────────────────────────┘         │
│                                                                  │
│  IRQ Numbers (x86 traditional):                                 │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                 │
│  IRQ 0  → Timer (every tick)                                    │
│  IRQ 1  → Keyboard                                              │
│  IRQ 2  → Cascade (connects to second PIC)                      │
│  IRQ 3  → Serial port 2 (COM2)                                  │
│  IRQ 4  → Serial port 1 (COM1)                                  │
│  IRQ 5  → Sound card / LPT2                                     │
│  IRQ 6  → Floppy disk (legacy)                                  │
│  IRQ 7  → Parallel port (LPT1)                                  │
│  IRQ 8  → Real-time clock                                       │
│  IRQ 9  → ACPI                                                  │
│  IRQ 10 → Available                                             │
│  IRQ 11 → Available (often USB/Network)                         │
│  IRQ 12 → PS/2 Mouse                                            │
│  IRQ 13 → Math coprocessor                                      │
│  IRQ 14 → Primary IDE/SATA                                      │
│  IRQ 15 → Secondary IDE/SATA                                    │
│                                                                  │
│  Modern Systems (MSI/MSI-X):                                    │
│  • No shared IRQ lines!                                         │
│  • Each device gets unique interrupt vector                     │
│  • Device writes to memory to signal interrupt                  │
│  • GPU can have 2048 MSI-X vectors!                             │
│  • Much better performance, no IRQ conflicts                    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### Interrupt Handling: Complete Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                INTERRUPT HANDLING SEQUENCE                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Scenario: Disk I/O completes while CPU is running user code    │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Step 1: Device Signals Interrupt                               │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  Disk controller:                                               │
│  1. Completes data transfer via DMA                             │
│  2. Asserts IRQ line (or sends MSI message)                     │
│  3. Waits for acknowledgment                                    │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Step 2: CPU Detects Interrupt (end of instruction)             │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  CPU state before interrupt:                                    │
│  ┌────────────────────────────────────┐                         │
│  │ Mode: User (ring 3)                │                         │
│  │ PC:   0x400150 (user code)         │                         │
│  │ Stack: User stack                  │                         │
│  │ EAX:  0x12345678                   │                         │
│  │ EBX:  0xABCDEF00                   │                         │
│  │ ... (all registers)                │                         │
│  └────────────────────────────────────┘                         │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Step 3: Hardware Context Save (automatic!)                     │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  CPU automatically (in hardware):                               │
│  1. Disables further interrupts (sets IF=0)                     │
│  2. Saves state to kernel stack:                                │
│     ┌─────────────────────────┐                                 │
│     │ Kernel Stack (grows ↓)  │                                 │
│     ├─────────────────────────┤                                 │
│     │ User SS (stack segment) │  ← Pushed by CPU               │
│     │ User ESP (stack ptr)    │  ← Pushed by CPU               │
│     │ EFLAGS                  │  ← Pushed by CPU               │
│     │ User CS (code segment)  │  ← Pushed by CPU               │
│     │ User EIP (return addr)  │  ← Pushed by CPU               │
│     │ Error code (optional)   │  ← Pushed by CPU               │
│     └─────────────────────────┘                                 │
│  3. Switches to kernel mode (ring 0)                            │
│  4. Loads new CS:EIP from IDT (Interrupt Descriptor Table)      │
│  5. Switches to kernel stack                                    │
│                                                                  │
│  All in ~10-20 CPU cycles!                                      │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Step 4: IDT Lookup                                             │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  Interrupt Descriptor Table (IDT):                              │
│  ┌──────────┬─────────────────────────────────┐                 │
│  │ Vector # │ Handler Address                 │                 │
│  ├──────────┼─────────────────────────────────┤                 │
│  │    0     │ divide_error_handler()          │                 │
│  │    1     │ debug_handler()                 │                 │
│  │   ...    │ ...                             │                 │
│  │   14     │ page_fault_handler()            │                 │
│  │   32     │ timer_interrupt_handler()       │                 │
│  │   33     │ keyboard_handler()              │                 │
│  │   ...    │ ...                             │                 │
│  │   46     │ disk_interrupt_handler() ◄──────┼─ We go here!   │
│  │   ...    │ ...                             │                 │
│  │   255    │ spurious_interrupt()            │                 │
│  └──────────┴─────────────────────────────────┘                 │
│                                                                  │
│  CPU reads IDT[46] and jumps to disk_interrupt_handler()        │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Step 5: Interrupt Handler Executes                             │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  void disk_interrupt_handler(void) {                            │
│      // 1. Save remaining registers (PUSHA instruction)         │
│      save_all_registers();                                      │
│                                                                  │
│      // 2. Acknowledge interrupt (clear IRQ line)               │
│      outb(DISK_STATUS, ACK_INTERRUPT);                          │
│                                                                  │
│      // 3. Read device status                                   │
│      status = inb(DISK_STATUS);                                 │
│                                                                  │
│      // 4. Process the interrupt                                │
│      if (status & ERROR_BIT) {                                  │
│          handle_disk_error();                                   │
│      } else {                                                   │
│          // Data ready! Update data structures                  │
│          mark_io_complete(pending_io);                          │
│      }                                                           │
│                                                                  │
│      // 5. Wake up blocked process                              │
│      wakeup(waiting_process);                                   │
│                                                                  │
│      // 6. Send EOI (End Of Interrupt) to interrupt controller  │
│      send_eoi(IRQ_DISK);                                        │
│                                                                  │
│      // 7. Restore registers and return                         │
│      restore_all_registers();                                   │
│      IRET;  // Special return instruction                       │
│  }                                                               │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│  Step 6: Return from Interrupt (IRET)                           │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  IRET instruction (hardware):                                   │
│  1. Pops EIP, CS, EFLAGS from stack                             │
│  2. Pops ESP, SS (if privilege level changes)                   │
│  3. Switches back to user mode (ring 3)                         │
│  4. Re-enables interrupts (IF=1)                                │
│  5. Resumes user code exactly where it left off                 │
│                                                                  │
│  User code continues as if nothing happened!                    │
│  (Except: disk I/O is now complete)                             │
│                                                                  │
│  Total latency: 1-5 microseconds                                │
│  • Hardware save/restore: ~50 cycles                            │
│  • Handler execution: ~500-2000 cycles                          │
│  • Context switch (if needed): +1000s of cycles                 │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### Interrupt Priority and Masking

```
┌─────────────────────────────────────────────────────────────────┐
│                INTERRUPT PRIORITY SYSTEM                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Priority Levels (x86):                                         │
│  ═══════════════════════                                        │
│                                                                  │
│  Highest → Lowest:                                              │
│  ┌──────────────────────────────────────────────┐               │
│  │ 1. NMI (Non-Maskable Interrupt)             │               │
│  │    • Cannot be disabled                      │               │
│  │    • Used for: Hardware errors, watchdog     │               │
│  │    • Vector 2                                │               │
│  │                                              │               │
│  │ 2. Hardware Exceptions                       │               │
│  │    • Page faults, divide by zero, etc.       │               │
│  │    • Vectors 0-31 (reserved by CPU)          │               │
│  │                                              │               │
│  │ 3. Maskable Hardware Interrupts              │               │
│  │    • Can be disabled with CLI instruction    │               │
│  │    • Vectors 32-255                          │               │
│  │    • Priority by vector number (higher = priority)│          │
│  │                                              │               │
│  │ 4. Software Interrupts                       │               │
│  │    • Lowest priority                         │               │
│  │    • Initiated by INT instruction            │               │
│  └──────────────────────────────────────────────┘               │
│                                                                  │
│  Interrupt Masking:                                             │
│  ═══════════════════                                            │
│                                                                  │
│  Global disable (all maskable interrupts):                      │
│  ┌──────────────────────────────────┐                           │
│  │ CLI    ; Clear Interrupt Flag    │                           │
│  │ // Critical section               │                           │
│  │ // No interrupts here!            │                           │
│  │ STI    ; Set Interrupt Flag       │                           │
│  └──────────────────────────────────┘                           │
│                                                                  │
│  Per-IRQ masking (via interrupt controller):                    │
│  ┌──────────────────────────────────┐                           │
│  │ // Disable IRQ 14 (disk)         │                           │
│  │ outb(PIC_MASK, 1 << 14);         │                           │
│  │                                  │                           │
│  │ // Re-enable IRQ 14              │                           │
│  │ outb(PIC_MASK, ~(1 << 14));      │                           │
│  └──────────────────────────────────┘                           │
│                                                                  │
│  Why mask interrupts?                                           │
│  • Protect critical sections (kernel data structures)           │
│  • Prevent interrupt storms                                     │
│  • During interrupt handler (prevent nesting)                   │
│  • Synchronization primitive                                    │
│                                                                  │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ │
│                                                                  │
│  Nested Interrupts:                                             │
│  ═══════════════════                                            │
│                                                                  │
│  Can high-priority interrupt preempt low-priority handler?      │
│                                                                  │
│  ┌────────────────────────────────────────────┐                 │
│  │ Timeline:                                  │                 │
│  │                                            │                 │
│  │ User code                                  │                 │
│  │    ├──────────────────────────────────────►│                 │
│  │    │                                       │                 │
│  │    ▼ IRQ 11 (network, low priority)        │                 │
│  │ Network handler                            │                 │
│  │    ├───────┬───────────────────────────────►│                 │
│  │    │       │                               │                 │
│  │    │       ▼ IRQ 0 (timer, high priority)  │                 │
│  │    │    Timer handler (nested!)            │                 │
│  │    │       ├─────────────────────────────► │                 │
│  │    │       │ (if nesting enabled)          │                 │
│  │    │    ◄──┘ IRET                          │                 │
│  │    │                                       │                 │
│  │    ◄───────┘ IRET                          │                 │
│  │                                            │                 │
│  │ User code resumes                          │                 │
│  │    ◄───────────────────────────────────────┘                 │
│  └────────────────────────────────────────────┘                 │
│                                                                  │
│  Linux typically: Interrupts disabled during handler            │
│  Real-time systems: Allow nesting for critical interrupts       │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### Interrupt Storm: When Things Go Wrong

```
┌─────────────────────────────────────────────────────────────────┐
│                INTERRUPT STORM PROBLEM                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Problem: Device generates interrupts faster than CPU can handle │
│                                                                  │
│  Example: 10 Gbit network receiving packets                     │
│  • 10 Gbps / 1500 byte packets = 833,000 packets/sec           │
│  • If interrupt per packet = 833,000 interrupts/sec!            │
│  • Each interrupt: ~2000 cycles                                 │
│  • Total: 1.6 billion cycles/sec @ 3 GHz                        │
│  • Result: 50%+ CPU just handling interrupts!                   │
│                                                                  │
│  Timeline of disaster:                                          │
│  ┌────────────────────────────────────────────┐                 │
│  │ User code                                  │                 │
│  │ │IRQ│ │IRQ│ │IRQ│ │IRQ│ │IRQ│ │IRQ│       │                 │
│  │  ▓    ▓    ▓    ▓    ▓    ▓   ← Too many! │                 │
│  │                                            │                 │
│  │ CPU never gets to do real work!            │                 │
│  └────────────────────────────────────────────┘                 │
│                                                                  │
│  Solutions:                                                     │
│  ═══════════                                                    │
│                                                                  │
│  1. Interrupt Coalescing                                        │
│     • Device waits and batches interrupts                       │
│     • Fire interrupt only after N packets or T microseconds     │
│     • Trade: Slight latency for huge throughput improvement     │
│                                                                  │
│  2. NAPI (New API) - Linux                                      │
│     • First packet: Interrupt                                   │
│     • Switch to polling mode                                    │
│     • Process many packets                                      │
│     • When idle: Back to interrupt mode                         │
│     • Best of both worlds!                                      │
│                                                                  │
│  3. MSI-X + Multiple Queues                                     │
│     • Modern NICs have 64+ queues                               │
│     • Each queue → different CPU core                           │
│     • Distributes interrupt load                                │
│     • Scales with cores!                                        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Comparison

| Method | CPU Usage | Throughput | Use Case |
|--------|-----------|------------|----------|
| Polling | 100% during I/O | Low | Simple devices |
| Interrupt | Context switch per transfer | Medium | Keyboard, serial |
| DMA | Minimal | High | Disk, network |

---

## I/O Scheduling

### Disk I/O Scheduling

Traditional HDDs have mechanical seek time, making scheduling important:

```
┌─────────────────────────────────────────────────────────────────┐
│                    I/O SCHEDULING ALGORITHMS                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Request queue: [98, 183, 37, 122, 14, 124, 65, 67]            │
│  Current head position: 53                                      │
│                                                                  │
│  FCFS (First Come First Served):                               │
│  53 → 98 → 183 → 37 → 122 → 14 → 124 → 65 → 67                │
│  Total movement: 640 tracks                                     │
│                                                                  │
│  SSTF (Shortest Seek Time First):                              │
│  53 → 65 → 67 → 37 → 14 → 98 → 122 → 124 → 183               │
│  Total movement: 236 tracks                                     │
│  Problem: Starvation of far requests                           │
│                                                                  │
│  SCAN (Elevator):                                               │
│  53 → 37 → 14 → [0] → 65 → 67 → 98 → 122 → 124 → 183         │
│  Total movement: 236 tracks                                     │
│  Move in one direction until end, then reverse                 │
│                                                                  │
│  C-SCAN (Circular SCAN):                                        │
│  53 → 65 → 67 → 98 → 122 → 124 → 183 → [199] → [0] → 14 → 37 │
│  Always scan in same direction, jump back                      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Linux I/O Schedulers

```bash
# Check current scheduler
cat /sys/block/sda/queue/scheduler

# Available schedulers (kernel 5.x+)
# [mq-deadline] kyber bfq none
```

| Scheduler | Description | Best For |
|-----------|-------------|----------|
| **none** | No scheduling, FIFO | NVMe, VMs |
| **mq-deadline** | Deadline-based, prevents starvation | HDDs, latency-sensitive |
| **bfq** | Budget Fair Queuing | Desktop, interactive |
| **kyber** | Token-based, low overhead | Fast SSDs |

---

## Buffering and Caching

### Buffer Types

```
┌─────────────────────────────────────────────────────────────────┐
│                    I/O BUFFERING                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  No Buffering:                                                  │
│  ┌───────┐    ┌──────────┐                                     │
│  │Process│◄───│  Device  │  Direct transfer, process blocks   │
│  └───────┘    └──────────┘                                     │
│                                                                  │
│  Single Buffering:                                              │
│  ┌───────┐    ┌────────┐    ┌──────────┐                       │
│  │Process│◄───│ Buffer │◄───│  Device  │                       │
│  └───────┘    └────────┘    └──────────┘                       │
│  Process reads from buffer while device fills next              │
│                                                                  │
│  Double Buffering:                                              │
│  ┌───────┐    ┌────────┐    ┌──────────┐                       │
│  │Process│◄───│Buffer A│◄───│  Device  │                       │
│  │       │    └────────┘    │          │                       │
│  │       │◄───┌────────┐◄───│          │                       │
│  └───────┘    │Buffer B│    └──────────┘                       │
│               └────────┘                                        │
│  Process uses A while device fills B, then swap                │
│                                                                  │
│  Circular Buffering:                                            │
│  Multiple buffers in ring for high-throughput streaming        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Page Cache

The Linux page cache buffers disk I/O:

```c
// Read operation flow
ssize_t read_with_cache(int fd, void *buf, size_t count) {
    // 1. Check page cache for requested pages
    // 2. If cache hit: copy from cache to user buffer
    // 3. If cache miss: 
    //    a. Allocate page frame
    //    b. Read from disk into page frame
    //    c. Add to page cache
    //    d. Copy to user buffer
    // 4. Return bytes read
}

// Bypass cache with O_DIRECT
int fd = open("data", O_RDONLY | O_DIRECT);
// Reads go directly to/from device
// Used by databases for their own caching
```

---

## Modern I/O: io_uring

Linux's revolutionary async I/O interface (kernel 5.1+):

### Traditional async I/O Problems

```c
// Old approach: one syscall per operation
for (int i = 0; i < 1000; i++) {
    read(fds[i], bufs[i], sizes[i]);  // 1000 syscalls!
}

// aio_* functions: complex, limited
// epoll: only notification, still need syscall to read
```

### io_uring Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    IO_URING ARCHITECTURE                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│     User Space                    Kernel Space                  │
│                                                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              Shared Memory (mmap'd)                     │   │
│  │                                                          │   │
│  │  ┌──────────────────────────────────────────────────┐   │   │
│  │  │          Submission Queue (SQ)                   │   │   │
│  │  │   ┌─────┬─────┬─────┬─────┬─────┐               │   │   │
│  │  │   │ SQE │ SQE │ SQE │     │     │ ← Add entries │   │   │
│  │  │   └─────┴─────┴─────┴─────┴─────┘               │   │   │
│  │  │   head ──────────────────────────► tail          │   │   │
│  │  └──────────────────────────────────────────────────┘   │   │
│  │                          │                              │   │
│  │                          │ io_uring_enter()             │   │
│  │                          │ (submit + wait, optional)   │   │
│  │                          ▼                              │   │
│  │  ┌──────────────────────────────────────────────────┐   │   │
│  │  │          Completion Queue (CQ)                   │   │   │
│  │  │   ┌─────┬─────┬─────┬─────┬─────┐               │   │   │
│  │  │   │ CQE │ CQE │     │     │     │ ← Poll these  │   │   │
│  │  │   └─────┴─────┴─────┴─────┴─────┘               │   │   │
│  │  │   head ────────────────────► tail                │   │   │
│  │  └──────────────────────────────────────────────────┘   │   │
│  │                                                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                  │
│  Zero syscalls for submission in SQPOLL mode!                  │
│  Batch multiple operations per syscall                          │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### io_uring Example

```c
#include <liburing.h>

void async_reads(const char *files[], int count) {
    struct io_uring ring;
    io_uring_queue_init(256, &ring, 0);
    
    // Submit all reads
    for (int i = 0; i < count; i++) {
        int fd = open(files[i], O_RDONLY);
        struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);
        
        io_uring_prep_read(sqe, fd, buffers[i], BUF_SIZE, 0);
        sqe->user_data = i;  // Track which request
    }
    
    // Single syscall submits all
    io_uring_submit(&ring);
    
    // Reap completions
    for (int i = 0; i < count; i++) {
        struct io_uring_cqe *cqe;
        io_uring_wait_cqe(&ring, &cqe);
        
        int result = cqe->res;
        int index = cqe->user_data;
        
        // Process result...
        
        io_uring_cqe_seen(&ring, cqe);
    }
    
    io_uring_queue_exit(&ring);
}
```

### io_uring Benefits

| Feature | Benefit |
|---------|---------|
| **Batching** | Submit 1000s of ops with one syscall |
| **Shared memory** | Zero-copy submission/completion |
| **SQPOLL mode** | Kernel polls SQ, zero syscalls |
| **Link chains** | Dependencies between operations |
| **Fixed files/buffers** | Register once, use forever |

---

## Device Drivers

### Driver Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    DRIVER ARCHITECTURE                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  User Space    │   system calls (read, write, ioctl)           │
│  ──────────────│───────────────────────────────────────────────│
│                ▼                                                │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    VFS Layer                            │   │
│  │        (Virtual File System abstraction)                │   │
│  └───────────────────────────┬─────────────────────────────┘   │
│                              ▼                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              Device Driver (Kernel Module)              │   │
│  │                                                          │   │
│  │   struct file_operations {                              │   │
│  │       .open    = my_open,                               │   │
│  │       .read    = my_read,                               │   │
│  │       .write   = my_write,                              │   │
│  │       .ioctl   = my_ioctl,                              │   │
│  │       .release = my_close,                              │   │
│  │   };                                                     │   │
│  │                                                          │   │
│  │   - Interrupt handlers                                   │   │
│  │   - DMA setup                                            │   │
│  │   - Device initialization                                │   │
│  └───────────────────────────┬─────────────────────────────┘   │
│                              ▼                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                   Hardware Device                       │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Character vs Block Devices

| Aspect | Character Device | Block Device |
|--------|-----------------|--------------|
| Access | Byte stream | Fixed-size blocks |
| Buffering | No kernel buffering | Uses buffer cache |
| Random access | Not supported | Supported |
| Examples | Keyboard, serial port | Disk, SSD |
| Major/Minor | Yes | Yes |

```bash
# Check device types
ls -la /dev/
crw-rw----  1 root tty  4,  0  # c = character (tty0)
brw-rw----  1 root disk 8,  0  # b = block (sda)
```

---

## Interview Deep Dive Questions

<AccordionGroup>
  <Accordion title="Q1: Explain DMA and its advantages over PIO">
    **Answer:**
    
    **PIO (Programmed I/O)**:
    ```
    for each byte:
      CPU: check device ready
      CPU: read byte from device register
      CPU: write byte to memory
    ```
    - CPU executes every transfer instruction
    - CPU utilization: 100% during transfer
    - Good for: Small transfers, simple devices
    
    **DMA (Direct Memory Access)**:
    ```
    CPU: configure DMA controller
        - source address (device)
        - destination address (memory)
        - byte count
        - direction
    CPU: start transfer
    CPU: ...do other work...
    DMA: transfers data directly (bus master)
    DMA: interrupt CPU when complete
    ```
    
    **Advantages of DMA**:
    1. **CPU efficiency**: CPU free during transfer
    2. **Higher throughput**: DMA controller optimized for transfers
    3. **Lower latency**: No per-byte CPU overhead
    
    **DMA considerations**:
    - Needs contiguous physical memory (or scatter-gather DMA)
    - Cache coherency: CPU cache may have stale data
    - Setup overhead: Small transfers may be slower than PIO
  </Accordion>
  
  <Accordion title="Q2: Why is io_uring faster than traditional async I/O?">
    **Answer:**
    
    **Traditional read() syscall**:
    ```
    1. User calls read()
    2. Trap to kernel (mode switch)
    3. Kernel validates parameters
    4. Kernel initiates I/O
    5. Kernel copies data to user buffer
    6. Return to user (mode switch)
    
    Cost: 2 mode switches per operation
    ```
    
    **io_uring**:
    ```
    1. User writes SQE to shared memory (no syscall)
    2. User updates tail pointer (no syscall)
    3. [If needed] io_uring_enter() to notify kernel
    4. Kernel processes submission queue
    5. Kernel writes CQE to shared memory
    6. User reads CQE (no syscall)
    
    Cost: 0-1 syscall per BATCH of operations
    ```
    
    **Speed advantages**:
    
    1. **Batching**: 1 syscall for 1000 operations
    2. **SQPOLL mode**: Kernel thread polls SQ, zero user syscalls
    3. **Zero-copy**: Shared memory rings, no copy in/out
    4. **Registered buffers**: Pre-registered, skip validation
    5. **Registered files**: File descriptors pre-validated
    
    **Benchmarks**: 10-100x better IOPS for small I/O workloads
  </Accordion>
  
  <Accordion title="Q3: How does an interrupt work from hardware to user process?">
    **Answer:**
    
    ```
    1. HARDWARE: Device asserts interrupt line
       └─► Interrupt controller (APIC) receives signal
    
    2. INTERRUPT CONTROLLER:
       └─► Determines interrupt vector (IRQ number)
       └─► Signals CPU
    
    3. CPU:
       └─► Finishes current instruction
       └─► Saves context (registers, flags) to stack
       └─► Disables interrupts (for this level)
       └─► Looks up handler in IDT (Interrupt Descriptor Table)
       └─► Jumps to handler address
    
    4. KERNEL INTERRUPT HANDLER (Top Half):
       └─► Acknowledge interrupt to device
       └─► Quick processing (clear device state)
       └─► Schedule bottom half for heavy work
       └─► Return from interrupt (iret)
    
    5. KERNEL BOTTOM HALF (softirq/tasklet/workqueue):
       └─► Process data
       └─► Copy to user buffer
       └─► Wake up waiting process
    
    6. SCHEDULER:
       └─► User process becomes runnable
       └─► Eventually scheduled
    
    7. USER PROCESS:
       └─► read() returns with data
    ```
    
    **Top half vs Bottom half**:
    - Top half: Runs with interrupts disabled, must be fast
    - Bottom half: Can sleep, do heavy processing
    
    **Modern optimizations**:
    - NAPI (networking): Disable interrupts, switch to polling under load
    - Threaded IRQs: Handler runs in kernel thread context
  </Accordion>
  
  <Accordion title="Q4: Design a high-performance logging system">
    **Answer:**
    
    **Requirements**:
    - Write-heavy (100K+ logs/sec)
    - Durability (survive crashes)
    - Low latency (don't block application)
    
    **Design**:
    
    ```
    Application Threads
           │
           ▼
    ┌─────────────────────────────────────┐
    │     Lock-Free Ring Buffer           │
    │  (one per thread, thread-local)     │
    │  [log][log][log][...][log]          │
    └────────────────┬────────────────────┘
                     │
                     ▼
    ┌─────────────────────────────────────┐
    │        Aggregator Thread            │
    │  - Collects from all ring buffers   │
    │  - Batches logs                      │
    │  - Compresses (optional)            │
    └────────────────┬────────────────────┘
                     │
                     ▼
    ┌─────────────────────────────────────┐
    │        Async Writer Thread          │
    │  - Uses io_uring for batched writes │
    │  - O_APPEND for atomicity           │
    │  - fdatasync() periodically         │
    └─────────────────────────────────────┘
    ```
    
    **Key optimizations**:
    
    1. **Thread-local buffers**: No contention between threads
    2. **Ring buffers**: Fixed memory, no allocation
    3. **Batching**: Combine small logs into larger writes
    4. **io_uring**: Async, batched I/O submission
    5. **O_APPEND**: Kernel handles concurrent appends atomically
    6. **Periodic fsync**: Balance durability vs performance
    
    **Durability levels**:
    ```c
    // Level 1: OS buffer only (fastest, least durable)
    write(fd, log, len);
    
    // Level 2: fdatasync (data only, not metadata)
    write(fd, log, len);
    fdatasync(fd);  // Every N writes
    
    // Level 3: fsync (data + metadata)
    write(fd, log, len);
    fsync(fd);  // Slowest but safest
    ```
  </Accordion>
  
  <Accordion title="Q5: Explain O_DIRECT and when to use it">
    **Answer:**
    
    **Normal I/O path**:
    ```
    Application ←→ Page Cache ←→ Disk
    
    read():
    1. Check page cache
    2. If miss, read from disk to cache
    3. Copy from cache to user buffer
    
    write():
    1. Copy to page cache
    2. Mark page dirty
    3. Background writeback to disk
    ```
    
    **O_DIRECT I/O path**:
    ```
    Application ←→ Disk (bypass page cache)
    
    read()/write():
    1. Transfer directly between user buffer and disk
    2. No caching, no copy
    ```
    
    **Requirements for O_DIRECT**:
    - Buffer must be aligned (typically 512 bytes or 4KB)
    - Offset must be aligned
    - Size must be aligned
    
    **When to use O_DIRECT**:
    
    ✅ **Good use cases**:
    - Databases (manage own buffer pool)
    - Video streaming (no reread, waste cache)
    - Large sequential scans (would pollute cache)
    - When you need predictable latency
    
    ❌ **Bad use cases**:
    - General file access
    - Small random reads (cache helps)
    - When kernel caching is beneficial
    
    **Example** (PostgreSQL):
    ```c
    // PG uses shared_buffers (its own cache)
    // Can enable O_DIRECT to avoid double-caching
    fd = open(filename, O_RDWR | O_DIRECT);
    
    // Must use aligned buffer
    posix_memalign(&buf, 4096, 8192);
    pread(fd, buf, 8192, 0);
    ```
  </Accordion>
</AccordionGroup>

---

## Practice Exercises

<Steps>
  <Step title="I/O Scheduler Comparison">
    Use `fio` to benchmark random vs sequential I/O with different schedulers.
  </Step>
  <Step title="io_uring Program">
    Write a file copy program using io_uring. Compare performance with cp.
  </Step>
  <Step title="DMA Simulation">
    Implement a simulation of DMA controller behavior with interrupt generation.
  </Step>
  <Step title="Simple Driver">
    Write a character device driver that logs all read/write operations.
  </Step>
</Steps>

---

## Key Takeaways

<CardGroup cols={2}>
  <Card title="DMA Frees the CPU" icon="microchip">
    Device transfers data directly. CPU only sets up and handles completion.
  </Card>
  <Card title="io_uring is Revolutionary" icon="bolt">
    Batched, zero-copy async I/O. Essential for high-performance systems.
  </Card>
  <Card title="Buffering Matters" icon="layer-group">
    Double buffering enables overlapping I/O and processing.
  </Card>
  <Card title="Scheduler Choice Matters" icon="clock">
    Use none/kyber for NVMe, mq-deadline for HDDs.
  </Card>
</CardGroup>

---

Next: [Synchronization](/operating-systems/synchronization) →
