---
title: "CPU Scheduling"
sidebarTitle: "Scheduling"
description: "Scheduling algorithms, CFS, real-time scheduling, and multi-core considerations"
icon: "clock"
---

# CPU Scheduling: The Mathematics of Execution

The **CPU Scheduler** is the heart of the operating system's process management. It is the arbiter of time, deciding which process/thread gains control of the CPU, for how long, and on which core. In modern high-concurrency environments, a poorly designed scheduler can lead to system-wide jitter, priority inversion, and throughput collapse.

This module provides an exhaustive, senior-level deep dive into scheduling theory, the evolution from MLFQ to CFS, and the emerging EEVDF paradigm.

<Info>
**Prerequisites**: Understanding of [Process Management](/operating-systems/processes), Interrupts, and Basic Data Structures (Trees, Heaps).
**Depth Level**: Senior Engineering Interview & Systems Architecture.
</Info>

---

## 1. The Core Problem: Resource Contention

Scheduling is fundamentally an optimization problem under constraints. We have $N$ processes and $M$ CPUs (where $N \gg M$). The goal is to maximize "useful" work while satisfying various quality-of-service (QoS) requirements.

### 1.1. The Scheduling Framework
Every scheduler operates within a specific lifecycle:
1.  **Admission**: Which processes are allowed into the "Ready" state?
2.  **Dispatch**: Which process from the "Ready" queue gets the CPU?
3.  **Preemption**: When do we forcibly take the CPU away from a running process?
4.  **Termination/Yield**: How do we clean up or move a process back to waiting?

### 1.2. Little's Law and Queuing Theory
To understand scheduling performance, we must first look at the mathematical foundation: **Little's Law**.

$$L = \lambda W$$

Where:
- **$L$**: Average number of processes in the system (load).
- **$\lambda$**: Average arrival rate of processes.
- **$W$**: Average time a process spends in the system (response time).

**Implication for OS**: If you want to decrease response time ($W$) while keeping the arrival rate ($\lambda$) constant, you *must* decrease the number of processes competing for the CPU ($L$). This is why "Load Balancing" and "Admission Control" are critical.

---

## 2. Scheduling Metrics and Trade-offs

There is no "perfect" scheduler. Every algorithm optimizes for one metric at the expense of others.

| Metric | Focus | Mathematical Target | User Experience |
| :--- | :--- | :--- | :--- |
| **Throughput** | Productivity | Maximize $\sum Jobs / \Delta t$ | System finishes work faster. |
| **Response Time** | Latency | Minimize $T_{first\_response} - T_{arrival}$ | UI feels "snappy" and fluid. |
| **Turnaround Time** | Batch Speed | Minimize $T_{completion} - T_{arrival}$ | Large tasks finish sooner. |
| **Wait Time** | Fairness | Minimize $\sum T_{ready\_queue}$ | No process feels "ignored." |
| **CPU Utilization** | Efficiency | Maximize $T_{busy} / T_{total}$ | Getting your money's worth of hardware. |

### The Paradox of Choice
- **Interactive Systems**: Optimize for **Response Time**.
- **Batch Systems (HPC)**: Optimize for **Throughput**.
- **Real-Time Systems**: Optimize for **Predictability** (Jitter).

---

## 3. Classic Scheduling Algorithms: From Simple to Sophisticated

### 3.1. First-Come, First-Served (FCFS)
The simplest non-preemptive algorithm. Uses a FIFO queue.

**The Convoy Effect**:
If a CPU-bound process ($P_1$) arrives first, followed by ten I/O-bound processes ($P_2 \dots P_{11}$), the I/O-bound processes will wait for the entire duration of $P_1$'s burst. This leads to low device utilization because the I/O devices sit idle while processes wait for the CPU.

### 3.2. Shortest Job First (SJF) & SRTF
**SJF (Non-preemptive)** and **Shortest Remaining Time First (SRTF - Preemptive)** are provably optimal for minimizing average waiting time.

**The Prediction Problem**:
How do we know the length of the next CPU burst? We can't see the future, so we use **Exponential Averaging**:

$$\tau_{n+1} = \alpha t_n + (1 - \alpha) \tau_n$$

- $t_n$: Actual length of the $n^{th}$ burst.
- $\tau_n$: Predicted value for the $n^{th}$ burst.
- $\alpha$: Weight (usually $0.5$).

### 3.3. Round Robin (RR)
The foundation of time-sharing. Each process gets a **Time Quantum ($q$)**.

- **Small $q$**: Great response time, but high **Context Switch Overhead**.
- **Large $q$**: Low overhead, but approaches FCFS behavior (poor response time).
- **Rule of Thumb**: $q$ should be large relative to context switch time (e.g., $10ms$ vs $1\mu s$), and $80\%$ of CPU bursts should be shorter than $q$.

---

## 4. Multi-Level Feedback Queue (MLFQ): The Intelligent Arbiter

MLFQ is the "Gold Standard" for general-purpose OS (macOS, Windows, older Linux). It aims to minimize response time for interactive jobs while also being fair to long-running batch jobs, *without* knowing burst times in advance.

### 4.1. The Five Rules of MLFQ
1.  **Rule 1**: If Priority($A$) > Priority($B$), $A$ runs ($B$ doesn't).
2.  **Rule 2**: If Priority($A$) = Priority($B$), $A$ & $B$ run in RR.
3.  **Rule 3**: When a job enters the system, it is placed at the highest priority.
4.  **Rule 4a**: If a job uses up an entire time slice while running, its priority is **reduced** (it moves down one queue).
5.  **Rule 4b**: If a job gives up the CPU before the time slice is up, it stays at the **same** priority level.

### 4.2. Solving the "Gaming" and "Starvation" Problems
**The Gaming Problem**: A process could run for $99\%$ of a time slice, then issue an I/O to stay at high priority, effectively monopolizing the CPU.
**Solution (Rule 4 Revision - Accounting)**: Once a job uses up its time allotment at a given level (regardless of how many times it gave up the CPU), its priority is reduced.

**The Starvation Problem**: Too many interactive jobs could prevent long-running jobs from ever getting CPU time.
**Solution (Priority Boost)**: After some period $S$, move all jobs in the system to the topmost queue.

---

## 5. Proportional Share Scheduling: Lottery and Stride

Instead of optimizing for response time or turnaround, these schedulers ensure each job gets a certain percentage of the CPU.

### 5.1. Lottery Scheduling
Each process holds a number of **tickets**. The scheduler "picks a random ticket," and the winner runs.
- **Probabilistic Fairness**: Over time, if A has 75 tickets and B has 25, A will get $75\%$ of the CPU.
- **Advantages**: Extremely simple to implement; handles new processes easily.
- **Disadvantages**: Inaccurate over short time scales due to randomness.

### 5.2. Stride Scheduling
The deterministic version of Lottery.
- Each process has a **stride** ($Stride = BigValue / Tickets$).
- Each process has a **pass** value (starts at 0).
- Scheduler runs the process with the **lowest pass**.
- After running, $Pass = Pass + Stride$.

**Result**: Precise proportional sharing even over micro-intervals.

---

## 6. Linux Internals: The Completely Fair Scheduler (CFS)

Since Linux 2.6.23, CFS has been the default. It abandons traditional priority queues for a **Red-Black Tree**.

### 6.1. Virtual Runtime (vruntime)
The heart of CFS. Every process tracks its `vruntime`, which is the amount of time it has spent on the CPU, normalized by its priority.

$$vruntime += delta\_exec \times \frac{NICE\_0\_LOAD}{Weight}$$

- **High Priority (Nice -20)**: `vruntime` increases very slowly.
- **Low Priority (Nice +19)**: `vruntime` increases very quickly.
- **Target**: CFS always picks the process with the **minimum `vruntime`** to run next.

### 6.2. The Red-Black Tree Structure
CFS stores all runnable processes in a Red-Black Tree, keyed by `vruntime`.
- **Selection**: $O(1)$ (the leftmost node is cached as `min_vruntime`).
- **Insertion**: $O(\log N)$.
- **Self-Balancing**: Ensures the tree doesn't become a linked list.

### 6.3. Latency Targeting
Instead of a fixed quantum, CFS uses **Target Latency**.
- If target latency is $20ms$ and there are 4 equal-weight processes, each gets $5ms$.
- If there are 100 processes, $20ms / 100 = 0.2ms$, which might be below the context switch cost.
- **Solution**: `min_granularity` (e.g., $1ms$). If a process is scheduled, it is guaranteed at least this much time.

---

## 7. Multiprocessor Scheduling: Scaling to Cores

Moving from one CPU to many introduces two massive headaches: **Cache Affinity** and **Synchronization**.

### 7.1. Single Queue Multiprocessor Scheduling (SQMS)
One global queue. All CPUs pull from it.
- **Pros**: Perfectly balanced load.
- **Cons**: High lock contention on the global queue; **Zero cache affinity** (Process A might run on CPU 0, then CPU 5, losing all L1/L2 cache data).

### 7.2. Multi-Queue Multiprocessor Scheduling (MQMS)
Each CPU has its own runqueue.
- **Pros**: No lock contention between cores; Excellent cache affinity.
- **Cons**: **Load Imbalance**. CPU 0 might have 100 tasks while CPU 1 is idle.

### 7.3. Load Balancing: Push vs. Pull
Modern kernels (Linux) use MQMS with balancing:
- **Push Migration**: A kernel task periodically checks loads and "pushes" tasks from busy to idle cores.
- **Pull Migration**: An idle core looks at other cores' queues and "pulls" (steals) a task.

---

## 8. Real-Time Scheduling: When "Late" equals "Failure"

Real-time OS (RTOS) must guarantee that tasks meet their deadlines.

### 8.1. Rate Monotonic (RM)
**Static Priority**: Shorter period = Higher priority.
- **Schedulability Bound**: $U \leq n(2^{1/n} - 1)$. For large $n$, this is $\approx 69\%$. If CPU utilization is below this, all deadlines are guaranteed.

### 8.2. Earliest Deadline First (EDF)
**Dynamic Priority**: The task with the closest deadline runs first.
- **Schedulability**: $U \leq 100\%$. Optimal for uniprocessors.

---

## 8. Real-World Workload Stories

Theory is useful, but scheduling only really “sticks” when you can picture concrete systems.

### 8.1 Latency-Sensitive Web Service

- **Scenario**: A multi-threaded HTTP API serving p95 < 100 ms.
- **Workload**: Short CPU bursts, heavy I/O (database, caches), thousands of concurrent connections.
- **Scheduling Implications**:
  - Favor **low latency** over maximum throughput.
  - Too many worker threads lead to context switch storms and cache thrash.
  - Use **CFS** with reasonable `ulimit -u` / thread counts and tune `net.core.somaxconn` and backlog handling.
- **Design pattern**:
  - Use a **fixed-size worker pool** (bounded threads) + non-blocking I/O.
  - Keep CPU utilization in the 60–80% range under peak to absorb bursts.

### 8.2 Batch Analytics Cluster

- **Scenario**: Spark/Hadoop jobs running for minutes to hours.
- **Workload**: CPU-bound phases, long-running tasks, predictable throughput goals.
- **Scheduling Implications**:
  - Optimize for **throughput** and fairness across tenants, not per-request latency.
  - Use **cgroups** and **nice** values to prevent one user from monopolizing CPUs.
  - Longer time slices (higher `sched_latency_ns`) can reduce context switch overhead.

### 8.3 Low-Latency Trading / Audio Processing

- **Scenario**: Missed deadlines translate directly to financial loss or audible glitches.
- **Workload**: Small, periodic tasks with very tight deadlines.
- **Scheduling Implications**:
  - Use **real-time scheduling classes** (`SCHED_FIFO`, `SCHED_RR`) for critical threads.
  - Pin threads to specific cores and **isolate** those cores from general-purpose workloads (`isolcpus`, `nohz_full`).
  - Minimize kernel preemption and disable frequency scaling jitter where necessary.

For each system you build, ask: *Which of these patterns am I closest to?* Then map that back to **which scheduler knobs** you need to care about (priorities, affinities, cgroups, RT vs CFS).

---

## 9. The Future: EEVDF (Earliest Eligible Virtual Deadline First)
As of Linux 6.6, CFS is being replaced by **EEVDF**. 
- **The Flaw in CFS**: CFS is "fair" over long periods but can have high "lag" (short-term unfairness).
- **EEVDF Logic**: Processes have a "virtual deadline." The scheduler picks the one with the earliest deadline that is also "eligible" (hasn't overstayed its welcome). This provides much tighter latency bounds for audio and high-frequency trading applications.

---

## 10. Advanced Interview Questions

<AccordionGroup>
  <Accordion title="Q1: Why does CFS use a Red-Black Tree instead of a Heap?">
    A Heap (Priority Queue) has $O(1)$ for `find-min`, but $O(\log N)$ for `delete-min` and $O(\log N)$ for `insert`. However, a Heap is not efficient for **updating** a value or **deleting** a specific node (which happens when a process blocks). 
    
    A Red-Black Tree allows us to find the `min_vruntime` in $O(1)$ (by caching), but also provides $O(\log N)$ for searching/deleting arbitrary nodes (e.g., when a process is killed or changes its nice value).
  </Accordion>

  <Accordion title="Q2: Explain Priority Inversion and how a scheduler helps.">
    Priority Inversion occurs when a High-priority task (H) is waiting for a lock held by a Low-priority task (L), but a Medium-priority task (M) keeps L from running.
    
    **Solution: Priority Inheritance**. The scheduler temporarily boosts L's priority to H's level so L can finish its work, release the lock, and let H proceed.
  </Accordion>

  <Accordion title="Q3: What is the 'Convoy Effect' in FCFS?">
    The Convoy Effect occurs when several short processes wait for one long, CPU-bound process to finish. This leads to poor utilization of I/O devices (which wait for the short processes to trigger them) and poor average response time.
  </Accordion>
</AccordionGroup>

---

## Practice Lab: Building a Mini-Scheduler

**Objective**: Implement a Stride Scheduler in C.

```c
#include <stdio.h>
#include <limits.h>

typedef struct {
    int pid;
    int tickets;
    int stride;
    int pass;
} Process;

#define BIG_STRIDE 10000

void schedule(Process proc[], int n, int iterations) {
    for (int i = 0; i < iterations; i++) {
        // Find process with lowest pass
        int best = 0;
        for (int j = 1; j < n; j++) {
            if (proc[j].pass < proc[best].pass) best = j;
        }

        printf("Iteration %d: Running PID %d (Pass %d)\n", i, proc[best].pid, proc[best].pass);
        proc[best].pass += proc[best].stride;
    }
}

int main() {
    Process proc[] = {
        {1, 100, BIG_STRIDE/100, 0}, // 50% CPU
        {2, 50, BIG_STRIDE/50, 0},   // 25% CPU
        {3, 50, BIG_STRIDE/50, 0}    // 25% CPU
    };
    schedule(proc, 3, 12);
    return 0;
}
```

---

Next: [Memory Management](/operating-systems/memory-management) →
