---
title: "Containers & Virtualization"
sidebarTitle: "Containers & VMs"
description: "Namespaces, cgroups, containers, hypervisors, and virtualization technologies"
icon: "cubes"
---

# Containers & Virtualization

**Virtualization** and **containerization** are foundational technologies for modern cloud computing. Understanding these concepts is essential for senior engineers designing scalable, isolated, and efficient systems.

<Info>
**Interview Frequency**: Very High  
**Key Topics**: Namespaces, cgroups, Docker internals, hypervisors  
**Time to Master**: 12-15 hours
</Info>

---

## Virtualization Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                VIRTUALIZATION SPECTRUM                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Less Isolation                              More Isolation    │
│   Lower Overhead                              Higher Overhead   │
│   ◄───────────────────────────────────────────────────────────► │
│                                                                  │
│   ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐           │
│   │ Process │  │Container│  │   VM    │  │Hardware │           │
│   │         │  │         │  │         │  │Partition│           │
│   └─────────┘  └─────────┘  └─────────┘  └─────────┘           │
│                                                                  │
│   Examples:     Docker       KVM          IBM LPAR             │
│   chroot        LXC          VMware       Oracle LDOM          │
│   jails         Podman       Hyper-V                            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Linux Namespaces

**Namespaces** provide isolation of global system resources. Each namespace type isolates a different aspect of the system.

### Namespace Types

```
┌─────────────────────────────────────────────────────────────────┐
│                    LINUX NAMESPACES                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Namespace   │ Isolates                   │ Since               │
│  ────────────┼────────────────────────────┼──────────────────   │
│  Mount (mnt) │ Mount points               │ Linux 2.4.19 (2002) │
│  UTS         │ Hostname, domain name      │ Linux 2.6.19 (2006) │
│  IPC         │ SysV IPC, POSIX queues     │ Linux 2.6.19 (2006) │
│  PID         │ Process IDs                │ Linux 2.6.24 (2008) │
│  Network     │ Network stack              │ Linux 2.6.29 (2009) │
│  User        │ User/group IDs             │ Linux 3.8 (2013)    │
│  Cgroup      │ Cgroup root directory      │ Linux 4.6 (2016)    │
│  Time        │ System time                │ Linux 5.6 (2020)    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### PID Namespace

```
┌─────────────────────────────────────────────────────────────────┐
│                    PID NAMESPACE                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Host PID Namespace                                             │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  PID 1 (init/systemd)                                    │   │
│   │  PID 2 (kthreadd)                                        │   │
│   │  ...                                                     │   │
│   │  PID 1000 ──────────────────┐                            │   │
│   │                             │                            │   │
│   │    Container PID Namespace  │                            │   │
│   │    ┌────────────────────────┴───────────────────────┐   │   │
│   │    │  PID 1 (container's init)  ← Same as host 1000 │   │   │
│   │    │  PID 2 (app process)       ← Host sees PID 1001│   │   │
│   │    │  PID 3 (worker)            ← Host sees PID 1002│   │   │
│   │    └────────────────────────────────────────────────┘   │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                  │
│   Key points:                                                    │
│   • Container sees its own PID 1                                │
│   • Host sees container processes with different PIDs           │
│   • Container can't see host processes                          │
│   • Nested namespaces possible                                  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Network Namespace

```
┌─────────────────────────────────────────────────────────────────┐
│                    NETWORK NAMESPACE                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Host Network Namespace                                         │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  eth0: 192.168.1.100                                     │   │
│   │  docker0: 172.17.0.1 (bridge)                            │   │
│   │           │                                               │   │
│   │           │ veth pair                                     │   │
│   │           │                                               │   │
│   │   ┌───────┴─────────────────────────────────────────┐   │   │
│   │   │  Container Network Namespace                     │   │   │
│   │   │  eth0: 172.17.0.2                               │   │   │
│   │   │  lo: 127.0.0.1                                  │   │   │
│   │   │                                                 │   │   │
│   │   │  • Own routing table                            │   │   │
│   │   │  • Own iptables rules                           │   │   │
│   │   │  • Own network interfaces                       │   │   │
│   │   └─────────────────────────────────────────────────┘   │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                  │
│   Connection types:                                              │
│   • veth pair (virtual ethernet)                                │
│   • Bridge (docker0)                                            │
│   • macvlan (direct physical interface)                         │
│   • ipvlan (IP-based isolation)                                 │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Mount Namespace

```bash
# Each container has its own mount namespace
# Container sees only its own filesystem tree

# View mount namespace
ls -la /proc/self/ns/mnt

# Create new mount namespace
unshare --mount /bin/bash

# In new namespace, mounts are private
mount -t tmpfs tmpfs /mnt/test  # Only visible in this namespace
```

### Creating Namespaces

```c
#include <sched.h>
#include <unistd.h>

int main() {
    // Create new namespaces
    // CLONE_NEWPID | CLONE_NEWNET | CLONE_NEWNS | CLONE_NEWUTS
    
    if (unshare(CLONE_NEWPID | CLONE_NEWNET | CLONE_NEWNS) == -1) {
        perror("unshare");
        return 1;
    }
    
    // Fork to become PID 1 in new namespace
    if (fork() == 0) {
        // Child is now PID 1 in new PID namespace
        sethostname("container", 9);
        execl("/bin/bash", "bash", NULL);
    }
    
    return 0;
}
```

```bash
# Command-line namespace creation
# Create and enter new namespaces
unshare --pid --net --mount --uts --fork /bin/bash

# Enter existing namespace
nsenter --target <PID> --pid --net --mount /bin/bash

# List namespaces
lsns

# View process's namespaces
ls -la /proc/<PID>/ns/
```

---

## Control Groups (cgroups)

**cgroups** limit, account for, and isolate resource usage (CPU, memory, disk I/O, network) of process groups.

### cgroup v1 vs v2

```
┌─────────────────────────────────────────────────────────────────┐
│                    CGROUP VERSIONS                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  cgroup v1 (Legacy)           │  cgroup v2 (Unified)            │
│  ─────────────────────────────│──────────────────────────────   │
│  Multiple hierarchies         │  Single unified hierarchy       │
│  Per-controller mount         │  All controllers together       │
│  Complex, inconsistent        │  Simpler, consistent            │
│  /sys/fs/cgroup/<controller>  │  /sys/fs/cgroup/                │
│                               │                                  │
│  v1 structure:                │  v2 structure:                  │
│  /sys/fs/cgroup/              │  /sys/fs/cgroup/                │
│  ├── cpu/                     │  ├── cgroup.controllers        │
│  │   └── docker/              │  ├── cgroup.subtree_control    │
│  │       └── container1/      │  ├── system.slice/             │
│  ├── memory/                  │  │   └── docker.service/       │
│  │   └── docker/              │  └── user.slice/               │
│  │       └── container1/      │      └── user-1000.slice/      │
│  └── ...                      │                                  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### cgroup Controllers

```
┌─────────────────────────────────────────────────────────────────┐
│                    CGROUP CONTROLLERS                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Controller │ Controls                  │ Key Files             │
│  ───────────┼───────────────────────────┼─────────────────────  │
│  cpu        │ CPU time allocation       │ cpu.max, cpu.weight   │
│  cpuset     │ CPU/memory node affinity  │ cpuset.cpus           │
│  memory     │ Memory limits             │ memory.max, memory.   │
│             │                           │ current               │
│  io         │ Block I/O limits          │ io.max, io.weight     │
│  pids       │ Process count limits      │ pids.max              │
│  rdma       │ RDMA resource limits      │ rdma.max              │
│  hugetlb    │ HugeTLB usage            │ hugetlb.max           │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Using cgroups

```bash
# cgroup v2 examples

# Create a cgroup
mkdir /sys/fs/cgroup/mygroup

# Enable controllers for children
echo "+cpu +memory +io" > /sys/fs/cgroup/cgroup.subtree_control

# Set CPU limit (100ms per 1 second = 10% of one CPU)
echo "100000 1000000" > /sys/fs/cgroup/mygroup/cpu.max

# Set memory limit (500MB)
echo "500M" > /sys/fs/cgroup/mygroup/memory.max

# Add process to cgroup
echo $$ > /sys/fs/cgroup/mygroup/cgroup.procs

# View current usage
cat /sys/fs/cgroup/mygroup/memory.current
cat /sys/fs/cgroup/mygroup/cpu.stat
```

### systemd and cgroups

```bash
# systemd uses cgroups extensively
systemctl status docker.service

# View cgroup for a service
systemd-cgls

# Resource limits in service file
[Service]
CPUQuota=50%           # Limit to 50% of one CPU
MemoryMax=1G           # Limit to 1GB RAM
IOWeight=100           # I/O priority (1-10000)
TasksMax=100           # Max processes

# View resource usage
systemctl show docker.service --property=CPUUsageNSec
systemctl show docker.service --property=MemoryCurrent

# Real-time resource view
systemd-cgtop
```

---

## Container Architecture

### How Containers Work

```
┌─────────────────────────────────────────────────────────────────┐
│                    CONTAINER ARCHITECTURE                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Container = Namespaces + cgroups + Layered Filesystem         │
│                                                                  │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                    Container                             │   │
│   │  ┌─────────────────────────────────────────────────┐    │   │
│   │  │              Application Process                 │    │   │
│   │  └─────────────────────────────────────────────────┘    │   │
│   │                         │                               │   │
│   │  ┌──────────────────────┴──────────────────────────┐   │   │
│   │  │           Container Runtime (runc)              │   │   │
│   │  │                                                  │   │   │
│   │  │  ┌────────────┐  ┌────────────┐  ┌───────────┐ │   │   │
│   │  │  │ Namespaces │  │  cgroups   │  │ Seccomp   │ │   │   │
│   │  │  │ - PID      │  │ - CPU      │  │ - Syscall │ │   │   │
│   │  │  │ - Net      │  │ - Memory   │  │   filter  │ │   │   │
│   │  │  │ - Mount    │  │ - I/O      │  │           │ │   │   │
│   │  │  │ - User     │  │ - PIDs     │  │           │ │   │   │
│   │  │  └────────────┘  └────────────┘  └───────────┘ │   │   │
│   │  └─────────────────────────────────────────────────┘   │   │
│   │                                                         │   │
│   │  ┌─────────────────────────────────────────────────┐   │   │
│   │  │          Layered Filesystem (OverlayFS)         │   │   │
│   │  │  ┌──────────────────────────────────────────┐   │   │   │
│   │  │  │ Container Layer (writable)               │   │   │   │
│   │  │  └──────────────────────────────────────────┘   │   │   │
│   │  │  ┌──────────────────────────────────────────┐   │   │   │
│   │  │  │ Image Layer 3 (read-only)                │   │   │   │
│   │  │  └──────────────────────────────────────────┘   │   │   │
│   │  │  ┌──────────────────────────────────────────┐   │   │   │
│   │  │  │ Image Layer 2 (read-only)                │   │   │   │
│   │  │  └──────────────────────────────────────────┘   │   │   │
│   │  │  ┌──────────────────────────────────────────┐   │   │   │
│   │  │  │ Image Layer 1 / Base (read-only)         │   │   │   │
│   │  │  └──────────────────────────────────────────┘   │   │   │
│   │  └─────────────────────────────────────────────────┘   │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Container Runtime Stack

```
┌─────────────────────────────────────────────────────────────────┐
│                    CONTAINER RUNTIME STACK                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   High-Level Runtime (Docker CLI, Kubernetes)                   │
│              ↓                                                   │
│   Container Engine (dockerd, containerd, CRI-O)                 │
│              ↓                                                   │
│   Low-Level Runtime (runc, crun, kata-runtime)                  │
│              ↓                                                   │
│   Linux Kernel (namespaces, cgroups, seccomp)                   │
│                                                                  │
│   Example: Docker stack                                          │
│   ┌────────────────────────────────────────────────────────┐    │
│   │ docker run nginx                                        │    │
│   └───────────────────────────┬────────────────────────────┘    │
│                               ▼                                  │
│   ┌────────────────────────────────────────────────────────┐    │
│   │ dockerd (Docker daemon)                                 │    │
│   │ - Image management                                      │    │
│   │ - Networking                                            │    │
│   │ - Volumes                                               │    │
│   └───────────────────────────┬────────────────────────────┘    │
│                               ▼                                  │
│   ┌────────────────────────────────────────────────────────┐    │
│   │ containerd                                              │    │
│   │ - Container lifecycle                                   │    │
│   │ - Image distribution                                    │    │
│   └───────────────────────────┬────────────────────────────┘    │
│                               ▼                                  │
│   ┌────────────────────────────────────────────────────────┐    │
│   │ runc (OCI runtime)                                      │    │
│   │ - Creates namespaces                                    │    │
│   │ - Sets up cgroups                                       │    │
│   │ - Applies seccomp profile                               │    │
│   │ - Executes container process                            │    │
│   └────────────────────────────────────────────────────────┘    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Building a Container from Scratch

```c
// Minimal container implementation
#define _GNU_SOURCE
#include <sched.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/mount.h>
#include <sys/wait.h>

#define STACK_SIZE (1024 * 1024)
static char child_stack[STACK_SIZE];

int child_fn(void *arg) {
    // Set hostname
    sethostname("container", 9);
    
    // Create new root filesystem
    chroot("/path/to/rootfs");
    chdir("/");
    
    // Mount proc (needed for ps, top, etc.)
    mount("proc", "/proc", "proc", 0, NULL);
    
    // Execute shell
    char *args[] = {"/bin/sh", NULL};
    execv("/bin/sh", args);
    
    return 1;
}

int main() {
    int flags = CLONE_NEWPID    // New PID namespace
              | CLONE_NEWNET    // New network namespace
              | CLONE_NEWNS     // New mount namespace
              | CLONE_NEWUTS    // New UTS namespace
              | CLONE_NEWIPC;   // New IPC namespace
    
    pid_t child_pid = clone(child_fn, 
                            child_stack + STACK_SIZE,
                            flags | SIGCHLD, 
                            NULL);
    
    if (child_pid == -1) {
        perror("clone");
        exit(1);
    }
    
    waitpid(child_pid, NULL, 0);
    return 0;
}
```

---

## Virtual Machines

### Hypervisor Types

```
┌─────────────────────────────────────────────────────────────────┐
│                    HYPERVISOR TYPES                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Type 1 (Bare-metal)              Type 2 (Hosted)              │
│   ────────────────────             ────────────────             │
│   Runs directly on hardware        Runs on host OS              │
│                                                                  │
│   ┌─────────┐ ┌─────────┐          ┌─────────┐ ┌─────────┐     │
│   │   VM1   │ │   VM2   │          │   VM1   │ │   VM2   │     │
│   │ (Guest) │ │ (Guest) │          │ (Guest) │ │ (Guest) │     │
│   └────┬────┘ └────┬────┘          └────┬────┘ └────┬────┘     │
│        │           │                    │           │           │
│   ┌────┴───────────┴────┐          ┌────┴───────────┴────┐     │
│   │     Hypervisor      │          │     Hypervisor      │     │
│   │  (VMware ESXi,      │          │  (VirtualBox,       │     │
│   │   Hyper-V, Xen)     │          │   VMware Workstation)│     │
│   └─────────────────────┘          └──────────┬──────────┘     │
│              │                                │                 │
│   ┌──────────┴──────────┐          ┌──────────┴──────────┐     │
│   │      Hardware       │          │      Host OS        │     │
│   └─────────────────────┘          └──────────┬──────────┘     │
│                                    ┌──────────┴──────────┐     │
│                                    │      Hardware       │     │
│                                    └─────────────────────┘     │
│                                                                  │
│   Examples:                        Examples:                    │
│   • VMware ESXi                    • VirtualBox                 │
│   • Microsoft Hyper-V              • VMware Workstation         │
│   • Xen                            • Parallels                  │
│   • KVM (with QEMU)                • QEMU (user mode)           │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### KVM (Kernel-based Virtual Machine)

```
┌─────────────────────────────────────────────────────────────────┐
│                    KVM ARCHITECTURE                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                    Guest VM                              │   │
│   │   ┌─────────────────────────────────────────────────┐   │   │
│   │   │               Guest OS (Linux/Windows)          │   │   │
│   │   └─────────────────────────────────────────────────┘   │   │
│   │   ┌─────────────────────────────────────────────────┐   │   │
│   │   │               Virtual Hardware                   │   │   │
│   │   │   vCPU, vRAM, vNIC, vDisk                       │   │   │
│   │   └─────────────────────────────────────────────────┘   │   │
│   └─────────────────────────────────────────────────────────┘   │
│                               │                                  │
│                               ▼                                  │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                 QEMU (User Space)                        │   │
│   │   • Device emulation (virtio, IDE, etc.)                │   │
│   │   • VM management                                        │   │
│   │   • Migration support                                    │   │
│   └───────────────────────────┬─────────────────────────────┘   │
│                               │ ioctl                            │
│                               ▼                                  │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                 KVM (Kernel Module)                      │   │
│   │   • Creates /dev/kvm                                     │   │
│   │   • CPU virtualization (VT-x/AMD-V)                     │   │
│   │   • Memory virtualization (EPT/NPT)                     │   │
│   │   • Interrupt handling                                   │   │
│   └───────────────────────────┬─────────────────────────────┘   │
│                               │                                  │
│                               ▼                                  │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                 Hardware                                 │   │
│   │   • VT-x (Intel) / AMD-V                                │   │
│   │   • EPT / NPT (memory virtualization)                   │   │
│   │   • VT-d / IOMMU (device passthrough)                   │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Hardware Virtualization Extensions

```
┌─────────────────────────────────────────────────────────────────┐
│                    CPU VIRTUALIZATION                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Without VT-x (Binary Translation):                            │
│   Guest privileged code ──► Trap ──► Hypervisor emulates       │
│   Slow! Must translate every privileged instruction            │
│                                                                  │
│   With VT-x/AMD-V:                                               │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │  Root Mode (VMX root)     │  Non-Root Mode (VMX non-root)│   │
│   │  Hypervisor runs here     │  Guest runs here             │   │
│   │                           │                               │   │
│   │  Ring 0: Hypervisor       │  Ring 0: Guest kernel        │   │
│   │  Ring 3: (unused)         │  Ring 3: Guest userspace     │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                  │
│   VMCS (Virtual Machine Control Structure):                     │
│   • Guest state (registers, VMCS fields)                        │
│   • Host state (restore on VM exit)                             │
│   • VM-execution controls                                        │
│   • VM-exit controls                                             │
│                                                                  │
│   Key operations:                                                │
│   • VMXON: Enable VMX mode                                       │
│   • VMLAUNCH: Start VM                                           │
│   • VMRESUME: Resume VM after exit                               │
│   • VM Exit: Guest → Hypervisor (on sensitive operation)        │
│   • VM Entry: Hypervisor → Guest                                │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Memory Virtualization

```
┌─────────────────────────────────────────────────────────────────┐
│                    MEMORY VIRTUALIZATION                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Without EPT (Shadow Page Tables):                             │
│                                                                  │
│   Guest VA ──► Guest PT ──► Guest PA (hypervisor maintains     │
│                                        shadow → Host PA)        │
│                                                                  │
│   Problem: Shadow tables must be synced with guest tables       │
│            Every guest PT update requires hypervisor trap       │
│                                                                  │
│   ─────────────────────────────────────────────────────────────  │
│                                                                  │
│   With EPT/NPT (Nested Page Tables):                            │
│                                                                  │
│   Guest VA ──► Guest PT ──► Guest PA ──► EPT ──► Host PA       │
│                                                                  │
│   ┌─────────┐     ┌─────────────┐     ┌────────────┐           │
│   │ Guest   │────►│ Guest Page  │────►│ EPT (Nested│───► Host  │
│   │ Virtual │     │   Tables    │     │ Page Tables│    Physical│
│   │ Address │     │ (in guest)  │     │ (in HW)    │   Address │
│   └─────────┘     └─────────────┘     └────────────┘           │
│                                                                  │
│   Benefits:                                                      │
│   • No shadow table maintenance                                  │
│   • Fewer VM exits                                               │
│   • Near-native memory performance                               │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Containers vs VMs

```
┌─────────────────────────────────────────────────────────────────┐
│                CONTAINERS VS VIRTUAL MACHINES                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Containers                      Virtual Machines               │
│   ┌─────┐ ┌─────┐ ┌─────┐        ┌─────┐ ┌─────┐              │
│   │App A│ │App B│ │App C│        │App A│ │App B│              │
│   ├─────┤ ├─────┤ ├─────┤        ├─────┤ ├─────┤              │
│   │Bins │ │Bins │ │Bins │        │Bins │ │Bins │              │
│   │Libs │ │Libs │ │Libs │        │Libs │ │Libs │              │
│   └──┬──┘ └──┬──┘ └──┬──┘        ├─────┤ ├─────┤              │
│      │       │       │           │Guest│ │Guest│              │
│   ┌──┴───────┴───────┴──┐        │ OS  │ │ OS  │              │
│   │  Container Runtime  │        └──┬──┘ └──┬──┘              │
│   └──────────┬──────────┘           │       │                  │
│   ┌──────────┴──────────┐        ┌──┴───────┴──┐              │
│   │      Host OS        │        │  Hypervisor │              │
│   └──────────┬──────────┘        └──────┬──────┘              │
│   ┌──────────┴──────────┐        ┌──────┴──────┐              │
│   │      Hardware       │        │   Hardware  │              │
│   └─────────────────────┘        └─────────────┘              │
│                                                                  │
│   Comparison:                                                    │
│   ────────────────────────────────────────────────────────────  │
│                        │ Containers    │ VMs                    │
│   ─────────────────────┼───────────────┼──────────────────────  │
│   Boot time            │ Seconds       │ Minutes                │
│   Image size           │ MBs           │ GBs                    │
│   Performance          │ Near-native   │ ~2-10% overhead        │
│   Isolation            │ Process-level │ Hardware-level         │
│   OS support           │ Same kernel   │ Any OS                 │
│   Density              │ 100s per host │ 10s per host           │
│   Security             │ Weaker        │ Stronger               │
│   Portability          │ High          │ Medium                 │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### When to Use Each

| Use Case | Recommendation |
|----------|----------------|
| Microservices | Containers |
| Dev/Test environments | Containers |
| CI/CD pipelines | Containers |
| Multi-tenant with untrusted code | VMs |
| Running Windows on Linux | VMs |
| Legacy applications | VMs |
| Maximum isolation | VMs |
| Serverless functions | Containers or microVMs |

---

## Hybrid: MicroVMs and Kata Containers

```
┌─────────────────────────────────────────────────────────────────┐
│                    MICROVM ARCHITECTURE                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   MicroVMs (Firecracker, Cloud Hypervisor):                     │
│   • VM isolation with container-like speed                       │
│   • Minimal device model (only essentials)                       │
│   • Boot in ~125ms                                               │
│   • Memory footprint ~5MB per VM                                │
│                                                                  │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                    Firecracker VMM                       │   │
│   │   • Written in Rust (memory safe)                        │   │
│   │   • Minimal attack surface                               │   │
│   │   • Only virtio-net, virtio-block, serial                │   │
│   │   • No PCI, USB, graphics                                │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                  │
│   Kata Containers:                                               │
│   • OCI-compatible runtime                                       │
│   • Uses hypervisor for isolation                               │
│   • Runs standard container images                               │
│   • Compatible with Kubernetes                                   │
│                                                                  │
│   ┌──────────────────────────────────────────────────────────┐  │
│   │  docker run / kubectl                                     │  │
│   └───────────────────────────┬──────────────────────────────┘  │
│                               ▼                                  │
│   ┌──────────────────────────────────────────────────────────┐  │
│   │  containerd with Kata runtime                             │  │
│   └───────────────────────────┬──────────────────────────────┘  │
│                               ▼                                  │
│   ┌──────────────────────────────────────────────────────────┐  │
│   │  Lightweight VM (QEMU/Firecracker/Cloud Hypervisor)      │  │
│   │  ┌──────────────────────────────────────────────────┐   │  │
│   │  │  Minimal Linux kernel + kata-agent               │   │  │
│   │  │  Container processes run inside VM               │   │  │
│   │  └──────────────────────────────────────────────────┘   │  │
│   └──────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Interview Questions

<AccordionGroup>
  <Accordion title="How do containers provide isolation?">
    **Answer:**
    Containers use multiple Linux kernel features:
    
    1. **Namespaces** - Isolate system resources:
       - PID: Separate process tree
       - Network: Own network stack
       - Mount: Own filesystem view
       - User: Separate UID/GID mapping
       
    2. **cgroups** - Limit resources:
       - CPU time
       - Memory
       - I/O bandwidth
       - Number of processes
       
    3. **Seccomp** - Filter system calls
    4. **Capabilities** - Fine-grained privileges
    5. **OverlayFS** - Layered filesystem
  </Accordion>
  
  <Accordion title="What is the difference between a container and a VM?">
    **Key differences:**
    
    | Aspect | Container | VM |
    |--------|-----------|-----|
    | Isolation | Process-level (shared kernel) | Hardware-level (own kernel) |
    | Boot time | Seconds | Minutes |
    | Overhead | Minimal | 2-10% |
    | Size | MBs | GBs |
    | Security | Weaker isolation | Stronger isolation |
    
    Containers share the host kernel; VMs have their own kernel.
    Use VMs when you need different OS or stronger isolation.
    Use containers for fast, lightweight deployment.
  </Accordion>
  
  <Accordion title="How does Docker networking work?">
    **Answer:**
    
    Docker uses network namespaces and virtual ethernet (veth) pairs:
    
    1. **Bridge mode (default)**:
       - docker0 bridge on host
       - Each container gets a veth pair
       - One end in container, one on bridge
       - NAT for external access
    
    2. **Host mode**:
       - Container shares host network namespace
       - No isolation, but no overhead
    
    3. **Overlay**:
       - Multi-host networking
       - VXLAN encapsulation
       - Used by Docker Swarm/Kubernetes
  </Accordion>
  
  <Accordion title="Explain how cgroups work">
    **Answer:**
    
    cgroups (control groups) limit and account for resource usage:
    
    1. **Hierarchy**: Tree structure of process groups
    2. **Controllers**: cpu, memory, io, pids, etc.
    3. **Limits**: Set via pseudo-filesystem (`/sys/fs/cgroup`)
    
    Example: Limit container to 1 CPU and 512MB RAM:
    ```bash
    echo "100000 100000" > /sys/fs/cgroup/mycontainer/cpu.max
    echo "512M" > /sys/fs/cgroup/mycontainer/memory.max
    ```
    
    cgroup v2 unified hierarchy is now preferred over v1's multiple hierarchies.
  </Accordion>
</AccordionGroup>

---

## Summary

```
┌─────────────────────────────────────────────────────────────────┐
│            CONTAINERS & VIRTUALIZATION SUMMARY                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Namespaces:                                                     │
│  • PID, Network, Mount, User, UTS, IPC, Cgroup, Time            │
│  • Provide resource isolation                                    │
│                                                                  │
│  cgroups:                                                        │
│  • Limit CPU, memory, I/O, PIDs                                 │
│  • v2 unified hierarchy preferred                               │
│                                                                  │
│  Containers = Namespaces + cgroups + Layered FS                 │
│  • Fast, lightweight, shared kernel                              │
│  • Process-level isolation                                       │
│                                                                  │
│  VMs = Hypervisor + Full OS                                     │
│  • Hardware-level isolation                                      │
│  • Any OS, stronger security                                     │
│  • More overhead                                                 │
│                                                                  │
│  Hybrid (MicroVMs):                                              │
│  • Firecracker, Kata Containers                                 │
│  • VM isolation, container speed                                 │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```
