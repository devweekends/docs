---
title: "The Advanced Storage Stack: Block I/O & NVMe"
sidebarTitle: "Storage Stack"
description: "A deep dive into the Block I/O layer, Bio structures, Multi-queue scheduling, and NVMe internals"
icon: "database"
---

# The Advanced Storage Stack

When an application calls `write()`, it begins a journey through multiple kernel layers: the Virtual File System (VFS), the Page Cache, the Block Layer, and finally the hardware driver. For high-performance systems, understanding how the kernel batches, schedules, and dispatches these requests to modern SSDs is critical.

<Info>
**Mastery Level**: Senior Storage Engineer  
**Key Internals**: `struct bio`, `blk-mq`, NVMe SQ/CQ, I/O Schedulers (Kyber/BFQ)  
**Prerequisites**: [File Systems](/operating-systems/file-systems), [Device Drivers](/operating-systems/device-drivers)
</Info>

---

## 1. The Block I/O Layer: The `bio` and `request`

The Block Layer's job is to translate filesystem-level operations into hardware-level disk commands.

### 1.1 The `struct bio`
The `bio` is the basic unit of I/O in the kernel. It represents an I/O operation that is in flight.
- **Segments**: A `bio` can point to multiple non-contiguous memory pages (Scatter-Gather) that should be written to contiguous sectors on disk.
- **Lifecycle**: Created by the filesystem, passed to the block layer, and finally completed by the driver.

### 1.2 The `struct request`
To improve efficiency, the kernel doesn't send every `bio` to the disk immediately. It groups one or more `bio`s into a `request`.
- **Merging**: If two `bio`s target adjacent sectors, they are merged into a single `request` to reduce the number of commands sent to the hardware.

---

## 2. Multiqueue Block I/O (`blk-mq`)

In the era of HDDs, the kernel used a single request queue protected by a single lock. With modern SSDs that can handle millions of IOPS, this lock became a massive bottleneck.

### 2.1 The `blk-mq` Architecture
Modern Linux uses a two-level queueing system:
1. **Software Staging Queues**: Per-CPU queues where requests are initially placed. This eliminates lock contention between CPUs.
2. **Hardware Dispatch Queues**: These map to the actual hardware queues of the device (e.g., NVMe queues).

---

## 3. I/O Schedulers: Sorting the Stream

Schedulers decide the order in which requests are sent to the hardware.

| Scheduler | Type | Best For |
|-----------|------|----------|
| **MQ-Deadline** | Fixed | Predictable latency; default for most systems. |
| **Kyber** | Dynamic | High-performance NVMe; prioritizes read latency over write throughput. |
| **BFQ** | Complex | Desktop responsiveness; ensures no process is starved of I/O. |
| **None** | Passthrough | High-end NVMe where the hardware itself is the scheduler. |

---

## 4. NVMe Internals: Performance at Scale

NVMe (Non-Volatile Memory Express) is a protocol designed specifically for NAND flash over the PCIe bus.

### 4.1 Submission and Completion Queues (SQ/CQ)
- **Submission Queue (SQ)**: The driver writes a 64-byte command to a ring buffer in RAM and "rings a doorbell" (MMIO write).
- **Completion Queue (CQ)**: The device performs the I/O via DMA and writes a 16-byte completion entry back to another ring buffer, then triggers an interrupt.

### 4.2 Scaling with MSI-X
NVMe supports up to 64,000 queues. In a multi-core system, each CPU can have its own dedicated SQ/CQ pair. Using **MSI-X interrupts**, the completion interrupt for a specific CPU's queue is delivered directly to that CPU, maximizing cache locality.

---

## 5. Device Mapper: LVM and RAID

The Device Mapper (DM) is a kernel framework that allows for "Virtual Block Devices."
- **LVM (Logical Volume Manager)**: Allows for resizing volumes and thin provisioning.
- **RAID**: Implementing redundancy in software.
- **Multipathing**: Sending I/O over multiple physical cables to the same storage array for redundancy and speed.

---

## 6. End-to-End: From `write()` to NVMe

Follow a single `write(fd, buf, 4096)` call through the entire storage stack:

```
┌─────────────────────────────────────────────────────────────────────┐
│              WRITE() JOURNEY: USER SPACE TO NAND FLASH              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  1. USER SPACE                                                      │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  write(fd, buf, 4096);                                          │ │
│  │  → Triggers syscall: mov rax, 1; syscall                        │ │
│  └──────────────────────────────┬─────────────────────────────────┘ │
│                                 │ SYSCALL                           │
│  2. VFS LAYER                   ▼                                   │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  ksys_write() → vfs_write() → ext4_file_write_iter()            │ │
│  │  • Resolve fd → struct file → inode                             │ │
│  │  • Check permissions, file position                             │ │
│  └──────────────────────────────┬─────────────────────────────────┘ │
│                                 ▼                                   │
│  3. PAGE CACHE                                                      │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  generic_perform_write()                                        │ │
│  │  • Find or allocate page in Page Cache                          │ │
│  │  • Copy 4096 bytes from user buf → kernel page                  │ │
│  │  • Mark page DIRTY                                              │ │
│  │  • Return to user (write "complete" from app's view)            │ │
│  └──────────────────────────────┬─────────────────────────────────┘ │
│                                 │ (later, async)                    │
│  4. WRITEBACK                   ▼                                   │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  bdi_writeback thread wakes (dirty_expire_centisecs)            │ │
│  │  • Collects dirty pages for this inode                          │ │
│  │  • Calls ext4_writepages() → creates struct bio                 │ │
│  └──────────────────────────────┬─────────────────────────────────┘ │
│                                 ▼                                   │
│  5. BLOCK LAYER (blk-mq)                                            │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  submit_bio() → blk_mq_submit_bio()                             │ │
│  │  • bio → request (may merge adjacent bios)                      │ │
│  │  • Place in per-CPU software queue                              │ │
│  │  • I/O scheduler (mq-deadline) orders requests                  │ │
│  │  • Dispatch to hardware queue                                   │ │
│  └──────────────────────────────┬─────────────────────────────────┘ │
│                                 ▼                                   │
│  6. NVMe DRIVER                                                     │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  nvme_queue_rq()                                                │ │
│  │  • Build 64-byte NVMe command (opcode=Write, LBA, length)       │ │
│  │  • Write command to Submission Queue (SQ) ring buffer           │ │
│  │  • Ring doorbell: writel(sq_tail, doorbell_addr)                │ │
│  └──────────────────────────────┬─────────────────────────────────┘ │
│                                 │ PCIe MMIO                         │
│  7. NVMe CONTROLLER             ▼                                   │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  • Controller reads command from SQ via DMA                     │ │
│  │  • Fetches data from host RAM via DMA (scatter-gather)          │ │
│  │  • Writes to NAND flash (FTL handles wear leveling)             │ │
│  │  • Posts 16-byte completion entry to CQ                         │ │
│  │  • Triggers MSI-X interrupt                                     │ │
│  └──────────────────────────────┬─────────────────────────────────┘ │
│                                 ▼                                   │
│  8. COMPLETION                                                      │
│  ┌────────────────────────────────────────────────────────────────┐ │
│  │  nvme_irq() → nvme_process_cq()                                 │ │
│  │  • Read completion entry from CQ                                │ │
│  │  • Mark bio complete: bio_endio()                               │ │
│  │  • Clear dirty flag on page                                     │ │
│  │  • Update cq_head doorbell                                      │ │
│  └────────────────────────────────────────────────────────────────┘ │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### Key Latency Points

| Stage | Typical Latency | Bottleneck |
|-------|-----------------|------------|
| Syscall entry | ~100 ns | Mode switch |
| Page Cache copy | ~500 ns | Memory bandwidth |
| Block layer | ~1 μs | Software queuing |
| NVMe command | ~10-100 μs | Flash access |
| **Total (cached)** | **< 1 μs** | Returns after Page Cache |
| **Total (fsync)** | **~50-200 μs** | Waits for NVMe completion |

### Tracing It Yourself

```bash
# Trace the full path with bpftrace
sudo bpftrace -e '
  tracepoint:syscalls:sys_enter_write { @start[tid] = nsecs; }
  kprobe:submit_bio { printf("bio submitted: %d us\n", (nsecs - @start[tid])/1000); }
  kprobe:nvme_queue_rq { printf("nvme queued: %d us\n", (nsecs - @start[tid])/1000); }
'

# Watch block I/O in real-time
sudo biosnoop   # from bcc-tools
```

---

## 7. Interview Deep Dive: Senior Level

<AccordionGroup>
  <Accordion title="What is 'Write Amplification' and how does the OS mitigate it?">
    Write Amplification occurs because SSDs can only overwrite data by erasing entire "Blocks" (which are larger than "Pages"). If the OS modifies a small file, the SSD might have to read, erase, and rewrite a huge block. 
    
    The OS mitigates this using the **TRIM** command. When a file is deleted, the OS tells the SSD: "These sectors are no longer needed." The SSD's garbage collector can then skip those sectors, reducing unnecessary writes and extending the drive's life.
  </Accordion>

  <Accordion title="Explain the difference between 'Synchronous' and 'Asynchronous' I/O from a kernel perspective.">
    - **Synchronous (`read`/`write`)**: The calling process is put into "Uninterruptible Sleep" (D state) until the hardware interrupt confirms the I/O is complete.
    - **Asynchronous (`io_uring` / `AIO`)**: The process submits a request and continues executing immediately. It later checks a completion queue or receives a signal when the data is ready. This is critical for high-concurrency servers.
  </Accordion>

  <Accordion title="How does `fsync()` work across the storage stack?">
    When you call `fsync()`:
    1. VFS identifies all dirty pages in the Page Cache for that file.
    2. The Block Layer creates `bio`s with the `REQ_PREFLUSH` and `REQ_FUA` flags.
    3. The Driver sends a specialized command to the disk telling it to flush its **Internal Volatile Cache** to the persistent NAND. 
    4. The call only returns once the hardware confirms the data is physically on the platter/flash.
  </Accordion>
</AccordionGroup>

---

## 8. Advanced Practice

1. **Scheduler Investigation**: Run `cat /sys/block/sda/queue/scheduler` to see which I/O scheduler is active for your main disk.
2. **I/O Latency Profiling**: Use the `biolatency` tool from the BCC/ebpf suite to see a histogram of how long your disk requests are taking.
3. **NVMe Stats**: Use the `nvme-cli` tool (`nvme smart-log /dev/nvme0`) to check the health and total data written to your SSD.

---
Next: [Modern Observability with eBPF](/operating-systems/ebpf) →
