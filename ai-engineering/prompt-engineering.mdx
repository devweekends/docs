---
title: "Prompt Engineering"
description: "Master the art and science of crafting prompts that get reliable, high-quality outputs"
icon: "wand-magic-sparkles"
---

<Info>
**December 2025 Update**: Covers chain-of-thought, few-shot learning, system prompts, and the latest prompting techniques from OpenAI and Anthropic research.
</Info>

## Why Prompts Matter

The difference between a junior and senior AI engineer often comes down to prompt engineering. A well-crafted prompt can:
- Turn a $0.10 GPT-4o call into a $0.001 GPT-4o-mini call
- Reduce hallucinations by 90%
- Get structured, predictable outputs every time

<Note>
**The 80/20 Rule**: 80% of prompt quality comes from clear instructions and examples. The remaining 20% is advanced techniques.
</Note>

## The Anatomy of a Great Prompt

```
┌─────────────────────────────────────────────────────────────┐
│                      SYSTEM PROMPT                          │
│  • Role/Persona definition                                  │
│  • Capabilities and constraints                             │
│  • Output format requirements                               │
│  • Rules and guidelines                                     │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                      FEW-SHOT EXAMPLES                      │
│  • 2-5 input/output pairs                                   │
│  • Cover edge cases                                         │
│  • Show exact format expected                               │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                      USER INPUT                             │
│  • Clear, specific request                                  │
│  • Relevant context included                                │
│  • Output format reminder (optional)                        │
└─────────────────────────────────────────────────────────────┘
```

## System Prompts: Your AI's DNA

### Basic Structure

```python
SYSTEM_PROMPT = """You are an expert {role} with deep knowledge of {domain}.

## Your Capabilities
- {capability_1}
- {capability_2}
- {capability_3}

## Rules
1. Always {rule_1}
2. Never {rule_2}
3. When uncertain, {uncertainty_behavior}

## Output Format
{format_specification}
"""
```

### Production System Prompt

```python
CODE_REVIEW_PROMPT = """You are a senior software engineer performing code review.

## Your Expertise
- Python, JavaScript, TypeScript, Go
- Clean code principles and SOLID
- Security best practices
- Performance optimization

## Review Process
1. First, understand the code's purpose
2. Check for bugs and logic errors
3. Evaluate code quality and readability
4. Identify security vulnerabilities
5. Suggest performance improvements

## Rules
- Be constructive, not critical
- Explain WHY something is an issue
- Provide specific, actionable fixes
- Praise good patterns when you see them
- If code is good, say so briefly

## Output Format
Return a JSON object:
{
  "summary": "One-line summary of the code quality",
  "issues": [
    {
      "severity": "critical|major|minor|suggestion",
      "line": <line_number or null>,
      "issue": "Description of the problem",
      "fix": "Suggested solution with code"
    }
  ],
  "positive": ["List of things done well"],
  "score": <1-10>
}
"""
```

## Few-Shot Learning

### Why Few-Shot Works

LLMs learn patterns from examples. 2-5 examples can:
- Define exact output format
- Show edge case handling
- Reduce ambiguity dramatically

### Few-Shot Template

```python
def create_few_shot_prompt(task: str, examples: list[dict], query: str) -> str:
    prompt = f"Task: {task}\n\n"
    prompt += "Examples:\n"
    
    for i, ex in enumerate(examples, 1):
        prompt += f"\nExample {i}:\n"
        prompt += f"Input: {ex['input']}\n"
        prompt += f"Output: {ex['output']}\n"
    
    prompt += f"\nNow complete this:\nInput: {query}\nOutput:"
    return prompt

# Example: Sentiment Analysis
examples = [
    {"input": "This product is amazing!", "output": "positive"},
    {"input": "Terrible experience, want refund", "output": "negative"},
    {"input": "It's okay, nothing special", "output": "neutral"},
    {"input": "Love the design but shipping was slow", "output": "mixed"},
]

prompt = create_few_shot_prompt(
    task="Classify the sentiment of the review",
    examples=examples,
    query="Best purchase I've made this year, highly recommend!"
)
```

## Chain-of-Thought (CoT)

### The Problem

LLMs often fail at multi-step reasoning when asked to jump straight to the answer.

### The Solution

Force the model to "show its work" before answering.

```python
# ❌ Bad: Direct answer
prompt = "What is 23 * 47 + 156 / 4?"

# ✅ Good: Chain of thought
prompt = """What is 23 * 47 + 156 / 4?

Let's solve this step by step:
1. First, calculate 23 * 47
2. Then, calculate 156 / 4
3. Finally, add the results

Show your work:"""
```

### Zero-Shot CoT

Just add "Let's think step by step" to any prompt:

```python
REASONING_SUFFIX = "\n\nLet's approach this step by step:"

def add_cot(prompt: str) -> str:
    return prompt + REASONING_SUFFIX
```

### Structured CoT

```python
COT_PROMPT = """
{question}

## Analysis Framework
1. **Understand**: What is being asked?
2. **Identify**: What information do we have?
3. **Plan**: What steps are needed?
4. **Execute**: Work through each step
5. **Verify**: Does the answer make sense?

## Solution
"""
```

## Advanced Techniques

### Self-Consistency

Run the same prompt multiple times and take the majority answer:

```python
from collections import Counter
from openai import OpenAI

client = OpenAI()

def self_consistent_answer(prompt: str, n: int = 5) -> str:
    """Generate multiple answers and return the most common one"""
    answers = []
    
    for _ in range(n):
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7  # Some randomness needed
        )
        answers.append(response.choices[0].message.content.strip())
    
    # Return most common answer
    counter = Counter(answers)
    return counter.most_common(1)[0][0]
```

### Prompt Chaining

Break complex tasks into sequential prompts:

```python
async def research_and_write(topic: str) -> str:
    """Chain: Research → Outline → Write → Edit"""
    
    # Step 1: Research
    research = await llm_call(f"""
    Research the topic: {topic}
    List 5-7 key points with sources.
    """)
    
    # Step 2: Outline
    outline = await llm_call(f"""
    Based on this research:
    {research}
    
    Create a detailed article outline with sections and subsections.
    """)
    
    # Step 3: Write
    draft = await llm_call(f"""
    Write a comprehensive article following this outline:
    {outline}
    
    Use the research for accuracy. Target: 1500 words.
    """)
    
    # Step 4: Edit
    final = await llm_call(f"""
    Edit this article for clarity, flow, and engagement:
    {draft}
    
    Fix any errors. Improve transitions. Make it compelling.
    """)
    
    return final
```

### Role Prompting

Assign specific expertise for better outputs:

```python
EXPERT_ROLES = {
    "security": "You are a cybersecurity expert with 15 years of experience at Google. You've reviewed thousands of codebases for vulnerabilities.",
    
    "performance": "You are a performance engineer who optimized systems handling 1M+ requests/second at Netflix. You think in terms of latency percentiles and resource efficiency.",
    
    "architecture": "You are a principal architect who designed microservices at scale for Amazon. You balance pragmatism with technical excellence.",
    
    "ml": "You are a machine learning researcher from DeepMind. You understand both theoretical foundations and practical implementation details."
}

def expert_review(code: str, expertise: str) -> str:
    role = EXPERT_ROLES.get(expertise, "You are a senior software engineer.")
    return f"{role}\n\nReview this code:\n```\n{code}\n```"
```

### Constitutional AI (Self-Critique)

Have the model critique and improve its own output:

```python
def constitutional_response(query: str, principles: list[str]) -> str:
    # Initial response
    response = llm_call(query)
    
    # Critique against principles
    critique_prompt = f"""
    Original query: {query}
    Response: {response}
    
    Evaluate this response against these principles:
    {chr(10).join(f'- {p}' for p in principles)}
    
    What could be improved?
    """
    critique = llm_call(critique_prompt)
    
    # Revise based on critique
    revision_prompt = f"""
    Original response: {response}
    Critique: {critique}
    
    Provide an improved response addressing the critique.
    """
    
    return llm_call(revision_prompt)

# Usage
principles = [
    "Be helpful and accurate",
    "Avoid harmful content", 
    "Acknowledge uncertainty",
    "Cite sources when possible"
]
```

## Prompt Templates Library

### Summarization

```python
SUMMARIZE_PROMPT = """Summarize the following text in {length} sentences.

Focus on:
- Main arguments/findings
- Key data points
- Actionable conclusions

Text:
{text}

Summary:"""
```

### Data Extraction

```python
EXTRACT_PROMPT = """Extract structured data from this text.

Text: {text}

Extract the following fields (use null if not found):
{fields}

Return as JSON:"""
```

### Classification

```python
CLASSIFY_PROMPT = """Classify the following into one of these categories: {categories}

Guidelines:
{guidelines}

Text: {text}

Category:"""
```

### Translation with Context

```python
TRANSLATE_PROMPT = """Translate the following from {source_lang} to {target_lang}.

Context: {context}
Tone: {tone}
Domain: {domain}

Original: {text}

Translation:"""
```

## Debugging Prompts

### Common Issues and Fixes

| Problem | Cause | Solution |
|---------|-------|----------|
| Too verbose | No length constraint | Add "in X sentences" or "max Y words" |
| Wrong format | Ambiguous instructions | Add few-shot examples |
| Hallucinations | Asking for unknown facts | Add "If unsure, say 'I don't know'" |
| Inconsistent | High temperature | Set temperature=0 for determinism |
| Off-topic | Weak system prompt | Add explicit constraints |

### Prompt Testing Framework

```python
from dataclasses import dataclass
from typing import Callable

@dataclass
class PromptTest:
    name: str
    input: str
    expected_contains: list[str] = None
    expected_not_contains: list[str] = None
    validator: Callable[[str], bool] = None

def test_prompt(prompt_template: str, tests: list[PromptTest]) -> dict:
    results = {"passed": 0, "failed": 0, "details": []}
    
    for test in tests:
        prompt = prompt_template.format(input=test.input)
        response = llm_call(prompt)
        
        passed = True
        errors = []
        
        if test.expected_contains:
            for phrase in test.expected_contains:
                if phrase.lower() not in response.lower():
                    passed = False
                    errors.append(f"Missing: {phrase}")
        
        if test.expected_not_contains:
            for phrase in test.expected_not_contains:
                if phrase.lower() in response.lower():
                    passed = False
                    errors.append(f"Should not contain: {phrase}")
        
        if test.validator and not test.validator(response):
            passed = False
            errors.append("Custom validation failed")
        
        results["passed" if passed else "failed"] += 1
        results["details"].append({
            "test": test.name,
            "passed": passed,
            "errors": errors
        })
    
    return results
```

## Key Takeaways

<CardGroup cols={2}>
  <Card title="Be Specific" icon="bullseye">
    Vague prompts get vague answers. Specify format, length, tone, and constraints.
  </Card>
  <Card title="Show, Don't Tell" icon="images">
    Few-shot examples are worth a thousand words of instruction.
  </Card>
  <Card title="Think in Steps" icon="stairs">
    Chain-of-thought improves reasoning. Break complex tasks into chains.
  </Card>
  <Card title="Test and Iterate" icon="rotate">
    Prompts need testing like code. Build a test suite for critical prompts.
  </Card>
</CardGroup>

## What's Next

<Card title="OpenAI API" icon="arrow-right" href="/ai-engineering/openai-api">
  Apply your prompt engineering skills with the OpenAI API
</Card>
