---
title: "OpenAI API"
description: "Master the OpenAI API: chat completions, function calling, and structured outputs"
icon: "bolt"
---

## Getting Started

```python
from openai import OpenAI

client = OpenAI()  # Uses OPENAI_API_KEY env variable

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)

print(response.choices[0].message.content)
```

## Chat Completions

### Basic Usage

```python
from openai import OpenAI

client = OpenAI()

def chat(user_message: str, system_prompt: str = None) -> str:
    """Simple chat completion"""
    messages = []
    
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.append({"role": "user", "content": user_message})
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.7
    )
    
    return response.choices[0].message.content

# Usage
answer = chat(
    "Explain recursion",
    system_prompt="You are a programming tutor. Use simple analogies."
)
```

### Streaming Responses

```python
def chat_stream(user_message: str):
    """Stream response tokens as they're generated"""
    stream = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": user_message}],
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)

chat_stream("Write a short poem about coding")
```

### Conversation History

```python
class Conversation:
    def __init__(self, system_prompt: str = None):
        self.messages = []
        if system_prompt:
            self.messages.append({"role": "system", "content": system_prompt})
    
    def chat(self, user_message: str) -> str:
        self.messages.append({"role": "user", "content": user_message})
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=self.messages
        )
        
        assistant_message = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": assistant_message})
        
        return assistant_message

# Maintains context across turns
conv = Conversation("You are a helpful assistant")
print(conv.chat("My name is Alex"))
print(conv.chat("What's my name?"))  # Remembers "Alex"
```

## Function Calling

Let the model call your functions to take actions or retrieve data.

### Basic Function Calling

```python
import json

# Define available functions
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City name, e.g., 'London'"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "default": "celsius"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

# Your actual function implementation
def get_weather(location: str, unit: str = "celsius") -> dict:
    # In real app, call weather API
    return {"location": location, "temperature": 22, "unit": unit, "condition": "sunny"}

# Make the API call
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the weather in Tokyo?"}],
    tools=tools,
    tool_choice="auto"
)

# Check if model wants to call a function
message = response.choices[0].message

if message.tool_calls:
    for tool_call in message.tool_calls:
        function_name = tool_call.function.name
        arguments = json.loads(tool_call.function.arguments)
        
        # Execute the function
        if function_name == "get_weather":
            result = get_weather(**arguments)
            
            # Send result back to model
            messages = [
                {"role": "user", "content": "What's the weather in Tokyo?"},
                message,
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(result)
                }
            ]
            
            final_response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
            
            print(final_response.choices[0].message.content)
```

### Multiple Functions

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "search_products",
            "description": "Search for products in the catalog",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Search query"},
                    "category": {"type": "string", "enum": ["electronics", "clothing", "books"]},
                    "max_price": {"type": "number", "description": "Maximum price filter"}
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "add_to_cart",
            "description": "Add a product to the shopping cart",
            "parameters": {
                "type": "object",
                "properties": {
                    "product_id": {"type": "string"},
                    "quantity": {"type": "integer", "default": 1}
                },
                "required": ["product_id"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_cart",
            "description": "Get current cart contents",
            "parameters": {"type": "object", "properties": {}}
        }
    }
]
```

## Structured Outputs

Force the model to return valid JSON matching a schema.

### Using response_format

```python
from pydantic import BaseModel

class ProductReview(BaseModel):
    sentiment: str  # "positive", "negative", "neutral"
    score: float    # 0.0 to 1.0
    summary: str
    key_points: list[str]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "Analyze the product review and extract structured data."},
        {"role": "user", "content": "This laptop is amazing! Fast, lightweight, great battery. Only complaint is the webcam quality."}
    ],
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "product_review",
            "schema": ProductReview.model_json_schema()
        }
    }
)

# Guaranteed valid JSON matching schema
review = ProductReview.model_validate_json(response.choices[0].message.content)
print(review.sentiment)  # "positive"
print(review.key_points)  # ["Fast", "lightweight", "great battery"]
```

### Complex Nested Structures

```python
from pydantic import BaseModel
from typing import Optional

class Address(BaseModel):
    street: str
    city: str
    country: str
    postal_code: Optional[str] = None

class Person(BaseModel):
    name: str
    age: int
    email: str
    address: Address
    skills: list[str]

# Extract structured data from unstructured text
text = """
John Smith is a 32-year-old software engineer living at 
123 Tech Street, San Francisco, USA 94102. 
He's proficient in Python, JavaScript, and cloud computing.
Contact him at john.smith@email.com
"""

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "Extract person information from text."},
        {"role": "user", "content": text}
    ],
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "person_extraction",
            "schema": Person.model_json_schema()
        }
    }
)

person = Person.model_validate_json(response.choices[0].message.content)
```

## Vision (Image Input)

```python
import base64

def encode_image(image_path: str) -> str:
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

# Analyze an image
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image?"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{encode_image('photo.jpg')}"
                    }
                }
            ]
        }
    ]
)

print(response.choices[0].message.content)
```

## Error Handling

```python
from openai import OpenAI, APIError, RateLimitError, APIConnectionError
import time

client = OpenAI()

def robust_chat(message: str, max_retries: int = 3) -> str:
    """Chat with retry logic"""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": message}]
            )
            return response.choices[0].message.content
            
        except RateLimitError:
            wait = 2 ** attempt  # Exponential backoff
            print(f"Rate limited. Waiting {wait}s...")
            time.sleep(wait)
            
        except APIConnectionError:
            print("Connection error. Retrying...")
            time.sleep(1)
            
        except APIError as e:
            print(f"API error: {e}")
            raise
    
    raise Exception("Max retries exceeded")
```

## Cost Optimization

<Tip>
**Save Money:**
1. Use `gpt-4o-mini` for simple tasks ($0.15 vs $2.50 per 1M tokens)
2. Limit `max_tokens` when you know response length
3. Cache common responses
4. Use streaming to fail fast on bad responses
</Tip>

```python
# Cost-aware model selection
def smart_chat(message: str, complexity: str = "low") -> str:
    model = "gpt-4o-mini" if complexity == "low" else "gpt-4o"
    
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": message}],
        max_tokens=500  # Limit output length
    )
    
    return response.choices[0].message.content
```

## Quick Reference

| Feature | Method |
|---------|--------|
| Basic chat | `chat.completions.create()` |
| Streaming | `stream=True` parameter |
| Function calling | `tools` parameter |
| Structured output | `response_format` with JSON schema |
| Vision | Image in message content |
| Temperature | `temperature` (0-2) |

## Next Steps

<Card title="Vector Databases" icon="arrow-right" href="/ai-engineering/vector-databases">
  Learn to store and search embeddings with pgvector and Pinecone
</Card>
