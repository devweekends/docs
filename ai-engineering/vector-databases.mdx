---
title: "Vector Databases"
description: "Store and search embeddings with pgvector, Pinecone, and semantic search"
icon: "database"
---

## What Are Vector Databases?

Vector databases store **embeddings** (numerical representations of text, images, etc.) and enable fast **similarity search**. This is the foundation of RAG systems.

<Note>
**Key Concept**: Instead of keyword matching, vector search finds semantically similar content. "How do I fix a bug?" matches "debugging techniques" even without shared words.
</Note>

## Why Vector Databases?

| Traditional Search | Vector Search |
|-------------------|---------------|
| Exact keyword match | Semantic meaning |
| "bug fix" doesn't match "debugging" | "bug fix" matches "debugging" |
| Boolean queries | Similarity scores |
| Fast for exact matches | Fast for "find similar" |

## pgvector (PostgreSQL)

The easiest way to add vector search to existing PostgreSQL databases.

### Setup

```sql
-- Enable the extension
CREATE EXTENSION vector;

-- Create a table with vector column
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    embedding vector(1536),  -- OpenAI embedding dimension
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Create index for fast similarity search
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

### Python Integration

```python
import psycopg2
from openai import OpenAI

client = OpenAI()

def get_embedding(text: str) -> list[float]:
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# Connect to PostgreSQL
conn = psycopg2.connect("postgresql://user:pass@localhost/mydb")

def store_document(content: str, metadata: dict = None):
    """Store document with its embedding"""
    embedding = get_embedding(content)
    
    with conn.cursor() as cur:
        cur.execute("""
            INSERT INTO documents (content, embedding, metadata)
            VALUES (%s, %s, %s)
            RETURNING id
        """, (content, embedding, metadata))
        
        doc_id = cur.fetchone()[0]
        conn.commit()
        return doc_id

def search_similar(query: str, limit: int = 5) -> list[dict]:
    """Find similar documents using cosine similarity"""
    query_embedding = get_embedding(query)
    
    with conn.cursor() as cur:
        cur.execute("""
            SELECT id, content, metadata,
                   1 - (embedding <=> %s::vector) as similarity
            FROM documents
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (query_embedding, query_embedding, limit))
        
        results = []
        for row in cur.fetchall():
            results.append({
                "id": row[0],
                "content": row[1],
                "metadata": row[2],
                "similarity": row[3]
            })
        
        return results

# Usage
store_document("Python is a programming language", {"type": "definition"})
store_document("JavaScript runs in browsers", {"type": "definition"})
store_document("How to debug Python code effectively", {"type": "tutorial"})

results = search_similar("programming languages for beginners")
for r in results:
    print(f"{r['similarity']:.3f}: {r['content']}")
```

### Filtering with Metadata

```python
def search_with_filter(query: str, doc_type: str = None, limit: int = 5):
    """Search with optional metadata filter"""
    query_embedding = get_embedding(query)
    
    sql = """
        SELECT id, content, metadata,
               1 - (embedding <=> %s::vector) as similarity
        FROM documents
        WHERE 1=1
    """
    params = [query_embedding]
    
    if doc_type:
        sql += " AND metadata->>'type' = %s"
        params.append(doc_type)
    
    sql += " ORDER BY embedding <=> %s::vector LIMIT %s"
    params.extend([query_embedding, limit])
    
    with conn.cursor() as cur:
        cur.execute(sql, params)
        return cur.fetchall()

# Search only tutorials
results = search_with_filter("debugging tips", doc_type="tutorial")
```

## Pinecone

Managed vector database with excellent performance at scale.

### Setup

```python
from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key="your-api-key")

# Create index
pc.create_index(
    name="my-index",
    dimension=1536,
    metric="cosine",
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)

index = pc.Index("my-index")
```

### CRUD Operations

```python
from openai import OpenAI

client = OpenAI()

def get_embedding(text: str) -> list[float]:
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# Upsert vectors
def store_documents(documents: list[dict]):
    """Store multiple documents"""
    vectors = []
    
    for doc in documents:
        embedding = get_embedding(doc["content"])
        vectors.append({
            "id": doc["id"],
            "values": embedding,
            "metadata": {
                "content": doc["content"],
                **doc.get("metadata", {})
            }
        })
    
    index.upsert(vectors=vectors)

# Query
def search(query: str, top_k: int = 5, filter: dict = None):
    """Search similar vectors"""
    query_embedding = get_embedding(query)
    
    results = index.query(
        vector=query_embedding,
        top_k=top_k,
        include_metadata=True,
        filter=filter
    )
    
    return results.matches

# Usage
store_documents([
    {"id": "1", "content": "Python basics for beginners", "metadata": {"level": "beginner"}},
    {"id": "2", "content": "Advanced Python decorators", "metadata": {"level": "advanced"}},
])

# Search with filter
results = search(
    "Python tutorials",
    filter={"level": {"$eq": "beginner"}}
)
```

### Namespaces for Multi-Tenancy

```python
# Store per-user data in separate namespaces
def store_for_user(user_id: str, documents: list[dict]):
    vectors = [
        {
            "id": doc["id"],
            "values": get_embedding(doc["content"]),
            "metadata": doc
        }
        for doc in documents
    ]
    
    index.upsert(vectors=vectors, namespace=user_id)

def search_for_user(user_id: str, query: str):
    query_embedding = get_embedding(query)
    
    return index.query(
        vector=query_embedding,
        top_k=5,
        namespace=user_id,
        include_metadata=True
    )
```

## Chroma (Local/Embedded)

Great for development and smaller applications.

```python
import chromadb
from chromadb.utils import embedding_functions

# Use OpenAI embeddings
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    model_name="text-embedding-3-small"
)

# Create client and collection
client = chromadb.PersistentClient(path="./chroma_db")

collection = client.get_or_create_collection(
    name="documents",
    embedding_function=openai_ef
)

# Add documents (embeddings computed automatically)
collection.add(
    ids=["1", "2", "3"],
    documents=[
        "Python programming basics",
        "JavaScript for web development",
        "Database design principles"
    ],
    metadatas=[
        {"topic": "python"},
        {"topic": "javascript"},
        {"topic": "databases"}
    ]
)

# Query
results = collection.query(
    query_texts=["backend development"],
    n_results=2,
    where={"topic": {"$ne": "javascript"}}  # Filter
)

print(results["documents"])
```

## Comparison

| Feature | pgvector | Pinecone | Chroma |
|---------|----------|----------|--------|
| Type | Extension | Managed | Embedded/Server |
| Setup | Use existing PG | Cloud | pip install |
| Scale | Millions | Billions | Thousands |
| Cost | Free (self-hosted) | Pay per usage | Free |
| Best For | Existing PG apps | Production scale | Development |

## Chunking Strategies

Large documents need to be split into chunks before embedding.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def chunk_document(text: str, chunk_size: int = 1000, overlap: int = 200):
    """Split document into overlapping chunks"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    return splitter.split_text(text)

# Example
document = """Long document content here..."""
chunks = chunk_document(document)

# Store each chunk with reference to parent
for i, chunk in enumerate(chunks):
    store_document(
        content=chunk,
        metadata={
            "doc_id": "doc_123",
            "chunk_index": i,
            "total_chunks": len(chunks)
        }
    )
```

## Best Practices

<AccordionGroup>
  <Accordion title="Choose the Right Embedding Model" icon="brain">
    | Model | Dimensions | Best For |
    |-------|------------|----------|
    | text-embedding-3-small | 1536 | General purpose, cost-effective |
    | text-embedding-3-large | 3072 | Higher accuracy, more expensive |
    | Cohere embed-v3 | 1024 | Multilingual |
  </Accordion>
  
  <Accordion title="Optimize Chunk Size" icon="scissors">
    - **Too small**: Loses context, more storage
    - **Too large**: Less precise matches
    - **Sweet spot**: 500-1500 characters with 10-20% overlap
  </Accordion>
  
  <Accordion title="Use Hybrid Search" icon="magnifying-glass">
    Combine vector search with keyword search for best results:
    
    ```python
    def hybrid_search(query: str):
        # Vector search
        vector_results = search_similar(query)
        
        # Keyword search (BM25)
        keyword_results = full_text_search(query)
        
        # Combine and re-rank
        return reciprocal_rank_fusion(vector_results, keyword_results)
    ```
  </Accordion>
</AccordionGroup>

## Next Steps

<Card title="RAG Systems" icon="arrow-right" href="/ai-engineering/rag">
  Build Retrieval-Augmented Generation pipelines using vector databases
</Card>
