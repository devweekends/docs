---
title: "RAG Systems"
description: "Build Retrieval-Augmented Generation pipelines for knowledge-grounded AI"
icon: "magnifying-glass"
---

## What is RAG?

**Retrieval-Augmented Generation (RAG)** combines LLMs with external knowledge retrieval. Instead of relying solely on training data, the model retrieves relevant documents and uses them to generate accurate, grounded responses.

<Note>
**Why RAG?**
- LLMs have knowledge cutoffs
- Reduces hallucinations with source citations
- Works with private/proprietary data
- No need to fine-tune models
</Note>

## Basic RAG Pipeline

```
User Query → Embed Query → Vector Search → Retrieve Docs → Augment Prompt → LLM → Response
```

### Simple Implementation

```python
from openai import OpenAI
import psycopg2

client = OpenAI()

def get_embedding(text: str) -> list[float]:
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

def retrieve_context(query: str, top_k: int = 3) -> list[str]:
    """Retrieve relevant documents from vector DB"""
    query_embedding = get_embedding(query)
    
    conn = psycopg2.connect("postgresql://user:pass@localhost/mydb")
    with conn.cursor() as cur:
        cur.execute("""
            SELECT content FROM documents
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (query_embedding, top_k))
        
        return [row[0] for row in cur.fetchall()]

def rag_query(user_question: str) -> str:
    """RAG: Retrieve context and generate answer"""
    # Step 1: Retrieve relevant documents
    context_docs = retrieve_context(user_question)
    
    # Step 2: Build augmented prompt
    context = "\n\n".join([f"Document {i+1}:\n{doc}" for i, doc in enumerate(context_docs)])
    
    messages = [
        {
            "role": "system",
            "content": """You are a helpful assistant. Answer questions based on the provided context.
            If the context doesn't contain the answer, say "I don't have information about that."
            Always cite which document(s) you used."""
        },
        {
            "role": "user",
            "content": f"""Context:
{context}

Question: {user_question}

Answer based on the context above:"""
        }
    ]
    
    # Step 3: Generate response
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0
    )
    
    return response.choices[0].message.content

# Usage
answer = rag_query("What are the company's vacation policies?")
print(answer)
```

## Advanced RAG Techniques

### 1. Query Transformation

Improve retrieval by reformulating the user's query:

```python
def expand_query(original_query: str) -> list[str]:
    """Generate multiple query variations for better retrieval"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "Generate 3 alternative phrasings of this search query. Return as JSON array."
            },
            {"role": "user", "content": original_query}
        ],
        response_format={"type": "json_object"}
    )
    
    import json
    variations = json.loads(response.choices[0].message.content)
    return [original_query] + variations.get("queries", [])

def multi_query_retrieve(query: str, top_k: int = 5) -> list[str]:
    """Retrieve using multiple query variations"""
    queries = expand_query(query)
    
    all_docs = set()
    for q in queries:
        docs = retrieve_context(q, top_k=3)
        all_docs.update(docs)
    
    return list(all_docs)[:top_k]
```

### 2. Contextual Compression

Filter retrieved documents to only relevant parts:

```python
def compress_context(query: str, documents: list[str]) -> list[str]:
    """Extract only relevant parts from documents"""
    compressed = []
    
    for doc in documents:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """Extract only the parts of this document that are relevant to the query.
                    If nothing is relevant, respond with "NOT_RELEVANT"."""
                },
                {
                    "role": "user",
                    "content": f"Query: {query}\n\nDocument:\n{doc}"
                }
            ],
            max_tokens=500
        )
        
        result = response.choices[0].message.content
        if result != "NOT_RELEVANT":
            compressed.append(result)
    
    return compressed
```

### 3. Hybrid Search

Combine vector search with keyword search:

```python
def hybrid_search(query: str, top_k: int = 5) -> list[dict]:
    """Combine semantic and keyword search"""
    # Vector search
    query_embedding = get_embedding(query)
    
    conn = psycopg2.connect("postgresql://user:pass@localhost/mydb")
    with conn.cursor() as cur:
        # Vector similarity search
        cur.execute("""
            SELECT id, content, 
                   1 - (embedding <=> %s::vector) as semantic_score
            FROM documents
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (query_embedding, query_embedding, top_k * 2))
        
        vector_results = {row[0]: {"content": row[1], "semantic": row[2]} for row in cur.fetchall()}
        
        # Full-text search
        cur.execute("""
            SELECT id, content,
                   ts_rank(to_tsvector('english', content), plainto_tsquery('english', %s)) as bm25_score
            FROM documents
            WHERE to_tsvector('english', content) @@ plainto_tsquery('english', %s)
            ORDER BY bm25_score DESC
            LIMIT %s
        """, (query, query, top_k * 2))
        
        for row in cur.fetchall():
            if row[0] in vector_results:
                vector_results[row[0]]["bm25"] = row[2]
            else:
                vector_results[row[0]] = {"content": row[1], "bm25": row[2], "semantic": 0}
    
    # Reciprocal Rank Fusion
    def rrf_score(doc):
        k = 60  # RRF constant
        semantic_rank = sorted(vector_results.keys(), key=lambda x: vector_results[x].get("semantic", 0), reverse=True).index(doc) + 1
        bm25_rank = sorted(vector_results.keys(), key=lambda x: vector_results[x].get("bm25", 0), reverse=True).index(doc) + 1
        return 1/(k + semantic_rank) + 1/(k + bm25_rank)
    
    ranked = sorted(vector_results.keys(), key=rrf_score, reverse=True)[:top_k]
    return [vector_results[doc_id] for doc_id in ranked]
```

### 4. Re-ranking

Use a cross-encoder to re-rank retrieved documents:

```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

def rerank_documents(query: str, documents: list[str], top_k: int = 3) -> list[str]:
    """Re-rank documents using cross-encoder"""
    pairs = [[query, doc] for doc in documents]
    scores = reranker.predict(pairs)
    
    # Sort by score
    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in ranked[:top_k]]
```

## RAG with Citations

```python
def rag_with_citations(query: str) -> dict:
    """RAG with source citations"""
    # Retrieve with metadata
    docs = retrieve_with_metadata(query)  # Returns [{content, source, page}, ...]
    
    # Build context with source markers
    context_parts = []
    sources = []
    
    for i, doc in enumerate(docs):
        context_parts.append(f"[Source {i+1}]: {doc['content']}")
        sources.append({"id": i+1, "source": doc["source"], "page": doc.get("page")})
    
    context = "\n\n".join(context_parts)
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": """Answer based on the provided sources. 
                Always cite sources using [Source N] format.
                If unsure, say so."""
            },
            {
                "role": "user",
                "content": f"Sources:\n{context}\n\nQuestion: {query}"
            }
        ]
    )
    
    return {
        "answer": response.choices[0].message.content,
        "sources": sources
    }
```

## Evaluation

### Metrics for RAG

```python
def evaluate_rag(test_cases: list[dict]) -> dict:
    """Evaluate RAG system on test cases"""
    results = {
        "retrieval_precision": [],
        "answer_relevance": [],
        "faithfulness": []
    }
    
    for case in test_cases:
        query = case["query"]
        expected_docs = case["expected_docs"]
        expected_answer = case["expected_answer"]
        
        # Get RAG response
        retrieved = retrieve_context(query)
        answer = rag_query(query)
        
        # Retrieval precision
        relevant_retrieved = len(set(retrieved) & set(expected_docs))
        precision = relevant_retrieved / len(retrieved) if retrieved else 0
        results["retrieval_precision"].append(precision)
        
        # Answer relevance (use LLM as judge)
        relevance = judge_relevance(query, answer)
        results["answer_relevance"].append(relevance)
        
        # Faithfulness (is answer grounded in retrieved docs?)
        faithfulness = judge_faithfulness(answer, retrieved)
        results["faithfulness"].append(faithfulness)
    
    return {k: sum(v)/len(v) for k, v in results.items()}
```

## Common Issues & Solutions

<AccordionGroup>
  <Accordion title="Retrieved documents not relevant" icon="xmark">
    **Solutions:**
    - Improve chunking (smaller, more focused chunks)
    - Use query expansion
    - Add metadata filters
    - Try different embedding models
  </Accordion>
  
  <Accordion title="LLM ignores context" icon="robot">
    **Solutions:**
    - Use stronger prompting ("Only use provided context")
    - Lower temperature
    - Put context closer to the question
    - Use structured prompts
  </Accordion>
  
  <Accordion title="Too much irrelevant context" icon="file-lines">
    **Solutions:**
    - Use contextual compression
    - Reduce top_k
    - Add re-ranking step
    - Filter by similarity threshold
  </Accordion>
  
  <Accordion title="Slow response times" icon="clock">
    **Solutions:**
    - Cache embeddings
    - Use smaller embedding models
    - Parallelize retrieval and generation
    - Pre-compute common queries
  </Accordion>
</AccordionGroup>

## Production RAG Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     User Interface                          │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    API Gateway                              │
│              (Rate limiting, Auth)                          │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                  RAG Orchestrator                           │
│   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │
│   │   Query     │  │  Retrieval  │  │  Response   │        │
│   │ Processing  │──│   Engine    │──│ Generation  │        │
│   └─────────────┘  └─────────────┘  └─────────────┘        │
└─────────────────────────────────────────────────────────────┘
         │                   │                   │
         ▼                   ▼                   ▼
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│   Cache     │      │  Vector DB  │      │   LLM API   │
│   (Redis)   │      │  (pgvector) │      │  (OpenAI)   │
└─────────────┘      └─────────────┘      └─────────────┘
```

## Next Steps

<Card title="AI Agents" icon="arrow-right" href="/ai-engineering/agents">
  Build autonomous agents that can use tools and make decisions
</Card>
