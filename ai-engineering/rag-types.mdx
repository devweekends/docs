---
title: "Types of RAG"
description: "Master different RAG architectures from basic retrieval to advanced agentic and graph-based systems"
icon: "layer-group"
---

<Info>
**December 2025 Update**: Covers the latest RAG patterns including Agentic RAG, Graph RAG, and Multi-Vector approaches used in production systems.
</Info>

## The RAG Evolution

RAG has evolved far beyond simple "retrieve and generate." Modern RAG systems use sophisticated architectures to handle complex queries, maintain context, and deliver accurate, grounded responses.

```
RAG Evolution Timeline
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

2022          2023           2024           2025
 │             │              │              │
 ▼             ▼              ▼              ▼
Basic RAG → Advanced RAG → Agentic RAG → Graph RAG
             + Re-ranking    + Memory       + Knowledge Graphs
             + Hybrid        + Multi-hop    + Multi-Modal
                            + Self-RAG     
```

## 1. Basic RAG

The foundation—retrieve documents based on vector similarity and generate a response.

```python
from openai import OpenAI
from typing import List
import json

class BasicRAG:
    """Simple retrieve-and-generate RAG"""
    
    def __init__(self, database):
        self.client = OpenAI()
        self.db = database
    
    def query(self, question: str, top_k: int = 5) -> str:
        # 1. Embed the question
        embedding = self._embed(question)
        
        # 2. Retrieve similar documents
        docs = self.db.vector_search(embedding, top_k=top_k)
        
        # 3. Build context
        context = "\n\n".join([
            f"[Source {i+1}]: {doc['content']}"
            for i, doc in enumerate(docs)
        ])
        
        # 4. Generate answer
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": """Answer the question based only on the provided sources.
                    Cite sources using [Source N] format."""
                },
                {
                    "role": "user",
                    "content": f"Sources:\n{context}\n\nQuestion: {question}"
                }
            ]
        )
        
        return response.choices[0].message.content
    
    def _embed(self, text: str) -> List[float]:
        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
```

**When to Use Basic RAG:**
- Simple Q&A over documentation
- Small to medium document collections
- Well-formed, specific questions
- MVP/prototype stage

**Limitations:**
- Single retrieval step may miss context
- No query understanding or rewriting
- Limited handling of complex queries
- No reasoning over multiple documents

---

## 2. Advanced RAG

Adds query processing, hybrid search, and re-ranking for improved accuracy.

```python
class AdvancedRAG:
    """RAG with query expansion, hybrid search, and re-ranking"""
    
    def __init__(self, database):
        self.client = OpenAI()
        self.db = database
    
    async def query(self, question: str) -> dict:
        # 1. Query Processing
        processed_queries = await self._process_query(question)
        
        # 2. Hybrid Retrieval (Vector + Keyword)
        all_docs = []
        for q in processed_queries:
            vector_docs = await self._vector_search(q)
            keyword_docs = await self._keyword_search(q)
            all_docs.extend(vector_docs + keyword_docs)
        
        # 3. Reciprocal Rank Fusion
        fused_docs = self._rrf_fusion(all_docs)
        
        # 4. Re-ranking with Cross-Encoder
        reranked_docs = await self._rerank(question, fused_docs[:20])
        
        # 5. Generate with best documents
        answer = await self._generate(question, reranked_docs[:5])
        
        return {
            "answer": answer,
            "sources": reranked_docs[:5],
            "query_expansions": processed_queries
        }
    
    async def _process_query(self, question: str) -> List[str]:
        """Expand query into multiple search variations"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """Generate 3 alternative phrasings of this question to improve search.
                    Return JSON: {"queries": ["...", "...", "..."]}"""
                },
                {"role": "user", "content": question}
            ],
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        return [question] + result.get("queries", [])
    
    def _rrf_fusion(self, docs: List[dict], k: int = 60) -> List[dict]:
        """Reciprocal Rank Fusion for combining multiple rankings"""
        scores = {}
        doc_map = {}
        
        for doc in docs:
            doc_id = doc['id']
            if doc_id not in scores:
                scores[doc_id] = 0
                doc_map[doc_id] = doc
            
            # RRF score: 1 / (k + rank)
            rank = doc.get('rank', 1)
            scores[doc_id] += 1 / (k + rank)
        
        # Sort by fused score
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        return [doc_map[doc_id] for doc_id in sorted_ids]
    
    async def _rerank(self, question: str, docs: List[dict]) -> List[dict]:
        """Re-rank documents using LLM as judge"""
        doc_texts = "\n".join([
            f"[{i+1}] {doc['content'][:500]}"
            for i, doc in enumerate(docs)
        ])
        
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """Rate relevance of each document to the question (0-10).
                    Return JSON: {"rankings": [{"doc": 1, "score": 8}, ...]}"""
                },
                {
                    "role": "user",
                    "content": f"Question: {question}\n\nDocuments:\n{doc_texts}"
                }
            ],
            response_format={"type": "json_object"}
        )
        
        rankings = json.loads(response.choices[0].message.content)
        
        # Apply scores and re-sort
        for ranking in rankings.get("rankings", []):
            idx = ranking["doc"] - 1
            if 0 <= idx < len(docs):
                docs[idx]["rerank_score"] = ranking["score"]
        
        return sorted(docs, key=lambda x: x.get("rerank_score", 0), reverse=True)
```

**Key Improvements over Basic RAG:**
- **Query Expansion**: Multiple query variations improve recall
- **Hybrid Search**: Combines semantic and keyword matching
- **RRF Fusion**: Intelligently merges multiple result sets
- **Re-ranking**: Precise relevance scoring with cross-encoders

---

## 3. Memory RAG

Maintains conversation history and long-term context for personalized responses.

```python
from datetime import datetime
from typing import Optional

class MemoryRAG:
    """RAG with short-term and long-term memory"""
    
    def __init__(self, database, memory_store):
        self.client = OpenAI()
        self.db = database
        self.memory = memory_store
    
    async def query(
        self,
        question: str,
        user_id: str,
        session_id: str
    ) -> dict:
        # 1. Get short-term memory (conversation context)
        conversation = await self.memory.get_conversation(session_id, limit=10)
        
        # 2. Get long-term memory (user preferences, facts)
        user_context = await self.memory.get_user_context(user_id)
        
        # 3. Contextualize the query
        contextualized_query = await self._contextualize_query(
            question, 
            conversation,
            user_context
        )
        
        # 4. Retrieve with contextualized query
        docs = await self._retrieve(contextualized_query)
        
        # 5. Generate with full context
        answer = await self._generate(
            question=question,
            documents=docs,
            conversation=conversation,
            user_context=user_context
        )
        
        # 6. Update memory
        await self._update_memory(
            user_id=user_id,
            session_id=session_id,
            question=question,
            answer=answer
        )
        
        return {"answer": answer, "sources": docs}
    
    async def _contextualize_query(
        self,
        question: str,
        conversation: List[dict],
        user_context: dict
    ) -> str:
        """Rewrite query with conversation context"""
        if not conversation:
            return question
        
        history = "\n".join([
            f"{'User' if m['role'] == 'user' else 'Assistant'}: {m['content']}"
            for m in conversation[-5:]  # Last 5 turns
        ])
        
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """Given the conversation history, rewrite the user's question
                    to be self-contained. Include relevant context from the conversation.
                    Return only the rewritten question."""
                },
                {
                    "role": "user",
                    "content": f"""Conversation:
{history}

User preferences: {json.dumps(user_context.get('preferences', {}))}

Current question: {question}

Rewritten question:"""
                }
            ]
        )
        
        return response.choices[0].message.content
    
    async def _update_memory(
        self,
        user_id: str,
        session_id: str,
        question: str,
        answer: str
    ):
        """Update both short-term and long-term memory"""
        # Short-term: conversation history
        await self.memory.add_message(session_id, "user", question)
        await self.memory.add_message(session_id, "assistant", answer)
        
        # Long-term: extract and store facts
        facts = await self._extract_facts(question, answer)
        for fact in facts:
            await self.memory.add_user_fact(user_id, fact)
    
    async def _extract_facts(self, question: str, answer: str) -> List[dict]:
        """Extract memorable facts from conversation"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """Extract user preferences or facts to remember.
                    Return JSON: {"facts": [{"type": "preference|fact", "content": "..."}]}
                    Only include genuinely useful information. Return empty if nothing notable."""
                },
                {
                    "role": "user",
                    "content": f"User asked: {question}\nResponse: {answer}"
                }
            ],
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        return result.get("facts", [])
```

**Memory Components:**
- **Short-term**: Conversation history for session context
- **Long-term**: User preferences, facts, and learned patterns
- **Query Contextualization**: Resolve pronouns and references

---

## 4. Agentic RAG

Uses iterative reasoning to answer complex, multi-step questions.

```python
from enum import Enum

class Action(Enum):
    SEARCH = "search"
    ANSWER = "answer"
    CLARIFY = "clarify"

class AgenticRAG:
    """RAG with iterative reasoning and multi-hop retrieval"""
    
    def __init__(self, database):
        self.client = OpenAI()
        self.db = database
        self.max_iterations = 5
    
    async def query(self, question: str) -> dict:
        context = []
        reasoning_chain = []
        
        for i in range(self.max_iterations):
            # Decide next action
            action, action_input = await self._plan_action(
                question=question,
                context=context,
                reasoning=reasoning_chain
            )
            
            reasoning_chain.append({
                "step": i + 1,
                "action": action.value,
                "input": action_input
            })
            
            if action == Action.ANSWER:
                # Ready to answer
                return {
                    "answer": action_input,
                    "reasoning": reasoning_chain,
                    "sources": context
                }
            
            elif action == Action.SEARCH:
                # Retrieve more information
                docs = await self._retrieve(action_input)
                context.extend(docs)
                
                reasoning_chain[-1]["result"] = f"Found {len(docs)} documents"
            
            elif action == Action.CLARIFY:
                # Need clarification
                return {
                    "answer": None,
                    "clarification_needed": action_input,
                    "reasoning": reasoning_chain
                }
        
        # Max iterations reached
        return {
            "answer": await self._force_answer(question, context),
            "reasoning": reasoning_chain,
            "sources": context,
            "warning": "Max iterations reached"
        }
    
    async def _plan_action(
        self,
        question: str,
        context: List[dict],
        reasoning: List[dict]
    ) -> tuple[Action, str]:
        """Decide what to do next"""
        context_summary = "\n".join([
            f"- {doc['content'][:200]}..." for doc in context[-5:]
        ]) if context else "No information gathered yet."
        
        reasoning_str = "\n".join([
            f"Step {r['step']}: {r['action']} - {r.get('result', r.get('input', ''))}"
            for r in reasoning
        ]) if reasoning else "No steps taken yet."
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": """You are a reasoning agent. Decide what to do next:

1. SEARCH: Need more information. Provide a search query.
2. ANSWER: Have enough info to answer. Provide the answer.
3. CLARIFY: Question is ambiguous. Provide clarification request.

Return JSON: {"action": "search|answer|clarify", "input": "..."}

Be thorough—search multiple times if needed for complex questions."""
                },
                {
                    "role": "user",
                    "content": f"""Question: {question}

Information gathered:
{context_summary}

Reasoning so far:
{reasoning_str}

What should I do next?"""
                }
            ],
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        action = Action(result["action"])
        return action, result["input"]
    
    async def _retrieve(self, query: str) -> List[dict]:
        """Search for documents"""
        embedding = self._embed(query)
        return self.db.vector_search(embedding, top_k=5)
```

**Agentic RAG Features:**
- **Iterative Reasoning**: Multiple retrieval steps
- **Dynamic Query Generation**: Creates targeted searches
- **Self-Assessment**: Knows when it has enough information
- **Multi-hop Reasoning**: Chains facts across documents

---

## 5. Multi-Vector RAG

Uses multiple embedding types (dense, sparse, metadata) for precise retrieval.

```python
class MultiVectorRAG:
    """RAG with dense, sparse, and metadata vectors"""
    
    def __init__(self, database):
        self.client = OpenAI()
        self.db = database
    
    async def index_document(self, doc_id: str, content: str, metadata: dict):
        """Index with multiple vector types"""
        
        # 1. Dense embedding (semantic)
        dense_embedding = self._embed_dense(content)
        
        # 2. Sparse embedding (BM25/keywords)
        sparse_embedding = self._embed_sparse(content)
        
        # 3. Metadata embedding
        metadata_text = " ".join([
            f"{k}: {v}" for k, v in metadata.items()
        ])
        metadata_embedding = self._embed_dense(metadata_text)
        
        # Store all embeddings
        await self.db.store_multi_vector(
            doc_id=doc_id,
            content=content,
            dense=dense_embedding,
            sparse=sparse_embedding,
            metadata_vec=metadata_embedding,
            metadata=metadata
        )
    
    async def query(
        self,
        question: str,
        filters: dict = None,
        weights: dict = None
    ) -> List[dict]:
        """Multi-vector retrieval with configurable weights"""
        weights = weights or {
            "dense": 0.5,
            "sparse": 0.3,
            "metadata": 0.2
        }
        
        # Embed query
        query_dense = self._embed_dense(question)
        query_sparse = self._embed_sparse(question)
        
        # Search with each vector type
        dense_results = await self.db.search(
            vector=query_dense,
            vector_type="dense",
            filters=filters
        )
        
        sparse_results = await self.db.search(
            vector=query_sparse,
            vector_type="sparse",
            filters=filters
        )
        
        # If metadata filters provided, search metadata vectors
        if filters:
            filter_text = " ".join([f"{k}: {v}" for k, v in filters.items()])
            filter_vec = self._embed_dense(filter_text)
            metadata_results = await self.db.search(
                vector=filter_vec,
                vector_type="metadata"
            )
        else:
            metadata_results = []
        
        # Weighted combination
        combined = self._weighted_fusion(
            dense_results,
            sparse_results,
            metadata_results,
            weights
        )
        
        return combined
    
    def _weighted_fusion(
        self,
        dense: List[dict],
        sparse: List[dict],
        metadata: List[dict],
        weights: dict
    ) -> List[dict]:
        """Combine results with weighted scoring"""
        scores = {}
        docs = {}
        
        for rank, doc in enumerate(dense):
            doc_id = doc['id']
            docs[doc_id] = doc
            scores[doc_id] = scores.get(doc_id, 0) + weights["dense"] / (rank + 1)
        
        for rank, doc in enumerate(sparse):
            doc_id = doc['id']
            docs[doc_id] = doc
            scores[doc_id] = scores.get(doc_id, 0) + weights["sparse"] / (rank + 1)
        
        for rank, doc in enumerate(metadata):
            doc_id = doc['id']
            docs[doc_id] = doc
            scores[doc_id] = scores.get(doc_id, 0) + weights["metadata"] / (rank + 1)
        
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        return [docs[doc_id] for doc_id in sorted_ids]
    
    def _embed_sparse(self, text: str) -> dict:
        """Create sparse BM25-style embedding"""
        # Simplified—use actual BM25 or SPLADE in production
        from collections import Counter
        import re
        
        words = re.findall(r'\w+', text.lower())
        word_counts = Counter(words)
        
        return {
            "indices": list(range(len(word_counts))),
            "values": list(word_counts.values()),
            "tokens": list(word_counts.keys())
        }
```

**Multi-Vector Advantages:**
- **Dense**: Captures semantic meaning
- **Sparse**: Handles exact keyword matches
- **Metadata**: Enables structured filtering
- **Weighted Fusion**: Tunable relevance balance

---

## 6. Graph RAG

Traverses knowledge graphs to follow relationships and discover connected information.

```python
from dataclasses import dataclass
from typing import Set

@dataclass
class Entity:
    id: str
    name: str
    type: str
    properties: dict

@dataclass
class Relationship:
    source: str
    target: str
    type: str
    properties: dict

class GraphRAG:
    """RAG using knowledge graph traversal"""
    
    def __init__(self, graph_db, vector_db):
        self.client = OpenAI()
        self.graph = graph_db
        self.vectors = vector_db
    
    async def query(self, question: str) -> dict:
        # 1. Extract entities from question
        entities = await self._extract_entities(question)
        
        # 2. Find entities in knowledge graph
        graph_entities = []
        for entity in entities:
            matches = await self.graph.find_entity(entity["name"], entity["type"])
            graph_entities.extend(matches)
        
        # 3. Traverse graph to find related information
        subgraph = await self._traverse_graph(graph_entities, depth=2)
        
        # 4. Also do vector search for additional context
        vector_docs = await self._vector_search(question)
        
        # 5. Combine graph and vector context
        context = self._build_context(subgraph, vector_docs)
        
        # 6. Generate answer
        answer = await self._generate(question, context)
        
        return {
            "answer": answer,
            "entities": [e.name for e in graph_entities],
            "relationships": len(subgraph["relationships"]),
            "documents": len(vector_docs)
        }
    
    async def _extract_entities(self, question: str) -> List[dict]:
        """Extract named entities from question"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """Extract named entities from the question.
                    Return JSON: {"entities": [{"name": "...", "type": "person|org|concept|product|..."}]}"""
                },
                {"role": "user", "content": question}
            ],
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        return result.get("entities", [])
    
    async def _traverse_graph(
        self,
        start_entities: List[Entity],
        depth: int = 2
    ) -> dict:
        """Traverse knowledge graph from starting entities"""
        visited: Set[str] = set()
        entities = []
        relationships = []
        
        queue = [(e, 0) for e in start_entities]
        
        while queue:
            entity, current_depth = queue.pop(0)
            
            if entity.id in visited or current_depth > depth:
                continue
            
            visited.add(entity.id)
            entities.append(entity)
            
            # Get relationships
            rels = await self.graph.get_relationships(entity.id)
            
            for rel in rels:
                relationships.append(rel)
                
                # Get connected entity
                connected_id = rel.target if rel.source == entity.id else rel.source
                connected = await self.graph.get_entity(connected_id)
                
                if connected and connected.id not in visited:
                    queue.append((connected, current_depth + 1))
        
        return {"entities": entities, "relationships": relationships}
    
    def _build_context(
        self,
        subgraph: dict,
        vector_docs: List[dict]
    ) -> str:
        """Build context from graph and documents"""
        parts = []
        
        # Add graph context
        parts.append("## Knowledge Graph Context")
        
        # Entities
        parts.append("\n### Entities:")
        for entity in subgraph["entities"]:
            parts.append(f"- **{entity.name}** ({entity.type}): {entity.properties}")
        
        # Relationships
        parts.append("\n### Relationships:")
        for rel in subgraph["relationships"]:
            parts.append(f"- {rel.source} --[{rel.type}]--> {rel.target}")
        
        # Add document context
        parts.append("\n## Document Context")
        for i, doc in enumerate(vector_docs, 1):
            parts.append(f"\n[Document {i}]: {doc['content']}")
        
        return "\n".join(parts)
    
    async def index_with_graph(self, doc_id: str, content: str):
        """Index document and extract graph entities"""
        # 1. Standard vector indexing
        embedding = self._embed(content)
        await self.vectors.store(doc_id, content, embedding)
        
        # 2. Extract entities and relationships
        graph_data = await self._extract_graph(content)
        
        # 3. Add to knowledge graph
        for entity in graph_data["entities"]:
            await self.graph.add_entity(entity)
        
        for rel in graph_data["relationships"]:
            await self.graph.add_relationship(rel)
    
    async def _extract_graph(self, content: str) -> dict:
        """Extract entities and relationships from text"""
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": """Extract entities and relationships from this text.
                    Return JSON:
                    {
                        "entities": [{"id": "unique_id", "name": "...", "type": "...", "properties": {...}}],
                        "relationships": [{"source": "id1", "target": "id2", "type": "WORKS_AT|OWNS|RELATED_TO|..."}]
                    }"""
                },
                {"role": "user", "content": content}
            ],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
```

**Graph RAG Benefits:**
- **Relationship Awareness**: Understands how entities connect
- **Multi-hop Discovery**: Finds indirectly related information
- **Structured Context**: Provides entity-relationship context
- **Complex Queries**: Handles "who works at company that acquired X" type questions

---

## Choosing the Right RAG Type

| RAG Type | Best For | Complexity | Latency |
|----------|----------|------------|---------|
| **Basic** | Simple Q&A, MVPs | Low | Fast |
| **Advanced** | Production systems, diverse queries | Medium | Moderate |
| **Memory** | Chatbots, personalized assistants | Medium | Moderate |
| **Agentic** | Complex, multi-step questions | High | Slow |
| **Multi-Vector** | Hybrid search requirements | Medium | Moderate |
| **Graph** | Connected data, relationship queries | High | Variable |

## Key Takeaways

<CardGroup cols={2}>
  <Card title="Start Simple" icon="seedling">
    Begin with Basic RAG, then add complexity as needed based on failure analysis.
  </Card>
  <Card title="Hybrid is Best" icon="code-merge">
    Combining vector + keyword search outperforms either alone for most use cases.
  </Card>
  <Card title="Memory Matters" icon="brain">
    For conversational AI, memory context dramatically improves response relevance.
  </Card>
  <Card title="Graphs for Relationships" icon="diagram-project">
    When entities and relationships matter, Graph RAG provides structured understanding.
  </Card>
</CardGroup>

## What's Next

<Card title="Tool Calling" icon="arrow-right" href="/ai-engineering/tool-calling">
  Learn how to give LLMs the ability to call functions and external APIs
</Card>
