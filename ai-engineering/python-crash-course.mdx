---
title: "Python Crash Course"
description: "Everything you need to know to build AI applications with Python"
icon: "python"
---

## Python for AI Engineers

This crash course covers exactly what you need for AI engineeringâ€”no fluff, just practical Python.

<Note>
**Already know Python?** Skip to the [Advanced Patterns](#advanced-patterns) section or move to the next module.
</Note>

## Setup

```bash
# Install Python 3.11+ (recommended for AI work)
# Windows
winget install Python.Python.3.11

# macOS
brew install python@3.11

# Verify installation
python --version  # Should be 3.11+
```

### Virtual Environments

Always use virtual environments for projects:

```bash
# Create virtual environment
python -m venv venv

# Activate
# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate

# Install packages
pip install openai langchain fastapi

# Save dependencies
pip freeze > requirements.txt

# Install from requirements
pip install -r requirements.txt
```

## Core Syntax

### Variables and Types

```python
# Type hints (use them - they help with AI code)
name: str = "GPT-4"
temperature: float = 0.7
max_tokens: int = 1000
is_streaming: bool = True
tags: list[str] = ["ai", "llm", "chat"]
config: dict[str, any] = {"model": "gpt-4", "temp": 0.7}

# None handling
response: str | None = None

# Constants (convention: UPPERCASE)
API_BASE_URL = "https://api.openai.com/v1"
DEFAULT_MODEL = "gpt-4o"
```

### Strings and F-Strings

```python
# F-strings (essential for prompts)
user_name = "Alice"
question = "What is AI?"

prompt = f"""You are a helpful assistant.

User: {user_name}
Question: {question}

Please provide a detailed answer."""

# Multi-line strings for system prompts
system_prompt = """You are an expert AI engineer.
You help users understand complex AI concepts.
Always provide code examples when relevant."""

# String methods
text = "  Hello, World!  "
text.strip()          # "Hello, World!"
text.lower()          # "  hello, world!  "
text.replace(",", "") # "  Hello World!  "
"AI" in text          # False

# Join and split
tags = ["python", "ai", "ml"]
", ".join(tags)       # "python, ai, ml"
"a,b,c".split(",")    # ["a", "b", "c"]
```

### Collections

```python
# Lists
messages = []
messages.append({"role": "user", "content": "Hello"})
messages.append({"role": "assistant", "content": "Hi!"})

# List comprehension (you'll use this constantly)
tokens = [1, 2, 3, 4, 5]
doubled = [t * 2 for t in tokens]  # [2, 4, 6, 8, 10]

# Filter with comprehension
long_messages = [m for m in messages if len(m["content"]) > 10]

# Dictionaries
config = {
    "model": "gpt-4o",
    "temperature": 0.7,
    "max_tokens": 1000
}

# Access with default
model = config.get("model", "gpt-3.5-turbo")

# Merge dicts
defaults = {"temperature": 0.5, "max_tokens": 500}
overrides = {"temperature": 0.9}
final = {**defaults, **overrides}  # temperature is 0.9

# Dictionary comprehension
word_counts = {"hello": 5, "world": 3, "ai": 10}
filtered = {k: v for k, v in word_counts.items() if v > 4}
```

### Control Flow

```python
# If/elif/else
def get_model_tier(model: str) -> str:
    if "gpt-4" in model:
        return "premium"
    elif "gpt-3.5" in model:
        return "standard"
    else:
        return "unknown"

# Ternary operator
status = "success" if response.ok else "error"

# For loops
for message in messages:
    print(f"{message['role']}: {message['content']}")

# Enumerate (get index)
for i, message in enumerate(messages):
    print(f"{i}: {message['content']}")

# Zip (iterate multiple lists)
questions = ["What is AI?", "How does GPT work?"]
answers = ["AI is...", "GPT works by..."]
for q, a in zip(questions, answers):
    print(f"Q: {q}\nA: {a}")

# While loops
retry_count = 0
while retry_count < 3:
    try:
        response = call_api()
        break
    except Exception:
        retry_count += 1
```

## Functions

### Basic Functions

```python
def create_prompt(
    question: str,
    context: str = "",
    max_length: int = 1000
) -> str:
    """Create a prompt for the LLM.
    
    Args:
        question: The user's question
        context: Optional context to include
        max_length: Maximum prompt length
        
    Returns:
        Formatted prompt string
    """
    prompt = f"Question: {question}"
    if context:
        prompt = f"Context: {context}\n\n{prompt}"
    return prompt[:max_length]

# Call with positional args
prompt = create_prompt("What is AI?")

# Call with keyword args
prompt = create_prompt(
    question="What is AI?",
    context="AI stands for artificial intelligence",
    max_length=500
)
```

### Lambda Functions

```python
# Short inline functions
get_token_count = lambda text: len(text.split())

# Sorting with lambda
messages = [
    {"role": "user", "tokens": 50},
    {"role": "assistant", "tokens": 200},
    {"role": "user", "tokens": 30}
]
sorted_messages = sorted(messages, key=lambda m: m["tokens"])
```

### *args and **kwargs

```python
def call_llm(prompt: str, *examples, **options):
    """
    *examples: Variable positional args (tuple)
    **options: Variable keyword args (dict)
    """
    print(f"Prompt: {prompt}")
    print(f"Examples: {examples}")
    print(f"Options: {options}")

# Usage
call_llm(
    "Translate to French",
    "Hello -> Bonjour",
    "Goodbye -> Au revoir",
    model="gpt-4",
    temperature=0.3
)
```

## Classes

### Basic Classes

```python
from dataclasses import dataclass
from typing import Optional

# Modern Python: Use dataclasses for data containers
@dataclass
class Message:
    role: str
    content: str
    tokens: int = 0
    
# Create instance
msg = Message(role="user", content="Hello")

# Access attributes
print(msg.role)  # "user"

# Traditional class for behavior
class ChatBot:
    def __init__(self, model: str = "gpt-4o"):
        self.model = model
        self.messages: list[Message] = []
        self.total_tokens = 0
    
    def add_message(self, role: str, content: str) -> None:
        msg = Message(role=role, content=content)
        self.messages.append(msg)
    
    def get_history(self) -> list[dict]:
        return [
            {"role": m.role, "content": m.content}
            for m in self.messages
        ]
    
    def clear(self) -> None:
        self.messages = []
        self.total_tokens = 0

# Usage
bot = ChatBot(model="gpt-4o")
bot.add_message("user", "Hello!")
bot.add_message("assistant", "Hi there!")
print(bot.get_history())
```

### Inheritance

```python
from abc import ABC, abstractmethod

class BaseLLM(ABC):
    """Abstract base class for LLM providers"""
    
    @abstractmethod
    def complete(self, prompt: str) -> str:
        pass
    
    @abstractmethod
    def stream(self, prompt: str):
        pass

class OpenAILLM(BaseLLM):
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
    
    def complete(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    
    def stream(self, prompt: str):
        for chunk in self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        ):
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

class AnthropicLLM(BaseLLM):
    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)
    
    def complete(self, prompt: str) -> str:
        response = self.client.messages.create(
            model="claude-3-opus-20240229",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text
```

## Error Handling

```python
import logging
from openai import OpenAI, APIError, RateLimitError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def safe_completion(prompt: str, retries: int = 3) -> str | None:
    """Call OpenAI with retry logic"""
    client = OpenAI()
    
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
            
        except RateLimitError as e:
            logger.warning(f"Rate limited, attempt {attempt + 1}/{retries}")
            time.sleep(2 ** attempt)  # Exponential backoff
            
        except APIError as e:
            logger.error(f"API error: {e}")
            raise
            
        except Exception as e:
            logger.exception(f"Unexpected error: {e}")
            raise
    
    return None

# Custom exceptions
class TokenLimitExceeded(Exception):
    """Raised when prompt exceeds token limit"""
    pass

class ModelNotAvailable(Exception):
    """Raised when requested model is unavailable"""
    pass

def validate_prompt(prompt: str, max_tokens: int = 4000):
    token_count = len(prompt.split()) * 1.3  # Rough estimate
    if token_count > max_tokens:
        raise TokenLimitExceeded(
            f"Prompt has ~{token_count} tokens, max is {max_tokens}"
        )
```

## Async Programming

Essential for AI applications that make many API calls:

```python
import asyncio
from openai import AsyncOpenAI

async def get_completion(client: AsyncOpenAI, prompt: str) -> str:
    """Async completion call"""
    response = await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

async def batch_completions(prompts: list[str]) -> list[str]:
    """Process multiple prompts concurrently"""
    client = AsyncOpenAI()
    
    # Create tasks for all prompts
    tasks = [get_completion(client, p) for p in prompts]
    
    # Run all concurrently
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Handle results
    completions = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"Prompt {i} failed: {result}")
            completions.append(None)
        else:
            completions.append(result)
    
    return completions

# Run async code
async def main():
    prompts = [
        "What is Python?",
        "What is JavaScript?",
        "What is Rust?"
    ]
    results = await batch_completions(prompts)
    for p, r in zip(prompts, results):
        print(f"Q: {p}\nA: {r}\n")

# Entry point
if __name__ == "__main__":
    asyncio.run(main())
```

### Async Generators (Streaming)

```python
async def stream_response(prompt: str):
    """Stream response tokens"""
    client = AsyncOpenAI()
    
    async for chunk in await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    ):
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

async def main():
    async for token in stream_response("Tell me a story"):
        print(token, end="", flush=True)
```

## Advanced Patterns

### Decorators

```python
import functools
import time
from typing import Callable

def retry(max_attempts: int = 3, delay: float = 1.0):
    """Decorator for automatic retries"""
    def decorator(func: Callable):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_error = None
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_error = e
                    time.sleep(delay * (attempt + 1))
            raise last_error
        return wrapper
    return decorator

def timer(func: Callable):
    """Decorator to time function execution"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = time.perf_counter() - start
        print(f"{func.__name__} took {elapsed:.2f}s")
        return result
    return wrapper

def cache_result(func: Callable):
    """Simple caching decorator"""
    cache = {}
    @functools.wraps(func)
    def wrapper(*args):
        if args not in cache:
            cache[args] = func(*args)
        return cache[args]
    return wrapper

# Usage
@retry(max_attempts=3, delay=2.0)
@timer
def call_api(prompt: str) -> str:
    return client.chat.completions.create(...)
```

### Context Managers

```python
from contextlib import contextmanager
import time

@contextmanager
def track_tokens():
    """Track token usage in a block"""
    usage = {"prompt": 0, "completion": 0}
    yield usage
    print(f"Tokens used - Prompt: {usage['prompt']}, Completion: {usage['completion']}")

# Usage
with track_tokens() as usage:
    response = client.chat.completions.create(...)
    usage["prompt"] = response.usage.prompt_tokens
    usage["completion"] = response.usage.completion_tokens

@contextmanager
def timing(label: str):
    """Time a code block"""
    start = time.perf_counter()
    yield
    elapsed = time.perf_counter() - start
    print(f"{label}: {elapsed:.2f}s")

# Usage
with timing("API call"):
    response = client.chat.completions.create(...)
```

### Generators

```python
def chunk_text(text: str, chunk_size: int = 1000):
    """Generate text chunks (memory efficient)"""
    for i in range(0, len(text), chunk_size):
        yield text[i:i + chunk_size]

# Usage
large_document = "..." * 100000
for chunk in chunk_text(large_document):
    embedding = get_embedding(chunk)
    store_embedding(embedding)

def batch_items(items: list, batch_size: int = 100):
    """Batch items for processing"""
    for i in range(0, len(items), batch_size):
        yield items[i:i + batch_size]

# Process embeddings in batches
texts = ["text1", "text2", ...]  # 10000 texts
for batch in batch_items(texts, batch_size=100):
    embeddings = client.embeddings.create(input=batch)
```

### Type Hints with Generics

```python
from typing import TypeVar, Generic, Callable
from dataclasses import dataclass

T = TypeVar('T')

@dataclass
class Result(Generic[T]):
    """Generic result wrapper"""
    success: bool
    data: T | None
    error: str | None

def safe_call(func: Callable[..., T], *args, **kwargs) -> Result[T]:
    """Safely call a function and wrap result"""
    try:
        data = func(*args, **kwargs)
        return Result(success=True, data=data, error=None)
    except Exception as e:
        return Result(success=False, data=None, error=str(e))

# Usage
result = safe_call(client.chat.completions.create, model="gpt-4o", ...)
if result.success:
    print(result.data.choices[0].message.content)
else:
    print(f"Error: {result.error}")
```

## Working with Files

```python
from pathlib import Path
import json

# Pathlib (modern file handling)
project_dir = Path(__file__).parent
data_dir = project_dir / "data"
data_dir.mkdir(exist_ok=True)

# Read/write text
prompt_file = data_dir / "system_prompt.txt"
prompt_file.write_text("You are a helpful assistant.")
system_prompt = prompt_file.read_text()

# Read/write JSON
config_file = data_dir / "config.json"
config = {"model": "gpt-4o", "temperature": 0.7}
config_file.write_text(json.dumps(config, indent=2))
loaded_config = json.loads(config_file.read_text())

# List files
for md_file in data_dir.glob("*.md"):
    print(f"Found: {md_file.name}")

# Check existence
if not config_file.exists():
    print("Config not found!")
```

## Environment Variables

```python
import os
from dotenv import load_dotenv

# Load .env file
load_dotenv()

# Get with validation
def get_required_env(key: str) -> str:
    value = os.getenv(key)
    if not value:
        raise ValueError(f"Missing required env var: {key}")
    return value

# Usage
OPENAI_API_KEY = get_required_env("OPENAI_API_KEY")
DATABASE_URL = get_required_env("DATABASE_URL")
DEBUG = os.getenv("DEBUG", "false").lower() == "true"
```

## Common Libraries for AI

```python
# requirements.txt for AI projects
"""
openai>=1.0.0
anthropic>=0.5.0
langchain>=0.1.0
langchain-openai>=0.0.5
chromadb>=0.4.0
pinecone-client>=3.0.0
tiktoken>=0.5.0
pydantic>=2.0.0
python-dotenv>=1.0.0
httpx>=0.25.0
tenacity>=8.2.0
structlog>=23.0.0
"""

# Install all
# pip install openai anthropic langchain chromadb pydantic python-dotenv
```

## Quick Reference

<CardGroup cols={2}>
  <Card title="Data Classes" icon="database">
    Use `@dataclass` for data containers
  </Card>
  <Card title="Type Hints" icon="code">
    Always use type hints for clarity
  </Card>
  <Card title="Async/Await" icon="bolt">
    Use async for concurrent API calls
  </Card>
  <Card title="F-Strings" icon="text">
    Use f-strings for prompt templates
  </Card>
</CardGroup>

<Note>
**Next Step**: Now that you know Python, continue to [FastAPI Crash Course](/ai-engineering/fastapi-crash-course) to build APIs for your AI applications.
</Note>
