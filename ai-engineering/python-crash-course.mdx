### Lambda Functions (Anonymous Functions)

Lambdas are useful for quick, throwaway functions—often used in sorting, filtering, and mapping.


## Advanced Patterns (For Real-World AI)

### Decorators (Reusable Logic)

Decorators let you add features (like retries, timing, caching) to functions without changing their code. This is powerful for logging, error handling, and performance monitoring in AI pipelines.


### Context Managers (Resource Management)

Context managers help you manage resources (files, API calls, timing) safely and cleanly. They are essential for handling files, network connections, and tracking resource usage in AI workflows.

```python
from contextlib import contextmanager
import time

@contextmanager

def cache_result(func: Callable):
    """Simple caching decorator"""
    cache = {}
    @functools.wraps(func)
    def wrapper(*args):
        if args not in cache:
            cache[args] = func(*args)
        return cache[args]
    return wrapper

# Usage
@retry(max_attempts=3, delay=2.0)
@timer
    def add_message(self, role: str, content: str) -> None:
        msg = Message(role=role, content=content)
```

**Why it matters:**
- Decorators help you DRY (Don’t Repeat Yourself) for cross-cutting concerns like logging, retries, and caching.
- Used in many AI/ML libraries for extensibility.
        self.messages.append(msg)

```

**Why it matters:**
- Context managers ensure resources are released properly (files closed, timers stopped, etc.).
- Used in file I/O, database connections, and tracking resource usage in AI pipelines.
    def get_history(self) -> list[dict]:
        return [
            {"role": m.role, "content": m.content}
            for m in self.messages
        ]

    def clear(self) -> None:
        self.messages = []
        self.total_tokens = 0

# Usage
bot = ChatBot(model="gpt-4o")
bot.add_message("user", "Hello!")
bot.add_message("assistant", "Hi there!")
print(bot.get_history())
```

**Why it matters:**
- Dataclasses reduce boilerplate for data objects.
- Classes help organize code for bots, pipelines, and more.
Why? Keeps dependencies for each project separate. Avoids "it works on my machine" problems.

```bash
# Create a virtual environment
python -m venv venv
# Activate (Windows)
venv\Scripts\activate
# Activate (macOS/Linux)
source venv/bin/activate
```

### 3. Install AI Packages

```bash
pip install openai langchain fastapi
```

### 4. Manage Dependencies

```bash
# Save current packages
pip freeze > requirements.txt
# Install from requirements
pip install -r requirements.txt
```

---


## Python Core Syntax (with AI Context)

### Variables, Types, and Type Hints

Variables are how you store data in Python. Type hints make your code easier to read, debug, and use with tools like VS Code and mypy. They’re especially useful in AI projects where data types can get complex.

```python
# Basic types
name: str = "GPT-4"  # string
temperature: float = 0.7  # float (decimal)
max_tokens: int = 1000  # integer
is_streaming: bool = True  # boolean

# Lists and dicts (with type hints)
tags: list[str] = ["ai", "llm", "chat"]  # list of strings
config: dict[str, any] = {"model": "gpt-4", "temp": 0.7}  # dictionary

# None handling (optional values)
response: str | None = None  # can be a string or None

# Constants (by convention: UPPERCASE)
API_BASE_URL = "https://api.openai.com/v1"
DEFAULT_MODEL = "gpt-4o"
```

**Why it matters:**
- Type hints help catch bugs early and make your code easier for others (and future you) to understand.
- Constants are used for values that shouldn’t change (like API URLs or model names).

**Tip:** Use descriptive variable names. In AI, names like `embedding_dim`, `prompt_template`, or `token_count` make code much clearer.


### Strings and F-Strings (Prompt Engineering)

Strings are everywhere in AI: prompts, responses, logs, and more. F-strings are essential for building prompts and formatting output in AI applications.

```python
# F-strings for prompt templates
user_name = "Alice"
question = "What is AI?"
prompt = f"""You are a helpful assistant.\n\nUser: {user_name}\nQuestion: {question}\n\nPlease provide a detailed answer."""

# Multi-line strings for system prompts
system_prompt = """You are an expert AI engineer.\nYou help users understand complex AI concepts.\nAlways provide code examples when relevant."""

# String methods (cleaning and processing text)
text = "  Hello, World!  "
text = text.strip()          # "Hello, World!"
text = text.lower()          # "hello, world!"
text = text.replace(",", "") # "hello world!"
"AI" in text                 # False

# Join and split (for tags, CSV, etc.)
tags = ["python", "ai", "ml"]
joined = ", ".join(tags)       # "python, ai, ml"
split_tags = "a,b,c".split(",")    # ["a", "b", "c"]
```

**Why it matters:**
- F-strings are the best way to build prompts for LLMs.
- String methods help you clean and preprocess data for NLP tasks.

**Tip:** Use triple quotes (`"""..."""`) for multi-line prompts and system messages.

### Collections


### Collections: Lists & Dictionaries

Lists and dicts are the backbone of data handling in AI/ML code. You’ll use them for everything from storing chat messages to managing model configs.

```python
# Lists (ordered, mutable)
messages = []
messages.append({"role": "user", "content": "Hello"})
messages.append({"role": "assistant", "content": "Hi!"})

# List comprehensions (powerful for data processing)
tokens = [1, 2, 3, 4, 5]


# Filter with comprehension
long_messages = [m for m in messages if len(m["content"]) > 10]

# Dictionaries (key-value pairs)
config = {
    "model": "gpt-4o",
    "temperature": 0.7,
    "max_tokens": 1000
}

# Access with default (avoids KeyError)
model = config.get("model", "gpt-3.5-turbo")

# Merge dicts (common in config management)
defaults = {"temperature": 0.5, "max_tokens": 500}
```python
final = {**defaults, **overrides}  # temperature is 0.9

# Dictionary comprehension (filtering, transforming)
word_counts = {"hello": 5, "world": 3, "ai": 10}
filtered = {k: v for k, v in word_counts.items() if v > 4}
```

**Why it matters:**
- Most LLM APIs use lists of dicts for messages.
- List/dict comprehensions are essential for data wrangling.

**Tip:** Use comprehensions for filtering and transforming data in a single line.
# If/elif/else
def get_model_tier(model: str) -> str:
    if "gpt-4" in model:
        return "premium"
    elif "gpt-3.5" in model:
        return "standard"
    else:
        return "unknown"

# Ternary operator
status = "success" if response.ok else "error"

# For loops
for message in messages:
    print(f"{message['role']}: {message['content']}")

# Enumerate (get index)
for i, message in enumerate(messages):
    print(f"{i}: {message['content']}")

# Zip (iterate multiple lists)
questions = ["What is AI?", "How does GPT work?"]
answers = ["AI is...", "GPT works by..."]
for q, a in zip(questions, answers):
    print(f"Q: {q}\nA: {a}")

# While loops
retry_count = 0
while retry_count < 3:
    try:
        response = call_api()
        break
    except Exception:
        retry_count += 1
```

## Functions

### Basic Functions

```python
def create_prompt(
    question: str,
    context: str = "",
    max_length: int = 1000
) -> str:
    """Create a prompt for the LLM.
    
    Args:

        ### Control Flow (If, Loops, Patterns)

        Control flow lets you make decisions and repeat actions—critical for API retries, data processing, and more.

        ```python
        # If/elif/else (decision making)

### *args and **kwargs (Flexible APIs)

These let you write functions that accept any number of arguments—handy for wrappers and API calls.

```python
def call_llm(prompt: str, *examples, **options):
    """
    *examples: Variable positional args (tuple)
    **options: Variable keyword args (dict)
    """
    print(f"Prompt: {prompt}")
    print(f"Examples: {examples}")
    print(f"Options: {options}")

# Usage
call_llm(
    "Translate to French",
    "Hello -> Bonjour",
    "Goodbye -> Au revoir",
    model="gpt-4",
    temperature=0.3
)
```

**Why it matters:**
- Lets you build flexible, reusable APIs and wrappers.
- Used in many Python libraries and frameworks.

## Classes

### Basic Classes

```python
from dataclasses import dataclass
from typing import Optional

# Modern Python: Use dataclasses for data containers
@dataclass
class Message:
    role: str

        **Why it matters:**
        - If/else is used for model selection, error handling, and more.
        - Loops are essential for processing batches, streaming, and retries.

        **Common mistake:** Forgetting to break out of a while loop on success can cause infinite loops.
    content: str
    tokens: int = 0
    
# Create instance
msg = Message(role="user", content="Hello")

# Access attributes
print(msg.role)  # "user"

# Traditional class for behavior
class ChatBot:
    def __init__(self, model: str = "gpt-4o"):
        self.model = model
        self.messages: list[Message] = []
        self.total_tokens = 0
    
    def add_message(self, role: str, content: str) -> None:
        msg = Message(role=role, content=content)
        self.messages.append(msg)
    
    def get_history(self) -> list[dict]:
        return [
            {"role": m.role, "content": m.content}
            for m in self.messages
        ]
    
    def clear(self) -> None:
        self.messages = []
        self.total_tokens = 0

# Usage
bot = ChatBot(model="gpt-4o")
bot.add_message("user", "Hello!")
bot.add_message("assistant", "Hi there!")
print(bot.get_history())
```


### Inheritance (Extending Behavior)

Inheritance lets you define a base interface and extend it for different LLM providers, pipelines, etc.

```python
from abc import ABC, abstractmethod

class BaseLLM(ABC):
    """Abstract base class for LLM providers"""
    @abstractmethod
    def complete(self, prompt: str) -> str:
        pass
    @abstractmethod
    def stream(self, prompt: str):
        pass

class OpenAILLM(BaseLLM):
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
    def complete(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    def stream(self, prompt: str):
        for chunk in self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        ):
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

class AnthropicLLM(BaseLLM):
    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)
    def complete(self, prompt: str) -> str:
        response = self.client.messages.create(
            model="claude-3-opus-20240229",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text
```

**Why it matters:**
- Inheritance lets you swap out LLM providers or add new ones with minimal code changes.
- Abstract base classes enforce a consistent interface for all LLMs.


## Error Handling (Robust AI Code)

APIs fail. Networks drop. Good error handling is essential for production AI systems.

```python
import logging
from openai import OpenAI, APIError, RateLimitError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def safe_completion(prompt: str, retries: int = 3) -> str | None:
    """Call OpenAI with retry logic"""
    client = OpenAI()
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        except RateLimitError as e:
            logger.warning(f"Rate limited, attempt {attempt + 1}/{retries}")
            time.sleep(2 ** attempt)  # Exponential backoff
        except APIError as e:
            logger.error(f"API error: {e}")
            raise
        except Exception as e:
            logger.exception(f"Unexpected error: {e}")
            raise
    return None

# Custom exceptions
class TokenLimitExceeded(Exception):
    """Raised when prompt exceeds token limit"""
    pass
class ModelNotAvailable(Exception):
    """Raised when requested model is unavailable"""
    pass

def validate_prompt(prompt: str, max_tokens: int = 4000):
    token_count = len(prompt.split()) * 1.3  # Rough estimate
    if token_count > max_tokens:
        raise TokenLimitExceeded(
            f"Prompt has ~{token_count} tokens, max is {max_tokens}"
        )
```

**Why it matters:**
- AI APIs are rate-limited and can fail unpredictably.
- Always handle exceptions and log errors for debugging.

**Tip:** Use exponential backoff for retries and always log errors with enough context to debug later.


## Async Programming (Concurrent API Calls)

Async is essential for AI apps that make lots of API calls (e.g., batch completions, scraping, etc.).

```python
import asyncio
from openai import AsyncOpenAI

async def get_completion(client: AsyncOpenAI, prompt: str) -> str:
    """Async completion call"""
    response = await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

async def batch_completions(prompts: list[str]) -> list[str]:
    """Process multiple prompts concurrently"""
    client = AsyncOpenAI()
    tasks = [get_completion(client, p) for p in prompts]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    completions = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"Prompt {i} failed: {result}")
            completions.append(None)
        else:
            completions.append(result)
    return completions

# Run async code
async def main():
    prompts = [
        "What is Python?",
        "What is JavaScript?",
        "What is Rust?"
    ]
    results = await batch_completions(prompts)
    for p, r in zip(prompts, results):
        print(f"Q: {p}\nA: {r}\n")

# Entry point
if __name__ == "__main__":
    asyncio.run(main())
```

**Why it matters:**
- Async lets you run many API calls in parallel, making your apps much faster.
- Essential for chatbots, batch processing, and web scraping.

**Tip:** Use `asyncio.gather` to run many tasks at once and handle errors gracefully.


### Async Generators (Streaming Responses)

Streaming is key for real-time LLM apps (chatbots, dashboards, etc.).

```python
async def stream_response(prompt: str):
    """Stream response tokens"""
    client = AsyncOpenAI()
    async for chunk in await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    ):
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

async def main():
    async for token in stream_response("Tell me a story"):
        print(token, end="", flush=True)
```

**Why it matters:**
- Async generators let you process streaming data as it arrives.
- Used for real-time chat, dashboards, and live data feeds.

## Advanced Patterns

### Decorators

```python
import functools
import time
from typing import Callable

def retry(max_attempts: int = 3, delay: float = 1.0):
    """Decorator for automatic retries"""
    def decorator(func: Callable):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_error = None
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_error = e
                    time.sleep(delay * (attempt + 1))
            raise last_error
        return wrapper
    return decorator

def timer(func: Callable):
    """Decorator to time function execution"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = time.perf_counter() - start
        print(f"{func.__name__} took {elapsed:.2f}s")
        return result
    return wrapper

def cache_result(func: Callable):
    """Simple caching decorator"""
    cache = {}
    @functools.wraps(func)
    def wrapper(*args):
        if args not in cache:
            cache[args] = func(*args)
        return cache[args]
    return wrapper

# Usage
@retry(max_attempts=3, delay=2.0)
@timer
def call_api(prompt: str) -> str:
    return client.chat.completions.create(...)
```

### Context Managers

```python
from contextlib import contextmanager
import time

@contextmanager
def track_tokens():
    """Track token usage in a block"""
    usage = {"prompt": 0, "completion": 0}
    yield usage
    print(f"Tokens used - Prompt: {usage['prompt']}, Completion: {usage['completion']}")

# Usage
with track_tokens() as usage:
    response = client.chat.completions.create(...)
    usage["prompt"] = response.usage.prompt_tokens
    usage["completion"] = response.usage.completion_tokens

@contextmanager
def timing(label: str):
    """Time a code block"""
    start = time.perf_counter()
    yield
    elapsed = time.perf_counter() - start
    print(f"{label}: {elapsed:.2f}s")

# Usage
with timing("API call"):
    response = client.chat.completions.create(...)
```


### Generators (Efficient Data Processing)

Generators let you process large datasets (text, embeddings, etc.) without loading everything into memory. This is crucial for AI tasks like chunking documents, streaming data, or batching API calls.

```python
def chunk_text(text: str, chunk_size: int = 1000):
    """Generate text chunks (memory efficient)"""
    for i in range(0, len(text), chunk_size):
        yield text[i:i + chunk_size]

# Usage
large_document = "..." * 100000
for chunk in chunk_text(large_document):
    embedding = get_embedding(chunk)
    store_embedding(embedding)

def batch_items(items: list, batch_size: int = 100):
    """Batch items for processing"""
    for i in range(0, len(items), batch_size):
        yield items[i:i + batch_size]

# Process embeddings in batches
texts = ["text1", "text2", ...]  # 10000 texts
for batch in batch_items(texts, batch_size=100):
    embeddings = client.embeddings.create(input=batch)
```

**Why it matters:**
- Generators are memory efficient and let you process data streams or large files in AI pipelines.
- Used for chunking, batching, and streaming in LLM and embedding workflows.


### Type Hints with Generics (Reusable, Type-Safe Code)

Generics let you write reusable, type-safe code for any data type—useful for wrappers, results, etc.

```python
from typing import TypeVar, Generic, Callable
from dataclasses import dataclass

T = TypeVar('T')

@dataclass
class Result(Generic[T]):
    """Generic result wrapper"""
    success: bool
    data: T | None
    error: str | None

def safe_call(func: Callable[..., T], *args, **kwargs) -> Result[T]:
    """Safely call a function and wrap result"""
    try:
        data = func(*args, **kwargs)
        return Result(success=True, data=data, error=None)
    except Exception as e:
        return Result(success=False, data=None, error=str(e))

# Usage
result = safe_call(client.chat.completions.create, model="gpt-4o", ...)
if result.success:
    print(result.data.choices[0].message.content)
else:
    print(f"Error: {result.error}")
```

**Why it matters:**
- Generics let you write reusable, type-safe wrappers for any function or data type.
- Used in libraries like pydantic, FastAPI, and more.


## Working with Files (Data, Prompts, Config)

Most AI projects read/write files for prompts, configs, and data. Use `pathlib` for modern, cross-platform file handling.

```python
from pathlib import Path
import json

# Pathlib (modern file handling)
project_dir = Path(__file__).parent
data_dir = project_dir / "data"
data_dir.mkdir(exist_ok=True)

# Read/write text
prompt_file = data_dir / "system_prompt.txt"
prompt_file.write_text("You are a helpful assistant.")
system_prompt = prompt_file.read_text()

# Read/write JSON
config_file = data_dir / "config.json"
config = {"model": "gpt-4o", "temperature": 0.7}
config_file.write_text(json.dumps(config, indent=2))
loaded_config = json.loads(config_file.read_text())

# List files
for md_file in data_dir.glob("*.md"):
    print(f"Found: {md_file.name}")

# Check existence
if not config_file.exists():
    print("Config not found!")
```

**Why it matters:**
- File I/O is essential for saving prompts, logs, configs, and results in AI workflows.
- Pathlib makes file handling safer and more readable than using raw file paths.


## Environment Variables (Secrets & Config)

Never hardcode secrets (API keys, DB URLs) in your code. Use environment variables and `.env` files for security and flexibility.

```python
import os
from dotenv import load_dotenv

# Load .env file
load_dotenv()

# Get with validation
def get_required_env(key: str) -> str:
    value = os.getenv(key)
    if not value:
        raise ValueError(f"Missing required env var: {key}")
    return value

# Usage
OPENAI_API_KEY = get_required_env("OPENAI_API_KEY")
DATABASE_URL = get_required_env("DATABASE_URL")
DEBUG = os.getenv("DEBUG", "false").lower() == "true"
```

**Why it matters:**
- Keeps secrets out of your codebase and version control.
- Makes it easy to switch between dev, staging, and production environments.


## Common Libraries for AI

Here are the most-used libraries for modern AI/ML projects. These cover LLMs, embeddings, vector databases, data validation, and more.

```python
# requirements.txt for AI projects
"""
openai>=1.0.0           # OpenAI API client
anthropic>=0.5.0        # Anthropic Claude API client
langchain>=0.1.0        # LLM orchestration and chains
langchain-openai>=0.0.5 # LangChain integration for OpenAI
chromadb>=0.4.0         # Local vector database
pinecone-client>=3.0.0  # Pinecone vector DB client
tiktoken>=0.5.0         # Tokenizer for OpenAI models
pydantic>=2.0.0         # Data validation and settings
python-dotenv>=1.0.0    # Load .env files
httpx>=0.25.0           # Async HTTP client
tenacity>=8.2.0         # Retry logic
structlog>=23.0.0       # Structured logging
"""

# Install all
# pip install openai anthropic langchain chromadb pydantic python-dotenv
```

**Why it matters:**
- These libraries are the foundation for most modern AI engineering workflows.
- Learn to read requirements.txt files to understand and manage dependencies.


## Quick Reference

<CardGroup cols={2}>
    <Card title="Data Classes" icon="database">
        Use <code>@dataclass</code> for data containers. Reduces boilerplate and improves readability.
    </Card>
    <Card title="Type Hints" icon="code">
        Always use type hints for clarity, bug prevention, and better editor support.
    </Card>
    <Card title="Async/Await" icon="bolt">
        Use async for concurrent API calls and streaming data in AI apps.
    </Card>
    <Card title="F-Strings" icon="text">
        Use f-strings for prompt templates and readable string formatting.
    </Card>
</CardGroup>

---

<Note>
**Next Step:** Now that you know Python, continue to [FastAPI Crash Course](/ai-engineering/fastapi-crash-course) to build APIs for your AI applications.
</Note>
