---
title: "LLM Fundamentals"
description: "Deep dive into how LLMs work—tokenization, attention, inference, and cost optimization"
icon: "brain"
---

## Why This Matters

Most developers use LLMs like magic boxes. They copy-paste prompts from Twitter and pray it works.

**You'll be different.** Understanding *how* LLMs work lets you:
- Debug when outputs are wrong
- Optimize costs (save 10x on API bills)
- Design prompts that actually work
- Know when to use which model

<Note>
**Real Talk**: Companies waste thousands on AI because developers don't understand token economics. After this module, you won't.
</Note>

## The Core Mental Model

LLMs are **next-token predictors**. That's it. Everything else is a consequence of this simple idea.

```
Input: "The capital of France is"
Model thinks: What token is most likely next?
Output: " Paris" (with high probability)
```

![Next Token Prediction Flow](/images/courses/ai-next-token-prediction.svg)

They don't "know" facts. They predict what text is likely to follow based on patterns in training data.

<Warning>
**This explains hallucinations**: If "The CEO of Apple is Steve Jobs" appeared often in training data, the model might predict "Steve Jobs" even though Tim Cook is the current CEO. It predicts likely text, not true text.
</Warning>

## Tokenization: The Foundation

### Why Tokens Matter

LLMs don't see characters or words—they see **tokens**. This affects everything:
- **Pricing**: You pay per token, not per word
- **Context limits**: 128K tokens, not 128K words
- **Output quality**: Some words are multiple tokens, affecting generation

```python
import tiktoken

enc = tiktoken.encoding_for_model("gpt-4o")

# English is efficient (~4 chars per token)
english = "Hello, how are you today?"
print(f"English: {len(english)} chars → {len(enc.encode(english))} tokens")
# English: 25 chars → 6 tokens

# Code is less efficient
code = "def calculate_average(numbers: list[float]) -> float:"
print(f"Code: {len(code)} chars → {len(enc.encode(code))} tokens")
# Code: 54 chars → 15 tokens

# Non-English is expensive
arabic = "مرحبا كيف حالك"
print(f"Arabic: {len(arabic)} chars → {len(enc.encode(arabic))} tokens")
# Arabic: 14 chars → 12 tokens (almost 1 token per char!)

# Numbers are weird
numbers = "1234567890"
print(f"Numbers: {len(numbers)} chars → {len(enc.encode(numbers))} tokens")
# Numbers: 10 chars → 3 tokens
```

### Token Economics Calculator

```python
import tiktoken
from dataclasses import dataclass

@dataclass
class ModelPricing:
    name: str
    input_per_million: float
    output_per_million: float
    context_window: int

MODELS = {
    "gpt-4o": ModelPricing("gpt-4o", 2.50, 10.00, 128000),
    "gpt-4o-mini": ModelPricing("gpt-4o-mini", 0.15, 0.60, 128000),
    "gpt-4-turbo": ModelPricing("gpt-4-turbo", 10.00, 30.00, 128000),
    "claude-3-5-sonnet": ModelPricing("claude-3-5-sonnet", 3.00, 15.00, 200000),
    "claude-3-5-haiku": ModelPricing("claude-3-5-haiku", 0.25, 1.25, 200000),
}

def estimate_cost(
    prompt: str,
    expected_output_tokens: int = 500,
    model: str = "gpt-4o"
) -> dict:
    """Estimate API call cost"""
    enc = tiktoken.encoding_for_model("gpt-4")  # Close enough for estimation
    input_tokens = len(enc.encode(prompt))
    
    pricing = MODELS[model]
    
    input_cost = (input_tokens / 1_000_000) * pricing.input_per_million
    output_cost = (expected_output_tokens / 1_000_000) * pricing.output_per_million
    
    return {
        "model": model,
        "input_tokens": input_tokens,
        "output_tokens": expected_output_tokens,
        "input_cost": f"${input_cost:.6f}",
        "output_cost": f"${output_cost:.6f}",
        "total_cost": f"${input_cost + output_cost:.6f}",
        "context_used": f"{(input_tokens / pricing.context_window) * 100:.1f}%"
    }

# Compare costs across models
prompt = "Explain quantum computing in detail with examples..."
for model in ["gpt-4o", "gpt-4o-mini", "claude-3-5-sonnet"]:
    print(estimate_cost(prompt, model=model))
```

### The Token Limit Trap

```python
def smart_truncate(text: str, max_tokens: int = 4000, model: str = "gpt-4") -> str:
    """Truncate text to fit token limit while preserving meaning"""
    enc = tiktoken.encoding_for_model(model)
    tokens = enc.encode(text)
    
    if len(tokens) <= max_tokens:
        return text
    
    # Keep first 80% and last 20% to preserve context
    keep_start = int(max_tokens * 0.8)
    keep_end = max_tokens - keep_start
    
    truncated_tokens = tokens[:keep_start] + tokens[-keep_end:]
    
    return enc.decode(truncated_tokens)
```

## Embeddings: Semantic Understanding

Embeddings convert text into vectors where **similar meanings are close together**.

### The Intuition

![Word Embeddings Space](/images/courses/ai-embeddings-space.svg)

Notice how the relationship direction is consistent:
`King - Man + Woman ≈ Queen`

This vector arithmetic allows the model to understand analogies and relationships.

### Practical Embedding System

```python
from openai import OpenAI
import numpy as np
from typing import List
import json

client = OpenAI()

class EmbeddingCache:
    """Cache embeddings to avoid repeated API calls"""
    
    def __init__(self, cache_file: str = "embeddings_cache.json"):
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self) -> dict:
        try:
            with open(self.cache_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _save_cache(self):
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f)
    
    def get_embedding(self, text: str, model: str = "text-embedding-3-small") -> List[float]:
        cache_key = f"{model}:{text[:100]}"  # Use prefix for key
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        response = client.embeddings.create(model=model, input=text)
        embedding = response.data[0].embedding
        
        self.cache[cache_key] = embedding
        self._save_cache()
        
        return embedding
    
    def get_embeddings_batch(self, texts: List[str], model: str = "text-embedding-3-small") -> List[List[float]]:
        """Batch embed for efficiency (up to 2048 texts per call)"""
        # Check cache first
        uncached = [(i, t) for i, t in enumerate(texts) if f"{model}:{t[:100]}" not in self.cache]
        
        if uncached:
            indices, uncached_texts = zip(*uncached)
            response = client.embeddings.create(model=model, input=list(uncached_texts))
            
            for i, emb_data in zip(indices, response.data):
                cache_key = f"{model}:{texts[i][:100]}"
                self.cache[cache_key] = emb_data.embedding
            
            self._save_cache()
        
        return [self.cache[f"{model}:{t[:100]}"] for t in texts]


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Compute cosine similarity between two vectors"""
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def find_most_similar(query: str, documents: List[str], top_k: int = 3) -> List[tuple]:
    """Find most similar documents to query"""
    cache = EmbeddingCache()
    
    query_emb = cache.get_embedding(query)
    doc_embs = cache.get_embeddings_batch(documents)
    
    similarities = [
        (doc, cosine_similarity(query_emb, doc_emb))
        for doc, doc_emb in zip(documents, doc_embs)
    ]
    
    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]


# Example usage
documents = [
    "Python is a programming language",
    "JavaScript runs in browsers",
    "Machine learning uses neural networks",
    "Snakes are reptiles that slither",
    "The stock market closed higher today"
]

results = find_most_similar("coding languages", documents)
for doc, score in results:
    print(f"{score:.3f}: {doc}")
# 0.847: Python is a programming language
# 0.812: JavaScript runs in browsers
# 0.623: Machine learning uses neural networks
```

## Temperature & Sampling

### How Sampling Works

When the model predicts the next token, it outputs **probabilities for all possible tokens**:

```
"The capital of France is" → 
  " Paris": 0.85
  " Lyon": 0.05
  " a": 0.03
  " the": 0.02
  ...
```

**Temperature** controls how these probabilities are used:

```python
import numpy as np

def sample_with_temperature(logits: np.ndarray, temperature: float) -> int:
    """Demonstrate temperature sampling"""
    # Apply temperature
    scaled_logits = logits / temperature
    
    # Convert to probabilities
    probs = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits))
    
    # Sample
    return np.random.choice(len(probs), p=probs)

# Temperature 0: Always pick highest probability (deterministic)
# Temperature 0.5: Slightly random, but still favors likely tokens
# Temperature 1.0: Sample according to original probabilities
# Temperature 2.0: More random, even unlikely tokens have a chance
```

### When to Use What

| Task | Temperature | Why |
|------|-------------|-----|
| Code generation | 0 | Deterministic, reproducible |
| Factual Q&A | 0-0.3 | Minimize hallucination |
| General chat | 0.7 | Natural variation |
| Creative writing | 0.9-1.2 | Unexpected combinations |
| Brainstorming | 1.0-1.5 | Explore diverse ideas |

```python
from openai import OpenAI

client = OpenAI()

def generate(prompt: str, task_type: str = "general") -> str:
    """Generate with appropriate temperature for task"""
    temp_map = {
        "code": 0,
        "factual": 0.2,
        "general": 0.7,
        "creative": 1.0,
        "brainstorm": 1.3
    }
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=temp_map.get(task_type, 0.7)
    )
    
    return response.choices[0].message.content
```

## The Attention Mechanism

Attention is what makes Transformers special. It allows the model to focus on different parts of the input sentence when processing a specific word.

Consider the sentence:
> "The animal didn't cross the street because **it** was too tired."

To understand what "**it**" refers to, the model must pay attention to "**animal**" and ignore "**street**".

![Self-Attention Mechanism](/images/courses/ai-attention-mechanism.svg)

Without attention, the model wouldn't know if "it" referred to the animal or the street, making translation and comprehension impossible.

### Why Context Window Matters

```python
# GPT-4o has 128K context ≈ 96K words ≈ 300 pages

# But attention has quadratic complexity: O(n²)
# 128K tokens = 128K × 128K = 16 billion attention computations

# This is why:
# 1. Long contexts are slower
# 2. Long contexts cost more
# 3. Information at the start/end is remembered better than middle
```

### The "Lost in the Middle" Problem

Research shows LLMs struggle with information in the middle of long contexts:

```python
def structure_for_attention(docs: list[str], query: str) -> str:
    """Structure documents to avoid 'lost in the middle' problem"""
    # Put most relevant docs at START and END
    # Put less relevant docs in MIDDLE
    
    ranked = rank_by_relevance(docs, query)
    
    # Interleave: most relevant at edges
    n = len(ranked)
    reordered = []
    for i in range(n):
        if i % 2 == 0:
            reordered.insert(0, ranked[i])  # Add to start
        else:
            reordered.append(ranked[i])  # Add to end
    
    return "\n\n".join(reordered)
```

## Prompt Engineering That Works

### The COSTAR Framework

```python
def build_prompt_costar(
    context: str,
    objective: str,
    style: str,
    tone: str,
    audience: str,
    response_format: str
) -> str:
    """
    COSTAR framework for structured prompts
    
    C - Context: Background information
    O - Objective: What you want to achieve
    S - Style: Writing style (formal, casual, technical)
    T - Tone: Emotional tone (professional, friendly)
    A - Audience: Who will read this
    R - Response: Format of output
    """
    return f"""
# Context
{context}

# Objective
{objective}

# Style
Write in a {style} style.

# Tone
Maintain a {tone} tone.

# Audience
This is for {audience}.

# Response Format
{response_format}
"""

# Example
prompt = build_prompt_costar(
    context="We're launching a new AI code review tool for developers.",
    objective="Write a product announcement for our blog.",
    style="technical but accessible",
    tone="excited but professional",
    audience="software developers who use GitHub",
    response_format="Blog post with headline, 3-4 paragraphs, and a call to action."
)
```

### Few-Shot Prompting That Scales

```python
def few_shot_prompt(
    task_description: str,
    examples: list[dict],  # [{"input": ..., "output": ...}]
    input_text: str
) -> str:
    """Build a few-shot prompt with examples"""
    prompt = f"{task_description}\n\n"
    
    for i, ex in enumerate(examples, 1):
        prompt += f"Example {i}:\n"
        prompt += f"Input: {ex['input']}\n"
        prompt += f"Output: {ex['output']}\n\n"
    
    prompt += f"Now process this:\n"
    prompt += f"Input: {input_text}\n"
    prompt += f"Output:"
    
    return prompt

# Example: Sentiment analysis
examples = [
    {"input": "This product is amazing!", "output": "positive"},
    {"input": "Worst purchase ever.", "output": "negative"},
    {"input": "It's okay, nothing special.", "output": "neutral"},
]

prompt = few_shot_prompt(
    "Classify the sentiment of the text as positive, negative, or neutral.",
    examples,
    "The quality exceeded my expectations!"
)
```

### Chain of Thought (CoT) for Complex Reasoning

```python
def cot_prompt(question: str) -> str:
    """Force step-by-step reasoning"""
    return f"""
{question}

Let's solve this step by step:
1. First, I'll identify what we know
2. Then, I'll figure out what we need to find
3. Next, I'll work through the logic
4. Finally, I'll state the answer

Step 1:"""

# CoT is especially powerful for:
# - Math problems
# - Logic puzzles
# - Multi-step reasoning
# - Code debugging
```

## Production Patterns

### Retry with Exponential Backoff

```python
import time
from openai import OpenAI, RateLimitError, APIError
from functools import wraps

def retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0):
    """Decorator for automatic retry with exponential backoff"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except RateLimitError:
                    if attempt == max_retries - 1:
                        raise
                    delay = base_delay * (2 ** attempt)
                    print(f"Rate limited. Waiting {delay}s...")
                    time.sleep(delay)
                except APIError as e:
                    if attempt == max_retries - 1:
                        raise
                    print(f"API error: {e}. Retrying...")
                    time.sleep(base_delay)
            return None
        return wrapper
    return decorator

@retry_with_backoff(max_retries=3)
def call_llm(prompt: str) -> str:
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content
```

### Response Caching

```python
import hashlib
import json
from pathlib import Path

class LLMCache:
    """Cache LLM responses to disk"""
    
    def __init__(self, cache_dir: str = ".llm_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def _hash_request(self, model: str, messages: list, **kwargs) -> str:
        """Create unique hash for request"""
        key_data = json.dumps({"model": model, "messages": messages, **kwargs}, sort_keys=True)
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, model: str, messages: list, **kwargs) -> str | None:
        """Get cached response if exists"""
        cache_key = self._hash_request(model, messages, **kwargs)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        if cache_file.exists():
            with open(cache_file) as f:
                return json.load(f)["response"]
        return None
    
    def set(self, model: str, messages: list, response: str, **kwargs):
        """Cache a response"""
        cache_key = self._hash_request(model, messages, **kwargs)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        with open(cache_file, 'w') as f:
            json.dump({"response": response}, f)

def cached_llm_call(prompt: str, model: str = "gpt-4o-mini", use_cache: bool = True) -> str:
    """LLM call with caching"""
    cache = LLMCache()
    messages = [{"role": "user", "content": prompt}]
    
    if use_cache:
        cached = cache.get(model, messages)
        if cached:
            return cached
    
    client = OpenAI()
    response = client.chat.completions.create(model=model, messages=messages)
    result = response.choices[0].message.content
    
    if use_cache:
        cache.set(model, messages, result)
    
    return result
```

## Mini-Project: Cost-Aware Chat Application

Build a complete chat app that tracks and optimizes costs:

```python
from openai import OpenAI
from dataclasses import dataclass, field
from typing import List
import tiktoken

@dataclass
class Message:
    role: str
    content: str
    tokens: int = 0

@dataclass
class ConversationStats:
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_cost: float = 0.0
    message_count: int = 0

class CostAwareChat:
    """Chat application with cost tracking and optimization"""
    
    def __init__(self, model: str = "gpt-4o-mini", budget_limit: float = 1.0):
        self.client = OpenAI()
        self.model = model
        self.budget_limit = budget_limit
        self.messages: List[Message] = []
        self.stats = ConversationStats()
        self.encoder = tiktoken.encoding_for_model("gpt-4")
        
        # Pricing per million tokens
        self.pricing = {
            "gpt-4o": {"input": 2.50, "output": 10.00},
            "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        }
    
    def _count_tokens(self, text: str) -> int:
        return len(self.encoder.encode(text))
    
    def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        pricing = self.pricing[self.model]
        return (input_tokens / 1_000_000) * pricing["input"] + \
               (output_tokens / 1_000_000) * pricing["output"]
    
    def _build_messages(self) -> list:
        """Build message list, potentially summarizing old messages"""
        total_tokens = sum(m.tokens for m in self.messages)
        
        # If approaching context limit, summarize old messages
        if total_tokens > 10000:
            return self._summarize_and_build()
        
        return [{"role": m.role, "content": m.content} for m in self.messages]
    
    def _summarize_and_build(self) -> list:
        """Summarize conversation history to save tokens"""
        # Keep last 5 messages, summarize the rest
        recent = self.messages[-5:]
        old = self.messages[:-5]
        
        if old:
            old_text = "\n".join([f"{m.role}: {m.content}" for m in old])
            summary_response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{
                    "role": "user",
                    "content": f"Summarize this conversation in 2-3 sentences:\n{old_text}"
                }],
                max_tokens=200
            )
            summary = summary_response.choices[0].message.content
            
            return [
                {"role": "system", "content": f"Previous conversation summary: {summary}"},
                *[{"role": m.role, "content": m.content} for m in recent]
            ]
        
        return [{"role": m.role, "content": m.content} for m in recent]
    
    def chat(self, user_message: str) -> str:
        """Send message and get response with cost tracking"""
        # Check budget
        if self.stats.total_cost >= self.budget_limit:
            return f"Budget limit of ${self.budget_limit} reached. Total spent: ${self.stats.total_cost:.4f}"
        
        # Add user message
        user_tokens = self._count_tokens(user_message)
        self.messages.append(Message("user", user_message, user_tokens))
        
        # Build and send
        messages = self._build_messages()
        input_tokens = sum(self._count_tokens(m["content"]) for m in messages)
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages
        )
        
        assistant_content = response.choices[0].message.content
        output_tokens = self._count_tokens(assistant_content)
        
        # Track stats
        self.stats.total_input_tokens += input_tokens
        self.stats.total_output_tokens += output_tokens
        self.stats.total_cost += self._calculate_cost(input_tokens, output_tokens)
        self.stats.message_count += 1
        
        # Add assistant message
        self.messages.append(Message("assistant", assistant_content, output_tokens))
        
        return assistant_content
    
    def get_stats(self) -> dict:
        """Get conversation statistics"""
        return {
            "messages": self.stats.message_count,
            "input_tokens": self.stats.total_input_tokens,
            "output_tokens": self.stats.total_output_tokens,
            "total_cost": f"${self.stats.total_cost:.6f}",
            "budget_remaining": f"${self.budget_limit - self.stats.total_cost:.6f}",
            "model": self.model
        }


# Usage
chat = CostAwareChat(model="gpt-4o-mini", budget_limit=0.50)

print(chat.chat("What is machine learning?"))
print(chat.chat("Give me an example"))
print(chat.chat("How is it different from AI?"))

print("\n--- Stats ---")
print(chat.get_stats())
```

## Key Takeaways

<CardGroup cols={2}>
  <Card title="Tokens = Money" icon="dollar-sign">
    Every token costs. Cache, truncate, and choose models wisely. gpt-4o-mini is 17x cheaper than gpt-4o.
  </Card>
  <Card title="Embeddings Power Search" icon="magnifying-glass">
    Convert text to vectors for semantic similarity. Cache embeddings to avoid repeated costs.
  </Card>
  <Card title="Temperature = Creativity Dial" icon="temperature-half">
    0 for deterministic (code), 0.7 for balanced, 1+ for creative. Match to your task.
  </Card>
  <Card title="Context Has Limits" icon="window-maximize">
    128K tokens sounds like a lot, but attention degrades. Put important info at edges, not middle.
  </Card>
</CardGroup>

## What's Next

<Card title="OpenAI API Deep Dive" icon="arrow-right" href="/ai-engineering/openai-api">
  Master function calling, structured outputs, streaming, and production patterns
</Card>
