---
title: "LLM Fundamentals"
description: "Understand how large language models work, from tokenization to inference"
icon: "brain"
---

## How LLMs Work

Large Language Models are neural networks trained to predict the next token in a sequence. Understanding their internals helps you use them effectively.

<Note>
**Key Insight**: LLMs don't "understand" text—they predict statistically likely continuations based on patterns learned from training data.
</Note>

## Core Concepts

### 1. Tokenization

LLMs don't see text as characters or words—they see **tokens**.

```python
import tiktoken

# OpenAI's tokenizer for GPT-4
enc = tiktoken.encoding_for_model("gpt-4")

text = "Hello, world!"
tokens = enc.encode(text)
print(tokens)  # [9906, 11, 1917, 0]
print(len(tokens))  # 4 tokens

# Decode back
decoded = enc.decode(tokens)
print(decoded)  # "Hello, world!"
```

<Warning>
**Token Limits Matter**: GPT-4 Turbo has 128K token context. A token is roughly 4 characters or 0.75 words in English.
</Warning>

| Model | Context Window | Cost (Input/Output per 1M) |
|-------|---------------|---------------------------|
| GPT-4o | 128K | $2.50 / $10.00 |
| GPT-4o-mini | 128K | $0.15 / $0.60 |
| Claude 3.5 Sonnet | 200K | $3.00 / $15.00 |
| Claude 3.5 Haiku | 200K | $0.25 / $1.25 |

### 2. Embeddings

Embeddings convert text into numerical vectors that capture semantic meaning.

```python
from openai import OpenAI

client = OpenAI()

def get_embedding(text: str) -> list[float]:
    """Get embedding vector for text"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# Similar meanings = similar vectors
emb1 = get_embedding("The cat sat on the mat")
emb2 = get_embedding("A feline rested on the rug")
emb3 = get_embedding("Stock market crashed today")

# Cosine similarity
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print(cosine_similarity(emb1, emb2))  # ~0.85 (similar)
print(cosine_similarity(emb1, emb3))  # ~0.20 (different)
```

### 3. Temperature & Sampling

Temperature controls randomness in outputs:

| Temperature | Behavior | Use Case |
|-------------|----------|----------|
| 0.0 | Deterministic, picks most likely | Code, factual answers |
| 0.7 | Balanced creativity | General tasks |
| 1.0+ | High randomness | Creative writing, brainstorming |

```python
# Deterministic output for code generation
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a Python function to reverse a string"}],
    temperature=0  # Always gives same output
)

# Creative output for brainstorming
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Give me startup ideas for AI"}],
    temperature=1.0  # Different each time
)
```

## The Transformer Architecture

<Accordion title="Simplified Transformer Explanation" icon="microchip">
Transformers use **attention** to weigh the importance of different parts of input when generating each output token.

**Key Components:**
1. **Token Embeddings**: Convert tokens to vectors
2. **Positional Encoding**: Add position information
3. **Self-Attention**: Each token attends to all other tokens
4. **Feed-Forward Layers**: Process attention outputs
5. **Output Layer**: Predict probability of next token

The "attention" mechanism is what allows LLMs to understand context over long sequences.
</Accordion>

## Prompt Engineering Basics

### System vs User Messages

```python
messages = [
    {
        "role": "system",
        "content": "You are a helpful coding assistant. Always include code examples."
    },
    {
        "role": "user", 
        "content": "Explain Python decorators"
    }
]
```

### Few-Shot Prompting

Provide examples to guide the model's behavior:

```python
messages = [
    {"role": "system", "content": "Convert natural language to SQL."},
    {"role": "user", "content": "Show all users"},
    {"role": "assistant", "content": "SELECT * FROM users;"},
    {"role": "user", "content": "Show users older than 30"},
    {"role": "assistant", "content": "SELECT * FROM users WHERE age > 30;"},
    {"role": "user", "content": "Count users by country"}  # Model will follow pattern
]
```

### Chain of Thought

Ask the model to reason step by step:

```python
prompt = """
Solve this problem step by step:

A store has 50 apples. They sell 23 in the morning and receive a shipment of 15.
Then they sell 12 more. How many apples do they have?

Think through each step before giving the final answer.
"""
```

## Common Pitfalls

<Warning>
**Avoid These Mistakes:**
1. **Assuming determinism**: Same prompt can give different outputs
2. **Ignoring token limits**: Long contexts get truncated
3. **Trusting without verification**: LLMs can hallucinate confidently
4. **Not handling errors**: API calls can fail
</Warning>

## Practical Exercise

Build a simple tokenizer analyzer:

```python
import tiktoken

def analyze_prompt(text: str, model: str = "gpt-4"):
    """Analyze token usage for a prompt"""
    enc = tiktoken.encoding_for_model(model)
    tokens = enc.encode(text)
    
    return {
        "text_length": len(text),
        "token_count": len(tokens),
        "chars_per_token": len(text) / len(tokens),
        "estimated_cost_gpt4o": len(tokens) * 0.0000025,  # $2.50 per 1M input
        "tokens": tokens[:10]  # First 10 tokens
    }

# Try it
result = analyze_prompt("Explain quantum computing in simple terms")
print(result)
```

## Key Takeaways

<CardGroup cols={2}>
  <Card title="Tokens, Not Words" icon="puzzle-piece">
    LLMs process tokens. Count them to manage costs and context limits.
  </Card>
  <Card title="Embeddings Capture Meaning" icon="brain">
    Vector representations enable semantic search and similarity.
  </Card>
  <Card title="Temperature Controls Creativity" icon="temperature-half">
    Low for precision, high for creativity.
  </Card>
  <Card title="Prompts Are Programs" icon="code">
    Well-structured prompts with examples produce better outputs.
  </Card>
</CardGroup>

## Next Steps

<Card title="OpenAI API Deep Dive" icon="arrow-right" href="/ai-engineering/openai-api">
  Learn to use the OpenAI API effectively, including function calling and structured outputs
</Card>
