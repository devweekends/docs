---
title: "AI Engineer Interview Questions"
description: "Complete preparation guide covering LLMs, RAG systems, AI system design, deployment, monitoring, and safety"
icon: "brain-circuit"
---

<hr />

## 1. LLM Engineering & Prompt Design

<AccordionGroup>

<Accordion title="What are few-shot vs zero-shot prompting and when to use each?">

**Zero-shot prompting** asks the model to perform a task without examples, relying only on pre-trained knowledge.  
**Few-shot prompting** provides 1–5 examples to guide structure, style, and tone.

**Use zero-shot when:**
- Task is simple and well-known (e.g., translation)
- No examples available or cost-sensitive
- You want unbiased responses

**Use few-shot when:**
- Output format consistency is needed
- Domain-specific context or edge cases exist
- Zero-shot outputs are inconsistent

**Example:**
- Zero-shot: “Classify sentiment: ‘I love this phone!’”
- Few-shot: Add 2–3 labeled examples before the query.

_Few-shot usually improves accuracy by 10–30% but increases token usage 3–5x._
</Accordion>

<Accordion title="What is temperature in LLM generation and how does it affect outputs?">

Temperature controls **randomness** in text generation (range: 0–2).

- **Low (0–0.3):** Deterministic, precise outputs (coding, classification)
- **Medium (0.4–0.7):** Balanced tone (summaries, Q&A)
- **High (0.8–1.0):** Creative, diverse results (brainstorming)

**Examples:**
- Coding assistant → `temp=0.1`
- Customer support → `temp=0.4`
- Marketing content → `temp=0.8`

Combine with `top_p` (nucleus sampling) and `top_k` for fine control.
</Accordion>

<Accordion title="What is prompt injection and how to defend against it?">

Prompt injection happens when malicious input manipulates an LLM to break rules or expose system prompts.

**Attack types:**
- Direct: “Ignore previous instructions…”
- Context: Hidden malicious text in documents
- Jailbreak: Role-play or DAN-style prompts
- Prompt leak: Forcing model to reveal system prompt

**Defenses:**
1. **Input validation:** Filter keywords like “ignore”, “system prompt”.
2. **Prompt separation:** Clearly delimit system and user input.
3. **Instruction hierarchy:** Reiterate rules after user input.
4. **Output validation:** Sanitize responses before showing.
5. **Monitoring:** Log blocked attempts for review.

**Best practice:** Layered security — validate, isolate, and filter both input and output.
</Accordion>

</AccordionGroup>

<hr />

## 2. AI System Architecture

<AccordionGroup>

<Accordion title="Design a scalable AI chatbot for 10,000 concurrent users">

**Requirements:** 10k users, &lt;2s latency, 99.9% uptime, cost-efficient.

**Architecture overview:**
1. **Load Balancer:** NGINX / AWS ALB (SSL termination, DDoS protection)
2. **API Gateway:** Kong / AWS Gateway (rate limiting, JWT auth)
3. **App Servers:** FastAPI / Express (Kubernetes, auto-scaling)
4. **Cache:** Redis cluster for sessions &amp; frequent responses
5. **Model Serving:**
   - Managed (OpenAI/Vertex) for simplicity  
   - Self-hosted (vLLM + A100 GPUs) for cost optimization
6. **Message Queue:** RabbitMQ / Kafka for async tasks
7. **Databases:** PostgreSQL (metadata) + Pinecone/Milvus (vector search)
8. **Monitoring:** Prometheus + Grafana + ELK stack

**Flow:** User → Gateway → Cache → DB/vector store → Model → Streamed response  
**Cost:** ~$33k (managed) or ~$15k (self-hosted) monthly.  
**Reliability:** Circuit breakers, auto-scaling, blue-green deployments.
</Accordion>

<Accordion title="Why use a vector database in AI systems?">

Vector DBs (Pinecone, Milvus, Weaviate) store high-dimensional embeddings for **semantic search**.

**Use cases:**
- RAG (Retrieval Augmented Generation)
- Semantic similarity search
- Contextual recommendations

**Advantages:**
- Fast cosine/dot-product search
- Horizontal scalability
- &lt;100ms retrieval time

**Example:** Retrieve top 5 semantically similar docs before LLM generation.
</Accordion>

<Accordion title="Explain caching in AI system design">

Caching stores frequent responses or context to minimize repeated model calls.

**Tools:** Redis / Memcached  
**Strategies:**
- **Response caching:** For common queries  
- **Context caching:** Last N user messages  
- **Rate limiting:** Prevent abuse  
**Eviction:** LRU with TTL (1h for context, 24h for cache)
</Accordion>

</AccordionGroup>

<hr />

## 3. Model Deployment & Serving

<AccordionGroup>

<Accordion title="What are common model serving frameworks?">

- **vLLM:** High throughput inference, dynamic batching  
- **TorchServe:** Scalable PyTorch serving  
- **TensorRT / ONNX Runtime:** Optimized inference for GPUs/CPUs  
- **Ray Serve:** Distributed deployment for microservices  

**Example:** Deploy Mistral-7B on vLLM with 4×A100 GPUs for 200–300 tokens/sec per GPU.
</Accordion>

<Accordion title="Explain batch inference vs streaming">

- **Batch inference:** Process many inputs together — efficient for offline jobs  
- **Streaming:** Generate and send tokens live — ideal for chat or long text  

**Example:** Chatbot → stream tokens for smoother UX.
</Accordion>

<Accordion title="What is the difference between managed API vs self-hosted model?">

**Managed API (OpenAI, Anthropic):**
- ✅ Fast to integrate  
- ✅ No infra maintenance  
- ❌ Expensive at scale  
- ❌ Limited customization  

**Self-hosted (vLLM, Text-Gen WebUI):**
- ✅ Lower cost after 2–5M req/month  
- ✅ Control over weights  
- ❌ Needs GPU infra + MLOps skills  
</Accordion>

</AccordionGroup>

<hr />

## 4. RAG Systems (Retrieval Augmented Generation)

<AccordionGroup>

<Accordion title="What is RAG and why is it useful?">

RAG = **Retrieval + Generation** — retrieves relevant docs before generating output, grounding the model in factual context.

**Benefits:**
- Reduces hallucination  
- Keeps results current  
- Enables domain adaptation without retraining  

**Pipeline:** Embed → Store → Retrieve (top-k) → Construct prompt → Generate response
</Accordion>

<Accordion title="Explain the embedding process">

Text is converted into numerical vectors using embedding models (e.g., `text-embedding-3-small`).

**Steps:**
1. Tokenize text  
2. Convert to fixed-size vector  
3. Store in vector DB  

**Distance metrics:** Cosine similarity, dot product, Euclidean distance.
</Accordion>

<Accordion title="How to optimize RAG performance?">

- Smart document chunking (500–800 tokens)  
- Use metadata filters (type, tags, date)  
- Cache top-k retrievals  
- Re-rank using relevance scores  
</Accordion>

</AccordionGroup>

<hr />

## 5. MLOps, Monitoring & Ethics

<AccordionGroup>

<Accordion title="What is MLOps and why is it important?">

MLOps applies DevOps principles to ML — ensuring consistent deployment, monitoring, and governance.

**Key areas:**
- Continuous integration (CI)  
- Continuous training (CT)  
- Continuous deployment (CD)  
- Model registry/versioning  
- Drift detection and rollback  
</Accordion>

<Accordion title="How to monitor AI models in production?">

**Metrics:** Latency, accuracy, token usage, cost, drift  
**Tools:** Prometheus, Grafana, OpenTelemetry  
**Alerts:** P95 latency, error spikes, accuracy drops  

**Best practices:**  
- Log prompts safely  
- Anonymize data  
- Track feedback loops  
</Accordion>

<Accordion title="Explain AI safety and ethical considerations">

- Avoid harmful or biased outputs  
- Enforce strict usage policies  
- Apply constitutional AI + red teaming  
- Audit model data and behavior  

**Examples:**  
- Refuse harmful requests  
- Document dataset sources  
- Test for bias before release  
</Accordion>

</AccordionGroup>

<hr />

## Conclusion & Interview Tips

This guide covers all major AI engineering areas — from prompt design to scalable systems and ethical deployment.

### Key Preparation Tips
- Understand system trade-offs  
- Build RAG or LLM-serving demos  
- Learn caching, monitoring, CI/CD  
- Emphasize ethics & safety  
- Explain architecture choices clearly  

### During the Interview
- Clarify before answering  
- Think aloud for reasoning  
- Mention latency/cost trade-offs  
- Talk about monitoring and fallback  
- Stay calm & confident  

<Info>
Interviews test not just your AI knowledge — but your reasoning about **scale, safety, and reliability**. Stay grounded and structured.
</Info>

<Check>
Good luck with your AI Engineer interviews!
</Check>
