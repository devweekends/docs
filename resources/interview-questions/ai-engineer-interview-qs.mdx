---
title: "AI Engineer (Traditional ML/DL)"
description: "Machine Learning, Deep Learning, and MLOps"
icon: "brain"
---

# AI Engineer Interview Questions (50+ Detailed Q&A)

## 1. Machine Learning Fundamentals

<AccordionGroup>
<Accordion title="1. Bias vs Variance Tradeoff">
**Answer**:
*   **Bias**: Error due to overly simplistic assumptions (Underfitting). Model misses relations.
*   **Variance**: Error due to sensitivity to small fluctuations in training set (Overfitting). Model models noise.
*   **Tradeoff**: Increasing complexity decreases bias but increases variance. Goal: Sweet spot.
</Accordion>

<Accordion title="2. Supervised vs Unsupervised vs Semi-supervised">
**Answer**:
*   **Supervised**: Labeled data (Regression, Classification). Costly labeling.
*   **Unsupervised**: Unlabeled (Clustering, PCA). Finds structure.
*   **Semi-supervised**: Small labeled + Large unlabeled. (Label propagation).
</Accordion>

<Accordion title="3. Precision vs Recall (and F1 Score)">
**Answer**:
*   **Precision**: TP / (TP + FP). Quality. (Don't flag good email as spam).
*   **Recall**: TP / (TP + FN). Quantity. (Don't miss a cancer diagnosis).
*   **F1**: Harmonic mean. Used when classes are imbalanced.
</Accordion>

<Accordion title="4. ROC Curve & AUC">
**Answer**:
Plot of TPR (Recall) vs FPR (1-Specificity) at various thresholds.
AUC (Area Under Curve): 0.5 = Random. 1.0 = Perfect.
Metric independent of threshold.
</Accordion>

<Accordion title="5. Overfitting Prevention">
**Answer**:
1.  **More Data**: Generic.
2.  **Regularization**: L1 (Lasso), L2 (Ridge).
3.  **Cross-Validation**: K-Fold.
4.  **Ensembling**: Bagging/Boosting.
5.  **Simplifying Model**: Fewer features/layers.
</Accordion>

<Accordion title="6. Gradient Descent variants">
**Answer**:
*   **Batch**: Uses full dataset. Slow, stable.
*   **Stochastic (SGD)**: Uses 1 sample. Fast, noisy.
*   **Mini-batch**: Uses batch (32/64). Best of both.
</Accordion>

<Accordion title="7. Curse of Dimensionality">
**Answer**:
As dimensions grow, data becomes sparse. Distance metrics (Euclidean) lose meaning.
Fix: Dimensionality Reduction (PCA, t-SNE).
</Accordion>

<Accordion title="8. Normalization vs Standardization">
**Answer**:
*   **Normalization (Min-Max)**: Scales to [0, 1]. Sensitive to outliers.
*   **Standardization (Z-score)**: Mean 0, Std 1. Better for Gaussian data/SVM/Linear Reg.
</Accordion>

<Accordion title="9. Handling Imbalanced Data">
**Answer**:
*   **Resampling**: Oversample minority (SMOTE), Undersample majority.
*   **Metric Choice**: Don't use Accuracy. Use F1 / AUC-PR.
*   **weighted Loss**: Penalize wrong class more.
</Accordion>

<Accordion title="10. PCA (Principal Component Analysis)">
**Answer**:
Linear transformation to project data to lower dimension while preserving Variance.
Eigenvectors of Covariance matrix.
</Accordion>
</AccordionGroup>

## 2. Deep Learning

<AccordionGroup>
<Accordion title="11. Activation Functions (ReLU vs Sigmoid)">
**Answer**:
*   **Sigmoid**: 0 to 1. Vanishing gradient problem.
*   **ReLU**: max(0, x). Solves vanishing gradient. Dead ReLU issue.
*   **Softmax**: Probability distribution (Output layer).
</Accordion>

<Accordion title="12. Vanishing/Exploding Gradients">
**Answer**:
Deep networks. Gradients multiply via chain rule.
If < 1, decays to 0 (Vanishing). If > 1, explodes.
Fix: Residual connections (ResNet), BatchNorm, LSTM/GRU, ReLU.
</Accordion>

<Accordion title="13. CNN Internals (Convolution)">
**Answer**:
*   **Filter/Kernel**: Learns features (Edges -> Shapes).
*   **Stride**: Step size.
*   **Pooling (Max)**: Downsampling. Translation invariance.
*   **Flatten**: Connect to Dense layer.
</Accordion>

<Accordion title="14. RNN vs LSTM">
**Answer**:
*   **RNN**: Short term memory. Vanishing gradient.
*   **LSTM**: Gates (Input, Forget, Output). Cell state carries long term info.
</Accordion>

<Accordion title="15. Batch Normalization">
**Answer**:
Normalizes layer inputs (Mean 0, Var 1).
Stabilizes training. Allows higher learning rate. Acts as regularizer.
</Accordion>

<Accordion title="16. Dropout">
**Answer**:
Randomly zeroing neurons during training.
Prevents co-adaptation. Approximate bagging.
Only active in Training (Not inference).
</Accordion>

<Accordion title="17. Optimizers (Adam vs SGD)">
**Answer**:
*   **SGD**: Constant LR. Can get stuck in local minima/saddle points.
*   **Adam**: Adaptive Moment Estimation. Momentum + RMSProp. Adapts LR per parameter. Standard choice.
</Accordion>

<Accordion title="18. Transfer Learning">
**Answer**:
Use pre-trained model (ResNet on ImageNet).
Freeze early layers (Generic features). Fine-tune last layers (Specific task).
Requires less data.
</Accordion>

<Accordion title="19. Autoencoders">
**Answer**:
Unsupervised. Input -> Encoder -> Latent Space (Compressed) -> Decoder -> Output.
Loss: Reconstruction Error.
Used for: Denoising, Anomaly Detection.
</Accordion>

<Accordion title="20. GANs (Generative Adversarial Networks)">
**Answer**:
*   **Generator**: Creates fake data.
*   **Discriminator**: Tries to spot fake.
*   Min-Max game. Hard to stabilize (Mode collapse).
</Accordion>
</AccordionGroup>

## 3. Classical Algorithms

<AccordionGroup>
<Accordion title="21. Random Forest vs Gradient Boosting (XGBoost)">
**Answer**:
*   **RF (Bagging)**: Build trees in parallel. High variance reduction. Robust.
*   **GBM (Boosting)**: Build trees sequentially to correct errors. High bias reduction. SOTA for tabular.
</Accordion>

<Accordion title="22. SVM (Support Vector Machine)">
**Answer**:
Find Hyperplane maximizing Margin between classes.
**Kernel Trick**: Map non-linear data to high dimension where it is linear.
</Accordion>

<Accordion title="23. K-Means Clustering">
**Answer**:
1.  Pick K centroids.
2.  Assign points to nearest.
3.  Update centroid used mean.
4.  Repeat.
Assumes spherical clusters. Need to pick K (Elbow method).
</Accordion>

<Accordion title="24. Naive Bayes">
**Answer**:
Probabilistic classifier based on Bayes Theorem.
"Naive": Assumes features are independent (Simplification).
Good for Text (Spam filtering).
</Accordion>

<Accordion title="25. Logistic Regression">
**Answer**:
Classification (not regression).
Linear regression + Sigmoid function.
Outputs probability.
</Accordion>

<Accordion title="26. KNN (K-Nearest Neighbors)">
**Answer**:
Lazy learning (No training).
Distance metric (Euclidean).
Sensitive to noise and scale.
Computationally expensive at inference.
</Accordion>

<Accordion title="27. Decision Tree Pruning">
**Answer**:
Stopping tree growth (Pre-pruning) or cutting branches (Post-pruning) to prevent Overfitting.
</Accordion>

<Accordion title="28. Linear Regression Assumptions">
**Answer**:
Linearity, Homoscedasticity (Constant variance), Normality of errors, No Multicollinearity.
</Accordion>

<Accordion title="29. Regularization (L1 vs L2)">
**Answer**:
*   **L1 (Lasso)**: Adds abs(weights). Sparse solution (Feature selection).
*   **L2 (Ridge)**: Adds square(weights). Small weights. Stable.
</Accordion>

<Accordion title="30. Time Series Stationarity">
**Answer**:
Mean and Variance do not change over time.
ARIMA requires stationarity.
Check: Augmented Dickey-Fuller test.
</Accordion>
</AccordionGroup>

## 4. MLOps & Deployment

<AccordionGroup>
<Accordion title="31. Model Serving Patterns">
**Answer**:
*   **Realtime**: API (FastAPI/Flask).
*   **Batch**: Offline job (Airflow).
*   **Streaming**: Kafka consumer.
*   **Edge**: On-device (TFLite).
</Accordion>

<Accordion title="32. Data Drift vs Concept Drift">
**Answer**:
*   **Data Drift**: Input distribution changes (Users are younger now). `P(X)` changes.
*   **Concept Drift**: Relationship changes (House price vs Size rule changes). `P(Y|X)` changes.
</Accordion>

<Accordion title="33. Feature Store">
**Answer**:
Central repo for features.
Ensures consistency between Training (Offline) and Inference (Online).
Example: Feast.
</Accordion>

<Accordion title="34. A/B Testing in ML">
**Answer**:
Split traffic. Control (Model A) vs Treatment (Model B).
Metric: Click through rate / Conversion.
Statistical significance check.
</Accordion>

<Accordion title="35. Model Monitoring">
**Answer**:
Tracking: Latency, Error Rate, Drift, Prediction Distribution.
Tools: Prometheus, Grafana, EvidentlyAI.
</Accordion>

<Accordion title="36. Quantization">
**Answer**:
Converting FP32 weights to INT8.
Reduces model size (4x) and speeds up inference.
Post-training vs Quantization-aware training.
</Accordion>

<Accordion title="37. Containerization (Docker)">
**Answer**:
Packaging Code + Dependencies + OS.
Ensures "Works on my machine" = "Works on Cloud".
Base image: `python:3.9-slim`.
</Accordion>

<Accordion title="38. CI/CD for ML (CT/CD)">
**Answer**:
*   **CI**: Test data, code, model training.
*   **Continuous Training (CT)**: Auto-retrain on new data trigger.
</Accordion>

<Accordion title="39. Explainability (XAI)">
**Answer**:
*   **SHAP**: Shapley values (Game theory). Contribution of each feature.
*   **LIME**: Local approximation.
Necessary for regulated industries (Credit/Health).
</Accordion>

<Accordion title="40. ONNX (Open Neural Network Exchange)">
**Answer**:
Interoperability standard.
Train in PyTorch -> Export ONNX -> Run in C#/Java/Browser.
</Accordion>
</AccordionGroup>

## 5. Python for ML

<AccordionGroup>
<Accordion title="41. NumPy Broadcasting">
**Answer**:
Rule for performing math on arrays of different shapes.
Stretch smaller array to match larger.
Efficient (No copies).
</Accordion>

<Accordion title="42. Pandas Memory Optimization">
**Answer**:
*   `category` type for strings with low cardinality.
*   Downcast `float64` to `float32`.
*   Iterating: Use Vectorization (Apply), avoid loops.
</Accordion>

<Accordion title="43. Decorators">
**Answer**:
Wrapper function.
Used for logging, timing, validation, caching.
`@memoize`.
</Accordion>

<Accordion title="44. Generators (yield)">
**Answer**:
Lazy evaluation.
Process huge datasets line-by-line without loading into RAM.
</Accordion>

<Accordion title="45. Multiprocessing vs Threading">
**Answer**:
*   **Threading**: IO-bound. Limited by GIL.
*   **Multiprocessing**: CPU-bound. Separate memory space. Bypasses GIL.
</Accordion>

<Accordion title="46. List Comprehension">
**Answer**:
Concise syntax. Faster than for loops (C implementation).
`[x**2 for x in range(10) if x%2==0]`.
</Accordion>

<Accordion title="47. Virtual Environments">
**Answer**:
Dependency isolation.
`venv`, `conda`, `poetry`.
Prevents "dependency hell" (Conflicting versions).
</Accordion>

<Accordion title="48. Context Managers (`with`)">
**Answer**:
Resource management (Files, DB connections).
Ensures `__exit__` (Close) is called even on error.
</Accordion>

<Accordion title="49. PyTorch `autograd`">
**Answer**:
Automatic differentiation engine.
Builds computational graph dynamicallly.
Computes gradients for Backprop.
</Accordion>

<Accordion title="50. Serializing Models (Pickle)">
**Answer**:
Saving object to file.
Security risk (Arbitrary code execution).
Safer: `torch.save` (state_dict), ONNX, safetensors.
</Accordion>
</AccordionGroup>
